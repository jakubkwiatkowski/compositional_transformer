
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.4">
    
    
      
        <title>Transformer Block - grid_transformer</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.80dcb947.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#4051b5">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-grid_transformertransformer_block" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="grid_transformer" class="md-header__button md-logo" aria-label="grid_transformer" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            grid_transformer
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Transformer Block
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/jakubkwiatkowski/grid_transformer" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    grid_transformer
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="grid_transformer" class="md-nav__button md-logo" aria-label="grid_transformer" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    grid_transformer
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/jakubkwiatkowski/grid_transformer" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    grid_transformer
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/raven/" class="md-nav__link">
        Raven
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/sudoku/" class="md-nav__link">
        Sudoku
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1" type="checkbox" id="__nav_4_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1">
          Grid Transformer
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Grid Transformer" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          Grid Transformer
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../augmentation/" class="md-nav__link">
        Augmentation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../augmented_transformer/" class="md-nav__link">
        Augmented Transformer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../grid_transformer/" class="md-nav__link">
        Grid Transformer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../layers/" class="md-nav__link">
        Layers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../mask/" class="md-nav__link">
        Mask
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../parameters/" class="md-nav__link">
        Parameters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../position_embedding/" class="md-nav__link">
        Position Embedding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../preprocessing/" class="md-nav__link">
        Preprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../simple_transformer/" class="md-nav__link">
        Simple Transformer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../transformer/" class="md-nav__link">
        Transformer
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Transformer Block
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Transformer Block
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformerblock" class="md-nav__link">
    TransformerBlock
  </a>
  
    <nav class="md-nav" aria-label="TransformerBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#static-methods" class="md-nav__link">
    Static methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from_config" class="md-nav__link">
    from_config
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with_name_scope" class="md-nav__link">
    with_name_scope
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build" class="md-nav__link">
    build
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#finalize_state" class="md-nav__link">
    finalize_state
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_config" class="md-nav__link">
    get_config
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_weights" class="md-nav__link">
    get_weights
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformerblockbase" class="md-nav__link">
    TransformerBlockBase
  </a>
  
    <nav class="md-nav" aria-label="TransformerBlockBase">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#static-methods_1" class="md-nav__link">
    Static methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from_config_1" class="md-nav__link">
    from_config
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with_name_scope_1" class="md-nav__link">
    with_name_scope
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_1" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss_1" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric_1" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update_1" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable_1" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight_1" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build_1" class="md-nav__link">
    build
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call_1" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask_1" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape_1" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature_1" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params_1" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#finalize_state_1" class="md-nav__link">
    finalize_state
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_config_1" class="md-nav__link">
    get_config
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at_1" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at_1" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at_1" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at_1" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at_1" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at_1" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_weights_1" class="md-nav__link">
    get_weights
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights_1" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformerblock" class="md-nav__link">
    TransformerBlock
  </a>
  
    <nav class="md-nav" aria-label="TransformerBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#static-methods" class="md-nav__link">
    Static methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from_config" class="md-nav__link">
    from_config
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with_name_scope" class="md-nav__link">
    with_name_scope
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build" class="md-nav__link">
    build
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#finalize_state" class="md-nav__link">
    finalize_state
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_config" class="md-nav__link">
    get_config
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_weights" class="md-nav__link">
    get_weights
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformerblockbase" class="md-nav__link">
    TransformerBlockBase
  </a>
  
    <nav class="md-nav" aria-label="TransformerBlockBase">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#static-methods_1" class="md-nav__link">
    Static methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from_config_1" class="md-nav__link">
    from_config
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with_name_scope_1" class="md-nav__link">
    with_name_scope
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_1" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss_1" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric_1" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update_1" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable_1" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight_1" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build_1" class="md-nav__link">
    build
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call_1" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask_1" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape_1" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature_1" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params_1" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#finalize_state_1" class="md-nav__link">
    finalize_state
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_config_1" class="md-nav__link">
    get_config
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at_1" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at_1" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at_1" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at_1" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at_1" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at_1" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_weights_1" class="md-nav__link">
    get_weights
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights_1" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/jakubkwiatkowski/grid_transformer/edit/main/reference/grid_transformer/transformer_block.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


<h1 id="module-grid_transformertransformer_block">Module grid_transformer.transformer_block</h1>
<p>This module contains two classes <code>TransformerBlockBase</code> and <code>TransformerBlock</code>, which implement the transformer block as defined in the "Attention Is All You Need" paper by Google.</p>
<p>A transformer block consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network.</p>
<p>Classes:</p>
<ul>
<li>TransformerBlockBase:  A transformer block with original implementation of the call function</li>
<li>TransformerBlock: A transformer block with modified implementation of the call function which changes the order of layernorm and residual connection.</li>
</ul>
<p>Example usage:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Instantiate the transformer block</span>
<span class="n">tb</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="o">.</span><span class="n">TransformerBlockBase</span><span class="p">()</span>

<span class="c1"># Build the transformer block with input shape (batch_size, sequence_length, input_dim)</span>
<span class="n">tb</span><span class="o">.</span><span class="n">build</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>

<span class="c1"># Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">tb</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">)))</span>

<span class="c1"># Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim)</span>
<span class="n">output</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">tb</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Instantiate a transformer block with user-specified feed-forward network</span>
<span class="n">ffn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">)])</span>
<span class="n">tb</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="o">.</span><span class="n">TransformerBlock</span><span class="p">(</span><span class="n">ffn</span><span class="o">=</span><span class="n">ffn</span><span class="p">)</span>

<span class="c1"># Instantiate a TransformerBlock2</span>
<span class="n">tb2</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="o">.</span><span class="n">TransformerBlock</span><span class="p">()</span>

<span class="c1"># Build the transformer block with input shape (batch_size, sequence_length, input_dim)</span>
<span class="n">tb2</span><span class="o">.</span><span class="n">build</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>

<span class="c1"># Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">tb2</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">)))</span>

<span class="c1"># Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim)</span>
<span class="n">output</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">tb2</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Instantiate a transformer block with user-specified feed-forward network</span>
<span class="n">ffn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">)])</span>
<span class="n">tb</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="o">.</span><span class="n">TransformerBlock</span><span class="p">(</span><span class="n">ffn</span><span class="o">=</span><span class="n">ffn</span><span class="p">)</span>
</code></pre></div>

<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">This module contains two classes `TransformerBlockBase` and `TransformerBlock`, which implement the transformer block as defined in the &quot;Attention Is All You Need&quot; paper by Google.</span>

<span class="sd">A transformer block consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network.</span>

<span class="sd">Classes:</span>

<span class="sd">- TransformerBlockBase:  A transformer block with original implementation of the call function</span>

<span class="sd">- TransformerBlock: A transformer block with modified implementation of the call function which changes the order of layernorm and residual connection.</span>

<span class="sd">Example usage:</span>

<span class="sd">    import tensorflow as tf</span>

<span class="sd">    # Instantiate the transformer block</span>

<span class="sd">    tb = transformer_block.TransformerBlockBase()</span>

<span class="sd">    # Build the transformer block with input shape (batch_size, sequence_length, input_dim)</span>

<span class="sd">    tb.build((None, 10, 32))</span>

<span class="sd">    # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim)</span>

<span class="sd">    output = tb(tf.random.normal((32, 10, 32)))</span>

<span class="sd">    # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim)</span>

<span class="sd">    output, scores = tb(tf.random.normal((32, 10, 32)), return_attention_scores=True)</span>

<span class="sd">    # Instantiate a transformer block with user-specified feed-forward network</span>

<span class="sd">    ffn = tf.keras.Sequential([tf.keras.layers.Dense(256, activation=&#39;relu&#39;), tf.keras.layers.Dense(32)])</span>

<span class="sd">    tb = transformer_block.TransformerBlock(ffn=ffn)</span>

<span class="sd">    # Instantiate a TransformerBlock2</span>

<span class="sd">    tb2 = transformer_block.TransformerBlock()</span>

<span class="sd">    # Build the transformer block with input shape (batch_size, sequence_length, input_dim)</span>

<span class="sd">    tb2.build((None, 10, 32))</span>

<span class="sd">    # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim)</span>

<span class="sd">    output = tb2(tf.random.normal((32, 10, 32)))</span>

<span class="sd">    # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim)</span>

<span class="sd">    output, scores = tb2(tf.random.normal((32, 10, 32)), return_attention_scores=True)</span>

<span class="sd">    # Instantiate a transformer block with user-specified feed-forward network</span>

<span class="sd">    ffn = tf.keras.Sequential([tf.keras.layers.Dense(256, activation=&#39;relu&#39;), tf.keras.layers.Dense(32)])</span>

<span class="sd">    tb = transformer_block.TransformerBlock(ffn=ffn)</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">LayerNormalization</span>

<span class="k">class</span> <span class="nc">TransformerBlockBase</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    A transformer block as defined in the &quot;Attention Is All You Need&quot; paper by Google.</span>

<span class="sd">    It consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ff_mul</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ff_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">mha_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">ff_act</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span>

                 <span class="n">ffn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Initialize the transformer block.</span>

<span class="sd">        :param num_heads: Number of heads in the multi-head self-attention mechanism</span>

<span class="sd">        :param size: Dimension of the input/output vectors. If None, it will be inferred from input_shape during build()</span>

<span class="sd">        :param ff_mul: Multiplier for the feed-forward network size</span>

<span class="sd">        :param ff_size: Size of the feed-forward network. If None, it will be inferred from size and ff_mul</span>

<span class="sd">        :param dropout: Dropout rate for the feed-forward network</span>

<span class="sd">        :param mha_dropout: Dropout rate for the multi-head self-attention mechanism</span>

<span class="sd">        :param ff_act: Activation function for the feed-forward network</span>

<span class="sd">        :param ffn: A user-specified feed-forward network. If None, a default one will be created</span>

<span class="sd">        :param return_attention_scores: Whether to return attention scores</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerBlockBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ff_mul</span> <span class="o">=</span> <span class="n">ff_mul</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ff_size</span> <span class="o">=</span> <span class="n">ff_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ff_act</span> <span class="o">=</span> <span class="n">ff_act</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mha_dropout</span> <span class="o">=</span> <span class="n">mha_dropout</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">return_attention_scores</span><span class="o">=</span> <span class="n">return_attention_scores</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">ffn</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Build the transformer block.</span>

<span class="sd">        :param input_shape: Shape of the input tensor</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">att</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mha_dropout</span><span class="p">,</span> <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">ff_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_mul</span>

        <span class="c1"># self.ffn = self.ffn or build_dense_model(</span>

        <span class="c1">#     [</span>

        <span class="c1">#         self.ff_size,</span>

        <span class="c1">#         Dropout(self.dropout),</span>

        <span class="c1">#         self.size,</span>

        <span class="c1">#         Dropout(self.dropout)</span>

        <span class="c1">#     ],</span>

        <span class="c1">#     activation=self.ff_act,</span>

        <span class="c1">#     last_activation=None)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="ow">or</span> <span class="n">Sequential</span><span class="p">([</span>

            <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ff_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ff_act</span><span class="p">),</span>

            <span class="n">Dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">),</span>

            <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>

            <span class="n">Dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Perform the forward pass of the transformer block.</span>

<span class="sd">        :param inputs: Input tensor</span>

<span class="sd">        :return: Output tensor and attention scores if return_attention_scores is True</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_attention_scores</span><span class="p">:</span>

            <span class="n">x</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_attention_scores</span><span class="p">:</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">),</span> <span class="n">scores</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>

        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>

            <span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span>

            <span class="s2">&quot;num_heads&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>

            <span class="s2">&quot;ff_mul&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_mul</span><span class="p">,</span>

            <span class="s2">&quot;ff_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_size</span><span class="p">,</span>

            <span class="s2">&quot;ff_act&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_act</span><span class="p">,</span>

            <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>

            <span class="s2">&quot;mha_dropout&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha_dropout</span><span class="p">,</span>

            <span class="s2">&quot;return_attention_scores&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_attention_scores</span><span class="p">,</span>

            <span class="s2">&quot;ffn&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span>

        <span class="p">})</span>

        <span class="k">return</span> <span class="n">config</span>

<span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">TransformerBlockBase</span><span class="p">):</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    A TransformerBlock with a modified call function, which changes the order of layernorm and residual connection.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Perform the forward pass of the transformer block.</span>

<span class="sd">        :param inputs: Input tensor</span>

<span class="sd">        :return: Output tensor and attention scores if return_attention_scores is True</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_attention_scores</span><span class="p">:</span>

            <span class="n">x</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">inputs</span>

        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_attention_scores</span><span class="p">:</span>

            <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">scores</span>

        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
</code></pre></div>

</details>
<h2 id="classes">Classes</h2>
<h3 id="transformerblock">TransformerBlock</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ff_mul</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">ff_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mha_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
    <span class="n">ffn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>

<p>A TransformerBlock with a modified call function, which changes the order of layernorm and residual connection.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="n">TransformerBlock</span>(<span class="n">TransformerBlockBase</span>):

    <span class="s">&quot;&quot;&quot;</span>

<span class="s">    A TransformerBlock with a modified call function, which changes the order of layernorm and residual connection.</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="n">def</span> <span class="n">call</span>(<span class="nb">self</span>, <span class="n">inputs</span>):

        <span class="s">&quot;&quot;&quot;</span>

<span class="s">        Perform the forward pass of the transformer block.</span>

<span class="s">        :param inputs: Input tensor</span>

<span class="s">        :return: Output tensor and attention scores if return_attention_scores is True</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="nb">x</span> = <span class="nb">self</span>.<span class="n">layernorm1</span>(<span class="n">inputs</span>)

        <span class="k">if</span> <span class="nb">self</span>.<span class="n">return_attention_scores:</span>

            <span class="nb">x</span>, <span class="n">scores</span> = <span class="nb">self</span>.<span class="n">att</span>(<span class="nb">x</span>, <span class="nb">x</span>, <span class="n">return_attention_scores</span>=<span class="nb">True</span>)

        <span class="n">else:</span>

            <span class="nb">x</span> = <span class="nb">self</span>.<span class="n">att</span>(<span class="nb">x</span>, <span class="nb">x</span>)

        <span class="nb">x</span> = <span class="nb">x</span> + <span class="n">inputs</span>

        <span class="n">y</span> = <span class="nb">self</span>.<span class="n">layernorm2</span>(<span class="nb">x</span>)

        <span class="n">y</span> = <span class="nb">self</span>.<span class="n">ffn</span>(<span class="n">y</span>)

        <span class="k">if</span> <span class="nb">self</span>.<span class="n">return_attention_scores:</span>

            <span class="k">return</span> <span class="nb">x</span> + <span class="n">y</span>, <span class="n">scores</span>

        <span class="k">return</span> <span class="nb">x</span> + <span class="n">y</span>
</code></pre></div>

</details>
<hr />
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>grid_transformer.transformer_block.TransformerBlockBase</li>
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h4 id="static-methods">Static methods</h4>
<h4 id="from_config">from_config</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">from_config</span><span class="p">(</span>
    <span class="n">config</span>
<span class="p">)</span>
</code></pre></div>

<p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <code>set_weights</code>).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>config</td>
<td>None</td>
<td>A Python dictionary, typically the<br>output of get_config.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A layer instance.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@classmethod</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">from_config</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Creates a layer from its config.</span>

<span class="s2">        This method is the reverse of `get_config`,</span>

<span class="s2">        capable of instantiating the same layer from the config</span>

<span class="s2">        dictionary. It does not handle layer connectivity</span>

<span class="s2">        (handled by Network), nor weights (handled by `set_weights`).</span>

<span class="s2">        Args:</span>

<span class="s2">            config: A Python dictionary, typically the</span>

<span class="s2">                output of get_config.</span>

<span class="s2">        Returns:</span>

<span class="s2">            A layer instance.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">cls</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="with_name_scope">with_name_scope</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">with_name_scope</span><span class="p">(</span>
    <span class="n">method</span>
<span class="p">)</span>
</code></pre></div>

<p>Decorator to automatically enter the module name scope.</p>
<blockquote>
<blockquote>
<blockquote>
<p>class MyModule(tf.Module):
...   @tf.Module.with_name_scope
...   def <strong>call</strong>(self, x):
...     if not hasattr(self, 'w'):
...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))
...     return tf.matmul(x, self.w)</p>
</blockquote>
</blockquote>
</blockquote>
<p>Using the above module would produce <code>tf.Variable</code>s and <code>tf.Tensor</code>s whose
names included the module name:</p>
<blockquote>
<blockquote>
<blockquote>
<p>mod = MyModule()
mod(tf.ones([1, 2]))
<tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>
mod.w
<tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,
numpy=..., dtype=float32)></p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>method</td>
<td>None</td>
<td>The method to wrap.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>The original method wrapped such that it enters the module's name scope.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@classmethod</span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">with_name_scope</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span><span class="w"> </span><span class="k">method</span><span class="p">)</span><span class="err">:</span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Decorator to automatically enter the module name scope.</span>

<span class="ss">    &gt;&gt;&gt; class MyModule(tf.Module):</span>

<span class="ss">    ...   @tf.Module.with_name_scope</span>

<span class="ss">    ...   def __call__(self, x):</span>

<span class="ss">    ...     if not hasattr(self, &#39;w&#39;):</span>

<span class="ss">    ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))</span>

<span class="ss">    ...     return tf.matmul(x, self.w)</span>

<span class="ss">    Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose</span>

<span class="ss">    names included the module name:</span>

<span class="ss">    &gt;&gt;&gt; mod = MyModule()</span>

<span class="ss">    &gt;&gt;&gt; mod(tf.ones([1, 2]))</span>

<span class="ss">    &lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)&gt;</span>

<span class="ss">    &gt;&gt;&gt; mod.w</span>

<span class="ss">    &lt;tf.Variable &#39;my_module/Variable:0&#39; shape=(2, 3) dtype=float32,</span>

<span class="ss">    numpy=..., dtype=float32)&gt;</span>

<span class="ss">    Args:</span>

<span class="ss">      method: The method to wrap.</span>

<span class="ss">    Returns:</span>

<span class="ss">      The original method wrapped such that it enters the module&#39;s name scope.</span>

<span class="ss">    &quot;&quot;&quot;</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">method_with_name_scope</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="err">:</span>

<span class="w">      </span><span class="k">with</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">name_scope</span><span class="p">:</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">method</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">tf_decorator</span><span class="p">.</span><span class="n">make_decorator</span><span class="p">(</span><span class="k">method</span><span class="p">,</span><span class="w"> </span><span class="n">method_with_name_scope</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="instance-variables">Instance variables</h4>
<div class="codehilite"><pre><span></span><code><span class="n">activity_regularizer</span>
</code></pre></div>

<p>Optional regularizer function for the output of this layer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">compute_dtype</span>
</code></pre></div>

<p>The dtype of the layer's computations.</p>
<p>This is equivalent to <code>Layer.dtype_policy.compute_dtype</code>. Unless
mixed precision is used, this is the same as <code>Layer.dtype</code>, the dtype of
the weights.</p>
<p>Layers automatically cast their inputs to the compute dtype, which
causes computations and the output to be in the compute dtype as well.
This is done by the base Layer class in <code>Layer.__call__</code>, so you do not
have to insert these casts if implementing your own layer.</p>
<p>Layers often perform certain internal computations in higher precision
when <code>compute_dtype</code> is float16 or bfloat16 for numeric stability. The
output will still typically be float16 or bfloat16 in such cases.</p>
<div class="codehilite"><pre><span></span><code><span class="n">dtype</span>
</code></pre></div>

<p>The dtype of the layer weights.</p>
<p>This is equivalent to <code>Layer.dtype_policy.variable_dtype</code>. Unless
mixed precision is used, this is the same as <code>Layer.compute_dtype</code>, the
dtype of the layer's computations.</p>
<div class="codehilite"><pre><span></span><code><span class="n">dtype_policy</span>
</code></pre></div>

<p>The dtype policy associated with this layer.</p>
<p>This is an instance of a <code>tf.keras.mixed_precision.Policy</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">dynamic</span>
</code></pre></div>

<p>Whether the layer is dynamic (eager-only); set in the constructor.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inbound_nodes</span>
</code></pre></div>

<p>Return Functional API nodes upstream of this layer.</p>
<div class="codehilite"><pre><span></span><code><span class="nb">input</span>
</code></pre></div>

<p>Retrieves the input tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one input,
i.e. if it is connected to one incoming layer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">input_mask</span>
</code></pre></div>

<p>Retrieves the input mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<p>Returns:
    Input mask tensor (potentially None) or list of input
    mask tensors.</p>
<p>Raises:
    AttributeError: if the layer is connected to
    more than one incoming layers.</p>
<div class="codehilite"><pre><span></span><code><span class="n">input_shape</span>
</code></pre></div>

<p>Retrieves the input shape(s) of a layer.</p>
<p>Only applicable if the layer has exactly one input,
i.e. if it is connected to one incoming layer, or if all inputs
have the same shape.</p>
<div class="codehilite"><pre><span></span><code><span class="n">input_spec</span>
</code></pre></div>

<p><code>InputSpec</code> instance(s) describing the input format for this layer.</p>
<p>When you create a layer subclass, you can set <code>self.input_spec</code> to
enable the layer to run input compatibility checks when it is called.
Consider a <code>Conv2D</code> layer: it can only be called on a single input
tensor of rank 4. As such, you can set, in <code>__init__()</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputSpec</span><span class="p">(</span><span class="n">ndim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>

<p>Now, if you try to call the layer on an input that isn't rank 4
(for instance, an input of shape <code>(2,)</code>, it will raise a
nicely-formatted error:</p>
<div class="codehilite"><pre><span></span><code><span class="n">ValueError</span><span class="o">:</span><span class="w"> </span><span class="n">Input</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">conv2d</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">incompatible</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="o">:</span>
<span class="n">expected</span><span class="w"> </span><span class="n">ndim</span><span class="o">=</span><span class="mi">4</span><span class="o">,</span><span class="w"> </span><span class="n">found</span><span class="w"> </span><span class="n">ndim</span><span class="o">=</span><span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="n">Full</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">received</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="mi">2</span><span class="o">]</span>
</code></pre></div>

<p>Input checks that can be specified via <code>input_spec</code> include:
- Structure (e.g. a single input, a list of 2 inputs, etc)
- Shape
- Rank (ndim)
- Dtype</p>
<p>For more information, see <code>tf.keras.layers.InputSpec</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span>
</code></pre></div>

<p>List of losses added using the <code>add_loss()</code> API.</p>
<p>Variable regularization tensors are created when this property is
accessed, so it is eager safe: accessing <code>losses</code> under a
<code>tf.GradientTape</code> will propagate gradients back to the corresponding
variables.</p>
<div class="codehilite"><pre><span></span><code><span class="n">metrics</span>
</code></pre></div>

<p>List of metrics added using the <code>add_metric()</code> API.</p>
<div class="codehilite"><pre><span></span><code><span class="n">name</span>
</code></pre></div>

<p>Name of the layer (string), set in the constructor.</p>
<div class="codehilite"><pre><span></span><code><span class="n">name_scope</span>
</code></pre></div>

<p>Returns a <code>tf.name_scope</code> instance for this class.</p>
<div class="codehilite"><pre><span></span><code><span class="n">non_trainable_variables</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">non_trainable_weights</span>
</code></pre></div>

<p>List of all non-trainable weights tracked by this layer.</p>
<p>Non-trainable weights are <em>not</em> updated during training. They are
expected to be updated manually in <code>call()</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">outbound_nodes</span>
</code></pre></div>

<p>Return Functional API nodes downstream of this layer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">output</span>
</code></pre></div>

<p>Retrieves the output tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one output,
i.e. if it is connected to one incoming layer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">output_mask</span>
</code></pre></div>

<p>Retrieves the output mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<p>Returns:
    Output mask tensor (potentially None) or list of output
    mask tensors.</p>
<p>Raises:
    AttributeError: if the layer is connected to
    more than one incoming layers.</p>
<div class="codehilite"><pre><span></span><code><span class="n">output_shape</span>
</code></pre></div>

<p>Retrieves the output shape(s) of a layer.</p>
<p>Only applicable if the layer has one output,
or if all outputs have the same shape.</p>
<div class="codehilite"><pre><span></span><code><span class="n">stateful</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">submodules</span>
</code></pre></div>

<p>Sequence of all sub-modules.</p>
<p>Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).</p>
<blockquote>
<blockquote>
<blockquote>
<p>a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
list(a.submodules) == [b, c]
True
list(b.submodules) == [c]
True
list(c.submodules) == []
True</p>
</blockquote>
</blockquote>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">supports_masking</span>
</code></pre></div>

<p>Whether this layer supports computing a mask using <code>compute_mask</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">trainable</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">trainable_variables</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">trainable_weights</span>
</code></pre></div>

<p>List of all trainable weights tracked by this layer.</p>
<p>Trainable weights are updated via gradient descent during training.</p>
<div class="codehilite"><pre><span></span><code><span class="n">updates</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">variable_dtype</span>
</code></pre></div>

<p>Alias of <code>Layer.dtype</code>, the dtype of the weights.</p>
<div class="codehilite"><pre><span></span><code><span class="n">variables</span>
</code></pre></div>

<p>Returns the list of all layer variables/weights.</p>
<p>Alias of <code>self.weights</code>.</p>
<p>Note: This will not track the weights of nested <code>tf.Modules</code> that are
not themselves Keras layers.</p>
<div class="codehilite"><pre><span></span><code><span class="n">weights</span>
</code></pre></div>

<p>Returns the list of all layer variables/weights.</p>
<h4 id="methods">Methods</h4>
<h4 id="add_loss">add_loss</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">losses</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be
dependent on the inputs passed when calling a layer. Hence, when reusing
the same layer on different inputs <code>a</code> and <code>b</code>, some entries in
<code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method
automatically keeps track of dependencies.</p>
<p>This method can be used inside a subclassed layer or model's <code>call</code>
function, in which case <code>losses</code> should be a Tensor or list of Tensors.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>losses</td>
<td>None</td>
<td>Loss tensor, or list/tuple of tensors. Rather than tensors,<br>losses may also be zero-argument callables which create a loss<br>tensor.</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>None</td>
<td>Used for backwards compatibility only.</td>
<td>None</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">add_loss</span>(<span class="nb">self</span>, <span class="n">losses</span>, **<span class="n">kwargs</span>):

        <span class="s">&quot;&quot;&quot;Add loss tensor(s), potentially dependent on layer inputs.</span>

<span class="s">        Some losses (for instance, activity regularization losses) may be</span>

<span class="s">        dependent on the inputs passed when calling a layer. Hence, when reusing</span>

<span class="s">        the same layer on different inputs `a` and `b`, some entries in</span>

<span class="s">        `layer.losses` may be dependent on `a` and some on `b`. This method</span>

<span class="s">        automatically keeps track of dependencies.</span>

<span class="s">        This method can be used inside a subclassed layer or model&#39;s `call`</span>

<span class="s">        function, in which case `losses` should be a Tensor or list of Tensors.</span>

<span class="s">        Example:</span>

<span class="s">        ```python</span>

<span class="s">        class MyLayer(tf.keras.layers.Layer):</span>

<span class="s">          def call(self, inputs):</span>

<span class="s">            self.add_loss(tf.abs(tf.reduce_mean(inputs)))</span>

<span class="s">            return inputs</span>

<span class="s">        ```</span>

<span class="s">        This method can also be called directly on a Functional Model during</span>

<span class="s">        construction. In this case, any loss Tensors passed to this Model must</span>

<span class="s">        be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s">        losses become part of the model&#39;s topology and are tracked in</span>

<span class="s">        `get_config`.</span>

<span class="s">        Example:</span>

<span class="s">        ```python</span>

<span class="s">        inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">        x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s">        outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">        model = tf.keras.Model(inputs, outputs)</span>

<span class="s">        # Activity regularization.</span>

<span class="s">        model.add_loss(tf.abs(tf.reduce_mean(x)))</span>

<span class="s">        ```</span>

<span class="s">        If this is not the case for your loss (if, for example, your loss</span>

<span class="s">        references a `Variable` of one of the model&#39;s layers), you can wrap your</span>

<span class="s">        loss in a zero-argument lambda. These losses are not tracked as part of</span>

<span class="s">        the model&#39;s topology since they can&#39;t be serialized.</span>

<span class="s">        Example:</span>

<span class="s">        ```python</span>

<span class="s">        inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">        d = tf.keras.layers.Dense(10)</span>

<span class="s">        x = d(inputs)</span>

<span class="s">        outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">        model = tf.keras.Model(inputs, outputs)</span>

<span class="s">        # Weight regularization.</span>

<span class="s">        model.add_loss(lambda: tf.reduce_mean(d.kernel))</span>

<span class="s">        ```</span>

<span class="s">        Args:</span>

<span class="s">          losses: Loss tensor, or list/tuple of tensors. Rather than tensors,</span>

<span class="s">            losses may also be zero-argument callables which create a loss</span>

<span class="s">            tensor.</span>

<span class="s">          **kwargs: Used for backwards compatibility only.</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span>.<span class="nb">pop</span>(<span class="s">&quot;inputs&quot;</span>, <span class="n">None</span>)

        <span class="k">if</span> <span class="n">kwargs:</span>

            <span class="n">raise</span> <span class="n">TypeError</span>(<span class="nb">f</span><span class="s">&quot;Unknown keyword arguments: {kwargs.keys()}&quot;</span>)

        <span class="n">def</span> <span class="n">_tag_callable</span>(<span class="n">loss</span>):

            <span class="s">&quot;&quot;&quot;Tags callable loss tensor as `_unconditional_loss`.&quot;&quot;&quot;</span>

            <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

                <span class="c1"># We run the loss without autocasting, as regularizers are often</span>

                <span class="c1"># numerically unstable in float16.</span>

                <span class="k">with</span> <span class="n">autocast_variable</span>.<span class="n">enable_auto_cast_variables</span>(<span class="n">None</span>):

                    <span class="n">loss</span> = <span class="n">loss</span>()

            <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

                <span class="c1"># Will be filtered out when computing the .losses property</span>

                <span class="k">return</span> <span class="n">None</span>

            <span class="k">if</span> <span class="nb">not</span> <span class="n">tf</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

                <span class="n">loss</span> = <span class="n">tf</span>.<span class="n">convert_to_tensor</span>(<span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

            <span class="n">loss</span>.<span class="n">_unconditional_loss</span> = <span class="nb">True</span>

            <span class="k">return</span> <span class="n">loss</span>

        <span class="n">losses</span> = <span class="n">tf</span>.<span class="n">nest</span>.<span class="n">flatten</span>(<span class="n">losses</span>)

        <span class="n">callable_losses</span> = []

        <span class="n">eager_losses</span> = []

        <span class="n">symbolic_losses</span> = []

        <span class="k">for</span> <span class="n">loss</span> <span class="nb">in</span> <span class="n">losses:</span>

            <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

                <span class="n">callable_losses</span>.<span class="nb">append</span>(<span class="n">functools</span>.<span class="n">partial</span>(<span class="n">_tag_callable</span>, <span class="n">loss</span>))

                <span class="n">continue</span>

            <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

                <span class="n">continue</span>

            <span class="k">if</span> <span class="nb">not</span> <span class="n">tf</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>) <span class="o">and</span> <span class="nb">not</span> <span class="n">isinstance</span>(

                <span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>

            ):

                <span class="n">loss</span> = <span class="n">tf</span>.<span class="n">convert_to_tensor</span>(<span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

            <span class="c1"># TF Functions should take the eager path.</span>

            <span class="k">if</span> (

                <span class="n">tf_utils</span>.<span class="n">is_symbolic_tensor</span>(<span class="n">loss</span>)

                <span class="o">or</span> <span class="n">isinstance</span>(<span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>)

            ) <span class="o">and</span> <span class="nb">not</span> <span class="n">base_layer_utils</span>.<span class="n">is_in_tf_function</span>():

                <span class="n">symbolic_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

            <span class="n">elif</span> <span class="n">tf</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

                <span class="n">eager_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

        <span class="nb">self</span>.<span class="n">_callable_losses</span>.<span class="n">extend</span>(<span class="n">callable_losses</span>)

        <span class="n">in_call_context</span> = <span class="n">base_layer_utils</span>.<span class="n">call_context</span>().<span class="n">in_call</span>

        <span class="k">if</span> <span class="n">eager_losses</span> <span class="o">and</span> <span class="nb">not</span> <span class="n">in_call_context:</span>

            <span class="n">raise</span> <span class="n">ValueError</span>(

                <span class="s">&quot;Expected a symbolic Tensors or a callable for the loss value. &quot;</span>

                <span class="s">&quot;Please wrap your loss computation in a zero argument `lambda`.&quot;</span>

            )

        <span class="nb">self</span>.<span class="n">_eager_losses</span>.<span class="n">extend</span>(<span class="n">eager_losses</span>)

        <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="nb">in</span> <span class="n">symbolic_losses:</span>

            <span class="k">if</span> <span class="n">getattr</span>(<span class="nb">self</span>, <span class="s">&quot;_is_graph_network&quot;</span>, <span class="nb">False</span>):

                <span class="nb">self</span>.<span class="n">_graph_network_add_loss</span>(<span class="n">symbolic_loss</span>)

            <span class="n">else:</span>

                <span class="c1"># Possible a loss was added in a Layer&#39;s `build`.</span>

                <span class="nb">self</span>.<span class="n">_losses</span>.<span class="nb">append</span>(<span class="n">symbolic_loss</span>)
</code></pre></div>

</details>
<h4 id="add_metric">add_metric</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">value</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds metric tensor to the layer.</p>
<p>This method can be used inside the <code>call()</code> method of a subclassed layer
or model.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MyMetricLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyMetricLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;my_metric_layer&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_2&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div>

<p>This method can also be called directly on a Functional Model during
construction. In this case, any tensor passed to this Model must
be symbolic and be able to be traced back to the model's <code>Input</code>s. These
metrics become part of the model's topology and are tracked when you
save the model via <code>save()</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p>Note: Calling <code>add_metric()</code> with the result of a metric object on a
Functional Model, as shown in the example below, is not supported. This
is because we cannot trace the metric result tensor back to the model's
inputs.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>value</td>
<td>None</td>
<td>Metric tensor.</td>
<td>None</td>
</tr>
<tr>
<td>name</td>
<td>None</td>
<td>String metric name.</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>None</td>
<td>Additional keyword arguments for backward compatibility.<br>Accepted values:<br><code>aggregation</code> - When the <code>value</code> tensor provided is not the result<br>of calling a <code>keras.Metric</code> instance, it will be aggregated by<br>default using a <code>keras.Metric.Mean</code>.</td>
<td>None</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_metric</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Adds metric tensor to the layer.</span>

<span class="s2">        This method can be used inside the `call()` method of a subclassed layer</span>

<span class="s2">        or model.</span>

<span class="s2">        ```python</span>

<span class="s2">        class MyMetricLayer(tf.keras.layers.Layer):</span>

<span class="s2">          def __init__(self):</span>

<span class="s2">            super(MyMetricLayer, self).__init__(name=&#39;my_metric_layer&#39;)</span>

<span class="s2">            self.mean = tf.keras.metrics.Mean(name=&#39;metric_1&#39;)</span>

<span class="s2">          def call(self, inputs):</span>

<span class="s2">            self.add_metric(self.mean(inputs))</span>

<span class="s2">            self.add_metric(tf.reduce_sum(inputs), name=&#39;metric_2&#39;)</span>

<span class="s2">            return inputs</span>

<span class="s2">        ```</span>

<span class="s2">        This method can also be called directly on a Functional Model during</span>

<span class="s2">        construction. In this case, any tensor passed to this Model must</span>

<span class="s2">        be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s2">        metrics become part of the model&#39;s topology and are tracked when you</span>

<span class="s2">        save the model via `save()`.</span>

<span class="s2">        ```python</span>

<span class="s2">        inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">        x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">        outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">        model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">        model.add_metric(math_ops.reduce_sum(x), name=&#39;metric_1&#39;)</span>

<span class="s2">        ```</span>

<span class="s2">        Note: Calling `add_metric()` with the result of a metric object on a</span>

<span class="s2">        Functional Model, as shown in the example below, is not supported. This</span>

<span class="s2">        is because we cannot trace the metric result tensor back to the model&#39;s</span>

<span class="s2">        inputs.</span>

<span class="s2">        ```python</span>

<span class="s2">        inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">        x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">        outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">        model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">        model.add_metric(tf.keras.metrics.Mean()(x), name=&#39;metric_1&#39;)</span>

<span class="s2">        ```</span>

<span class="s2">        Args:</span>

<span class="s2">          value: Metric tensor.</span>

<span class="s2">          name: String metric name.</span>

<span class="s2">          **kwargs: Additional keyword arguments for backward compatibility.</span>

<span class="s2">            Accepted values:</span>

<span class="s2">            `aggregation` - When the `value` tensor provided is not the result</span>

<span class="s2">            of calling a `keras.Metric` instance, it will be aggregated by</span>

<span class="s2">            default using a `keras.Metric.Mean`.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="n">kwargs_keys</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">list</span><span class="p">(</span><span class="n">kwargs</span><span class="p">.</span><span class="k">keys</span><span class="p">())</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="p">(</span>

<span class="w">            </span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">kwargs_keys</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s2">&quot;aggregation&quot;</span>

<span class="w">        </span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span>

<span class="w">                </span><span class="n">f</span><span class="s2">&quot;Unknown keyword arguments: {kwargs.keys()}. &quot;</span>

<span class="w">                </span><span class="s2">&quot;Expected `aggregation`.&quot;</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hasattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;_metric_obj&quot;</span><span class="p">)</span>

<span class="w">        </span><span class="n">is_symbolic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">keras_tensor</span><span class="p">.</span><span class="n">KerasTensor</span><span class="p">)</span>

<span class="w">        </span><span class="n">in_call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">().</span><span class="n">in_call</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">name</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span>

<span class="w">            </span><span class="c1"># Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we</span>

<span class="w">            </span><span class="c1"># use metric name to lookup a metric. Without a name, a new Mean</span>

<span class="w">            </span><span class="c1"># metric wrapper will be created on every model/layer call. So, we</span>

<span class="w">            </span><span class="c1"># raise an error when no name is provided. We will do the same for</span>

<span class="w">            </span><span class="c1"># symbolic mode for consistency although a name will be generated if</span>

<span class="w">            </span><span class="c1"># no name is provided.</span>

<span class="w">            </span><span class="c1"># We will not raise this error in the foll use case for the sake of</span>

<span class="w">            </span><span class="c1"># consistency as name in provided in the metric constructor.</span>

<span class="w">            </span><span class="c1"># mean = metrics.Mean(name=&#39;my_metric&#39;)</span>

<span class="w">            </span><span class="c1"># model.add_metric(mean(outputs))</span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                </span><span class="s2">&quot;Please provide a name for your metric like &quot;</span>

<span class="w">                </span><span class="s2">&quot;`self.add_metric(tf.reduce_sum(inputs), &quot;</span>

<span class="w">                </span><span class="s2">&quot;name=&#39;mean_activation&#39;)`&quot;</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span>

<span class="w">            </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">value</span><span class="p">.</span><span class="n">_metric_obj</span><span class="p">.</span><span class="k">name</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">is_symbolic</span><span class="o">:</span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                </span><span class="s2">&quot;Expected a symbolic Tensor for the metric value, received: &quot;</span>

<span class="w">                </span><span class="o">+</span><span class="w"> </span><span class="n">str</span><span class="p">(</span><span class="k">value</span><span class="p">)</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="c1"># If a metric was added in a Layer&#39;s `call` or `build`.</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;_is_graph_network&quot;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="c1"># TF Function path should take the eager path.</span>

<span class="w">            </span><span class="c1"># If the given metric is available in `metrics` list we just update</span>

<span class="w">            </span><span class="c1"># state on it, otherwise we create a new metric instance and</span>

<span class="w">            </span><span class="c1"># add it to the `metrics` list.</span>

<span class="w">            </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;_metric_obj&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span>

<span class="w">            </span><span class="c1"># Tensors that come from a Metric object already updated the Metric</span>

<span class="w">            </span><span class="c1"># state.</span>

<span class="w">            </span><span class="n">should_update_state</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">metric_obj</span>

<span class="w">            </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric_obj</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">name</span>

<span class="w">            </span><span class="k">with</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics_lock</span><span class="o">:</span>

<span class="w">                </span><span class="k">match</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_existing_metric</span><span class="p">(</span><span class="k">name</span><span class="p">)</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="k">match</span><span class="o">:</span>

<span class="w">                    </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">match</span>

<span class="w">                </span><span class="n">elif</span><span class="w"> </span><span class="n">metric_obj</span><span class="o">:</span>

<span class="w">                    </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span>

<span class="w">                </span><span class="k">else</span><span class="o">:</span>

<span class="w">                    </span><span class="c1"># Build the metric object with the value&#39;s dtype if it</span>

<span class="w">                    </span><span class="c1"># defines one</span>

<span class="w">                    </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metrics_mod</span><span class="p">.</span><span class="n">Mean</span><span class="p">(</span>

<span class="w">                        </span><span class="k">name</span><span class="o">=</span><span class="k">name</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span>

<span class="w">                    </span><span class="p">)</span>

<span class="w">                    </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">should_update_state</span><span class="o">:</span>

<span class="w">                </span><span class="n">metric_obj</span><span class="p">(</span><span class="k">value</span><span class="p">)</span>

<span class="w">        </span><span class="k">else</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                    </span><span class="s2">&quot;Using the result of calling a `Metric` object &quot;</span>

<span class="w">                    </span><span class="s2">&quot;when calling `add_metric` on a Functional &quot;</span>

<span class="w">                    </span><span class="s2">&quot;Model is not supported. Please pass the &quot;</span>

<span class="w">                    </span><span class="s2">&quot;Tensor to monitor directly.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="c1"># Insert layers into the Keras Graph Network.</span>

<span class="w">            </span><span class="n">aggregation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s2">&quot;mean&quot;</span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">_graph_network_add_metric</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">aggregation</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="add_update">add_update</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_update</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">updates</span>
<span class="p">)</span>
</code></pre></div>

<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and
variance in a BatchNormalization layer) may be dependent on the inputs
passed when calling a layer. Hence, when reusing the same layer on
different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be
dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>This call is ignored when eager execution is enabled (in that case,
variable updates are run on the fly and thus do not need to be tracked
for later execution).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>updates</td>
<td>None</td>
<td>Update op, or list/tuple of update ops, or zero-arg callable<br>that returns an update op. A zero-arg callable should be passed in<br>order to disable running the updates by setting <code>trainable=False</code><br>on this Layer, when executing in Eager mode.</td>
<td>None</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls.do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">updates</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Add update op(s), potentially dependent on layer inputs.</span>

<span class="s2">        Weight updates (for instance, the updates of the moving mean and</span>

<span class="s2">        variance in a BatchNormalization layer) may be dependent on the inputs</span>

<span class="s2">        passed when calling a layer. Hence, when reusing the same layer on</span>

<span class="s2">        different inputs `a` and `b`, some entries in `layer.updates` may be</span>

<span class="s2">        dependent on `a` and some on `b`. This method automatically keeps track</span>

<span class="s2">        of dependencies.</span>

<span class="s2">        This call is ignored when eager execution is enabled (in that case,</span>

<span class="s2">        variable updates are run on the fly and thus do not need to be tracked</span>

<span class="s2">        for later execution).</span>

<span class="s2">        Args:</span>

<span class="s2">          updates: Update op, or list/tuple of update ops, or zero-arg callable</span>

<span class="s2">            that returns an update op. A zero-arg callable should be passed in</span>

<span class="s2">            order to disable running the updates by setting `trainable=False`</span>

<span class="s2">            on this Layer, when executing in Eager mode.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="n">call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">()</span>

<span class="w">        </span><span class="c1"># No need to run updates during Functional API construction.</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">in_keras_graph</span><span class="o">:</span>

<span class="w">            </span><span class="k">return</span>

<span class="w">        </span><span class="c1"># Callable updates are disabled by setting `trainable=False`.</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">frozen</span><span class="o">:</span>

<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="k">update</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">updates</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">callable</span><span class="p">(</span><span class="k">update</span><span class="p">)</span><span class="o">:</span>

<span class="w">                    </span><span class="k">update</span><span class="p">()</span>
</code></pre></div>

</details>
<h4 id="add_variable">add_variable</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_variable</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use! Alias for <code>add_weight</code>.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls.do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_variable</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use! Alias for `add_weight`.</span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span>

<span class="w">            </span><span class="s2">&quot;`layer.add_variable` is deprecated and &quot;</span>

<span class="w">            </span><span class="s2">&quot;will be removed in a future version. &quot;</span>

<span class="w">            </span><span class="s2">&quot;Please use the `layer.add_weight()` method instead.&quot;</span><span class="p">,</span>

<span class="w">            </span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>

<span class="w">        </span><span class="p">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="add_weight">add_weight</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synchronization</span><span class="o">=&lt;</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="o">=&lt;</span><span class="n">VariableAggregationV2</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds a new variable to the layer.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>None</td>
<td>Variable name.</td>
<td>None</td>
</tr>
<tr>
<td>shape</td>
<td>None</td>
<td>Variable shape. Defaults to scalar if unspecified.</td>
<td>scalar if unspecified</td>
</tr>
<tr>
<td>dtype</td>
<td>None</td>
<td>The type of the variable. Defaults to <code>self.dtype</code>.</td>
<td><code>self.dtype</code></td>
</tr>
<tr>
<td>initializer</td>
<td>None</td>
<td>Initializer instance (callable).</td>
<td>None</td>
</tr>
<tr>
<td>regularizer</td>
<td>None</td>
<td>Regularizer instance (callable).</td>
<td>None</td>
</tr>
<tr>
<td>trainable</td>
<td>None</td>
<td>Boolean, whether the variable should be part of the layer's<br>"trainable_variables" (e.g. variables, biases)<br>or "non_trainable_variables" (e.g. BatchNorm mean and variance).<br>Note that <code>trainable</code> cannot be <code>True</code> if <code>synchronization</code><br>is set to <code>ON_READ</code>.</td>
<td>None</td>
</tr>
<tr>
<td>constraint</td>
<td>None</td>
<td>Constraint instance (callable).</td>
<td>None</td>
</tr>
<tr>
<td>use_resource</td>
<td>None</td>
<td>Whether to use a <code>ResourceVariable</code> or not.<br>See <a href="&lt;br&gt;https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables">this guide</a><br> for more information.</td>
<td>None</td>
</tr>
<tr>
<td>synchronization</td>
<td>None</td>
<td>Indicates when a distributed a variable will be<br>aggregated. Accepted values are constants defined in the class<br><code>tf.VariableSynchronization</code>. By default the synchronization is set<br>to <code>AUTO</code> and the current <code>DistributionStrategy</code> chooses when to<br>synchronize. If <code>synchronization</code> is set to <code>ON_READ</code>, <code>trainable</code><br>must not be set to <code>True</code>.</td>
<td>None</td>
</tr>
<tr>
<td>aggregation</td>
<td>None</td>
<td>Indicates how a distributed variable will be aggregated.<br>Accepted values are constants defined in the class<br><code>tf.VariableAggregation</code>.</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>None</td>
<td>Additional keyword arguments. Accepted values are <code>getter</code>,<br><code>collections</code>, <code>experimental_autocast</code> and <code>caching_device</code>.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>The variable created.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>When giving unsupported dtype and no initializer or when<br>trainable has been set to True with synchronization set as<br><code>ON_READ</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="p">@</span><span class="n">doc_controls</span><span class="p">.</span><span class="n">for_subclass_implementers</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_weight</span><span class="p">(</span>

<span class="w">        </span><span class="nb">self</span><span class="p">,</span>

<span class="w">        </span><span class="n">name</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">shape</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">dtype</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">initializer</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">regularizer</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">trainable</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">constraint</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">use_resource</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">synchronization</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">AUTO</span><span class="p">,</span>

<span class="w">        </span><span class="n">aggregation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">VariableAggregation</span><span class="p">.</span><span class="n">NONE</span><span class="p">,</span>

<span class="w">        </span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>

<span class="w">    </span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s">&quot;&quot;&quot;Adds a new variable to the layer.</span>

<span class="w">        </span><span class="nl">Args</span><span class="p">:</span>

<span class="w">          </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="n">name</span><span class="p">.</span>

<span class="w">          </span><span class="nl">shape</span><span class="p">:</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="n">shape</span><span class="p">.</span><span class="w"> </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">scalar</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">unspecified</span><span class="p">.</span>

<span class="w">          </span><span class="nl">dtype</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="w"> </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="nb">self</span><span class="p">.</span><span class="n">dtype</span><span class="err">`</span><span class="p">.</span>

<span class="w">          </span><span class="nl">initializer</span><span class="p">:</span><span class="w"> </span><span class="n">Initializer</span><span class="w"> </span><span class="n">instance</span><span class="w"> </span><span class="p">(</span><span class="n">callable</span><span class="p">).</span>

<span class="w">          </span><span class="nl">regularizer</span><span class="p">:</span><span class="w"> </span><span class="n">Regularizer</span><span class="w"> </span><span class="n">instance</span><span class="w"> </span><span class="p">(</span><span class="n">callable</span><span class="p">).</span>

<span class="w">          </span><span class="nl">trainable</span><span class="p">:</span><span class="w"> </span><span class="kt">Boolean</span><span class="p">,</span><span class="w"> </span><span class="n">whether</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">part</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="err">&#39;</span><span class="n">s</span>

<span class="w">            </span><span class="s">&quot;trainable_variables&quot;</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="w"> </span><span class="n">variables</span><span class="p">,</span><span class="w"> </span><span class="n">biases</span><span class="p">)</span>

<span class="w">            </span><span class="n">or</span><span class="w"> </span><span class="s">&quot;non_trainable_variables&quot;</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="w"> </span><span class="n">BatchNorm</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">variance</span><span class="p">).</span>

<span class="w">            </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="err">`</span><span class="n">trainable</span><span class="err">`</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="err">`</span><span class="n">True</span><span class="err">`</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">`</span><span class="n">synchronization</span><span class="err">`</span>

<span class="w">            </span><span class="n">is</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="n">ON_READ</span><span class="err">`</span><span class="p">.</span>

<span class="w">          </span><span class="nl">constraint</span><span class="p">:</span><span class="w"> </span><span class="n">Constraint</span><span class="w"> </span><span class="n">instance</span><span class="w"> </span><span class="p">(</span><span class="n">callable</span><span class="p">).</span>

<span class="w">          </span><span class="nl">use_resource</span><span class="p">:</span><span class="w"> </span><span class="n">Whether</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="err">`</span><span class="n">ResourceVariable</span><span class="err">`</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">not</span><span class="p">.</span>

<span class="w">            </span><span class="n">See</span><span class="w"> </span><span class="p">[</span><span class="n">this</span><span class="w"> </span><span class="n">guide</span><span class="p">](</span>

<span class="w">            </span><span class="nl">https</span><span class="p">:</span><span class="c1">//www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)</span>

<span class="w">             </span><span class="k">for</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">information</span><span class="p">.</span>

<span class="w">          </span><span class="nl">synchronization</span><span class="p">:</span><span class="w"> </span><span class="n">Indicates</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">distributed</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span>

<span class="w">            </span><span class="n">aggregated</span><span class="p">.</span><span class="w"> </span><span class="n">Accepted</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">constants</span><span class="w"> </span><span class="n">defined</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">class</span>

<span class="w">            </span><span class="err">`</span><span class="n">tf</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="err">`</span><span class="p">.</span><span class="w"> </span><span class="n">By</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">synchronization</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">set</span>

<span class="w">            </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="n">AUTO</span><span class="err">`</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="err">`</span><span class="n">DistributionStrategy</span><span class="err">`</span><span class="w"> </span><span class="n">chooses</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">to</span>

<span class="w">            </span><span class="n">synchronize</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="err">`</span><span class="n">synchronization</span><span class="err">`</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="n">ON_READ</span><span class="err">`</span><span class="p">,</span><span class="w"> </span><span class="err">`</span><span class="n">trainable</span><span class="err">`</span>

<span class="w">            </span><span class="n">must</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="n">True</span><span class="err">`</span><span class="p">.</span>

<span class="w">          </span><span class="nl">aggregation</span><span class="p">:</span><span class="w"> </span><span class="n">Indicates</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">distributed</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">aggregated</span><span class="p">.</span>

<span class="w">            </span><span class="n">Accepted</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">constants</span><span class="w"> </span><span class="n">defined</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">class</span>

<span class="w">            </span><span class="err">`</span><span class="n">tf</span><span class="p">.</span><span class="n">VariableAggregation</span><span class="err">`</span><span class="p">.</span>

<span class="w">          </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Additional</span><span class="w"> </span><span class="n">keyword</span><span class="w"> </span><span class="n">arguments</span><span class="p">.</span><span class="w"> </span><span class="n">Accepted</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="err">`</span><span class="k">getter</span><span class="err">`</span><span class="p">,</span>

<span class="w">            </span><span class="err">`</span><span class="n">collections</span><span class="err">`</span><span class="p">,</span><span class="w"> </span><span class="err">`</span><span class="n">experimental_autocast</span><span class="err">`</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="err">`</span><span class="n">caching_device</span><span class="err">`</span><span class="p">.</span>

<span class="w">        </span><span class="nl">Returns</span><span class="p">:</span>

<span class="w">          </span><span class="n">The</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">created</span><span class="p">.</span>

<span class="w">        </span><span class="nl">Raises</span><span class="p">:</span>

<span class="w">          </span><span class="nl">ValueError</span><span class="p">:</span><span class="w"> </span><span class="n">When</span><span class="w"> </span><span class="n">giving</span><span class="w"> </span><span class="n">unsupported</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">initializer</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">when</span>

<span class="w">            </span><span class="n">trainable</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">synchronization</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">as</span>

<span class="w">            </span><span class="err">`</span><span class="n">ON_READ</span><span class="err">`</span><span class="p">.</span>

<span class="w">        </span><span class="s">&quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span>

<span class="w">        </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;partitioner&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">Ignored</span><span class="p">.</span>

<span class="w">        </span><span class="cp"># Validate optional keyword arguments.</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">kwargs</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="p">[</span>

<span class="w">                </span><span class="s">&quot;collections&quot;</span><span class="p">,</span>

<span class="w">                </span><span class="s">&quot;experimental_autocast&quot;</span><span class="p">,</span>

<span class="w">                </span><span class="s">&quot;caching_device&quot;</span><span class="p">,</span>

<span class="w">                </span><span class="s">&quot;getter&quot;</span><span class="p">,</span>

<span class="w">                </span><span class="s">&quot;layout&quot;</span><span class="p">,</span>

<span class="w">            </span><span class="p">]</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s">&quot;Unknown keyword argument:&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">kwarg</span><span class="p">)</span>

<span class="w">        </span><span class="n">collections_arg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;collections&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>

<span class="w">        </span><span class="cp"># &#39;experimental_autocast&#39; can be set to False by the caller to indicate</span>

<span class="w">        </span><span class="cp"># an AutoCastVariable should never be created.</span>

<span class="w">        </span><span class="n">autocast</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;experimental_autocast&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">True</span><span class="p">)</span>

<span class="w">        </span><span class="cp"># See the docstring for tf.Variable about the details for</span>

<span class="w">        </span><span class="cp"># caching_device.</span>

<span class="w">        </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;caching_device&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>

<span class="w">        </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;layout&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>

<span class="w">        </span><span class="cp"># Specially handling of auto layout fetch, based on the variable name</span>

<span class="w">        </span><span class="cp"># and attribute name. For built-in keras layers, usually the variable</span>

<span class="w">        </span><span class="cp"># name, eg &#39;kernel&#39;, will match with a &#39;kernel_layout&#39; attribute name on</span>

<span class="w">        </span><span class="cp"># the instance. We will try to do this auto fetch if layout is not</span>

<span class="w">        </span><span class="cp"># explicitly specified. This is mainly a quick workaround for not</span>

<span class="w">        </span><span class="cp"># applying too many interface change to built-in layers, until DTensor</span>

<span class="w">        </span><span class="cp"># is a public API.  Also see dtensor.utils.allow_initializer_layout for</span>

<span class="w">        </span><span class="cp"># more details.</span>

<span class="w">        </span><span class="cp"># TODO(scottzhu): Remove this once dtensor is public to end user.</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">layout</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">name</span><span class="o">:</span>

<span class="w">            </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">&quot;_layout&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">backend</span><span class="p">.</span><span class="n">floatx</span><span class="p">()</span>

<span class="w">        </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="cp"># The policy is &quot;_infer&quot;, so we infer the policy from the variable</span>

<span class="w">            </span><span class="cp"># dtype.</span>

<span class="w">            </span><span class="nb">self</span><span class="p">.</span><span class="n">_set_dtype_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">Policy</span><span class="p">(</span><span class="n">dtype</span><span class="p">.</span><span class="n">base_dtype</span><span class="p">.</span><span class="n">name</span><span class="p">))</span>

<span class="w">        </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span>

<span class="w">        </span><span class="n">regularizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">regularizers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">regularizer</span><span class="p">)</span>

<span class="w">        </span><span class="n">constraint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">constraints</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">synchronization</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">ON_READ</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                    </span><span class="s">&quot;Synchronization value can be set to &quot;</span>

<span class="w">                    </span><span class="s">&quot;VariableSynchronization.ON_READ only for non-trainable &quot;</span>

<span class="w">                    </span><span class="s">&quot;variables. You have specified trainable=True and &quot;</span>

<span class="w">                    </span><span class="s">&quot;synchronization=VariableSynchronization.ON_READ.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                </span><span class="cp"># Set trainable to be false when variable is to be synced on</span>

<span class="w">                </span><span class="cp"># read.</span>

<span class="w">                </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">False</span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">trainable</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">True</span>

<span class="w">        </span><span class="cp"># Initialize variable when no initializer provided</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">initializer</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="cp"># If dtype is DT_FLOAT, provide a uniform unit scaling initializer</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span><span class="o">:</span>

<span class="w">                </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;glorot_uniform&quot;</span><span class="p">)</span>

<span class="w">            </span><span class="cp"># If dtype is DT_INT/DT_UINT, provide a default value `zero`</span>

<span class="w">            </span><span class="cp"># If dtype is DT_BOOL, provide a default value `FALSE`</span>

<span class="w">            </span><span class="n">elif</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_integer</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_unsigned</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_bool</span><span class="o">:</span>

<span class="w">                </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;zeros&quot;</span><span class="p">)</span>

<span class="w">            </span><span class="cp"># NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX</span>

<span class="w">            </span><span class="cp"># here?</span>

<span class="w">            </span><span class="n">elif</span><span class="w"> </span><span class="s">&quot;getter&quot;</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">kwargs</span><span class="o">:</span>

<span class="w">                </span><span class="cp"># When `getter` is specified, it&#39;s possibly fine for</span>

<span class="w">                </span><span class="cp"># `initializer` to be None since it&#39;s up to the custom `getter`</span>

<span class="w">                </span><span class="cp"># to raise error in case it indeed needs `initializer`.</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                    </span><span class="n">f</span><span class="s">&quot;An initializer for variable {name} of type &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s">&quot;{dtype.base_dtype} is required for layer &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s">&quot;{self.name}. Received: {initializer}.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">        </span><span class="k">getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;getter&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">make_variable</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span>

<span class="w">            </span><span class="n">autocast</span>

<span class="w">            </span><span class="n">and</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">compute_dtype</span>

<span class="w">            </span><span class="o">!=</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span>

<span class="w">            </span><span class="n">and</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span>

<span class="w">        </span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="n">old_getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">getter</span>

<span class="w">            </span><span class="cp"># Wrap variable constructor to return an AutoCastVariable.</span>

<span class="w">            </span><span class="n">def</span><span class="w"> </span><span class="k">getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="n">autocast_variable</span><span class="p">.</span><span class="n">create_autocast_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

<span class="w">            </span><span class="cp"># Also the caching_device does not work with the mixed precision</span>

<span class="w">            </span><span class="cp"># API, disable it if it is specified.</span>

<span class="w">            </span><span class="cp"># TODO(b/142020079): Re-enable it once the bug is fixed.</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">caching_device</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">                </span><span class="n">tf_logging</span><span class="p">.</span><span class="n">warning</span><span class="p">(</span>

<span class="w">                    </span><span class="s">&quot;`caching_device` does not work with mixed precision API. &quot;</span>

<span class="w">                    </span><span class="s">&quot;Ignoring user specified `caching_device`.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">                </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">layout</span><span class="o">:</span>

<span class="w">            </span><span class="k">getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">functools</span><span class="p">.</span><span class="n">partial</span><span class="p">(</span><span class="k">getter</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="o">=</span><span class="n">layout</span><span class="p">)</span>

<span class="w">        </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_add_variable_with_custom_getter</span><span class="p">(</span>

<span class="w">            </span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>

<span class="w">            </span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>

<span class="w">            </span><span class="cp"># TODO(allenl): a `make_variable` equivalent should be added as a</span>

<span class="w">            </span><span class="cp"># `Trackable` method.</span>

<span class="w">            </span><span class="k">getter</span><span class="o">=</span><span class="k">getter</span><span class="p">,</span>

<span class="w">            </span><span class="cp"># Manage errors in Layer rather than Trackable.</span>

<span class="w">            </span><span class="n">overwrite</span><span class="o">=</span><span class="n">True</span><span class="p">,</span>

<span class="w">            </span><span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>

<span class="w">            </span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>

<span class="w">            </span><span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>

<span class="w">            </span><span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>

<span class="w">            </span><span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>

<span class="w">            </span><span class="n">collections</span><span class="o">=</span><span class="n">collections_arg</span><span class="p">,</span>

<span class="w">            </span><span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>

<span class="w">            </span><span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>

<span class="w">            </span><span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>

<span class="w">        </span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">regularizer</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="cp"># TODO(fchollet): in the future, this should be handled at the</span>

<span class="w">            </span><span class="cp"># level of variable creation, and weight regularization losses</span>

<span class="w">            </span><span class="cp"># should be variable attributes.</span>

<span class="w">            </span><span class="n">name_in_scope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="n">name</span><span class="p">[</span><span class="o">:</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="n">name</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">&quot;:&quot;</span><span class="p">)]</span>

<span class="w">            </span><span class="nb">self</span><span class="p">.</span><span class="n">_handle_weight_regularization</span><span class="p">(</span>

<span class="w">                </span><span class="n">name_in_scope</span><span class="p">,</span><span class="w"> </span><span class="n">variable</span><span class="p">,</span><span class="w"> </span><span class="n">regularizer</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">is_split_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">variable</span><span class="o">:</span>

<span class="w">                </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span>

<span class="w">                    </span><span class="nb">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="w">                </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                    </span><span class="nb">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="w">        </span><span class="nl">else</span><span class="p">:</span>

<span class="w">            </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span>

<span class="w">                </span><span class="nb">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

<span class="w">            </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                </span><span class="nb">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">variable</span>
</code></pre></div>

</details>
<h4 id="build">build</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">build</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_shape</span>
<span class="p">)</span>
</code></pre></div>

<p>Build the transformer block.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_shape</td>
<td>None</td>
<td>Shape of the input tensor</td>
<td>None</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">def</span><span class="w"> </span><span class="nv">build</span><span class="ss">(</span><span class="nv">self</span>,<span class="w"> </span><span class="nv">input_shape</span><span class="ss">)</span>:

<span class="w">        </span><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>

<span class="err">        Build the transformer block.</span>

<span class="err">        :param input_shape: Shape of the input tensor</span>

<span class="w">        </span><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>

<span class="err">        if self.size is None:</span>

<span class="err">            self.size = input_shape[-1]</span>

<span class="err">        self.att = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.size, dropout=self.mha_dropout, )</span>

<span class="err">        if self.ff_size is None:</span>

<span class="err">            self.ff_size = self.size * self.ff_mul</span>

<span class="err">        # self.ffn = self.ffn or build_dense_model(</span>

<span class="err">        #     [</span>

<span class="err">        #         self.ff_size,</span>

<span class="err">        #         Dropout(self.dropout),</span>

<span class="err">        #         self.size,</span>

<span class="err">        #         Dropout(self.dropout)</span>

<span class="err">        #     ],</span>

<span class="err">        #     activation=self.ff_act,</span>

<span class="err">        #     last_activation=None)</span>

<span class="err">        self.ffn = self.ffn or Sequential([</span>

<span class="err">            Dense(self.ff_size, activation=self.ff_act),</span>

<span class="err">            Dropout(self.dropout),</span>

<span class="err">            Dense(self.size),</span>

<span class="err">            Dropout(self.dropout)</span>

<span class="err">        ])</span>

<span class="err">        self.layernorm1 = LayerNormalization(epsilon=1e-6)</span>

<span class="err">        self.layernorm2 = LayerNormalization(epsilon=1e-6)</span>
</code></pre></div>

</details>
<h4 id="call">call</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Perform the forward pass of the transformer block.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>None</td>
<td>Input tensor</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Output tensor and attention scores if return_attention_scores is True</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">def</span><span class="w"> </span><span class="nv">call</span><span class="ss">(</span><span class="nv">self</span>,<span class="w"> </span><span class="nv">inputs</span><span class="ss">)</span>:

<span class="w">        </span><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>

<span class="err">        Perform the forward pass of the transformer block.</span>

<span class="err">        :param inputs: Input tensor</span>

<span class="err">        :return: Output tensor and attention scores if return_attention_scores is True</span>

<span class="w">        </span><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>

<span class="err">        x = self.layernorm1(inputs)</span>

<span class="err">        if self.return_attention_scores:</span>

<span class="err">            x, scores = self.att(x, x, return_attention_scores=True)</span>

<span class="err">        else:</span>

<span class="err">            x = self.att(x, x)</span>

<span class="err">        x = x + inputs</span>

<span class="err">        y = self.layernorm2(x)</span>

<span class="err">        y = self.ffn(y)</span>

<span class="err">        if self.return_attention_scores:</span>

<span class="err">            return x + y, scores</span>

<span class="err">        return x + y</span>
</code></pre></div>

</details>
<h4 id="compute_mask">compute_mask</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes an output mask tensor.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>None</td>
<td>Tensor or list of tensors.</td>
<td>None</td>
</tr>
<tr>
<td>mask</td>
<td>None</td>
<td>Tensor or list of tensors.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None or a tensor (or list of tensors,<br>one per output tensor of the layer).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@generic_utils</span><span class="p">.</span><span class="k">default</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">compute_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">mask</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Computes an output mask tensor.</span>

<span class="ss">        Args:</span>

<span class="ss">            inputs: Tensor or list of tensors.</span>

<span class="ss">            mask: Tensor or list of tensors.</span>

<span class="ss">        Returns:</span>

<span class="ss">            None or a tensor (or list of tensors,</span>

<span class="ss">                one per output tensor of the layer).</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">_supports_masking</span><span class="p">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="p">(</span><span class="n">m</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="err">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span>

<span class="w">                    </span><span class="ss">&quot;Layer &quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="ss">&quot; does not support masking, &quot;</span>

<span class="w">                    </span><span class="ss">&quot;but was passed an input_mask: &quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">str</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="nl">supported</span><span class="p">:</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">mask</span><span class="p">.</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="k">None</span>

<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="n">supported</span><span class="p">,</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="k">default</span>

<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="k">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="n">mask</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">mask</span>
</code></pre></div>

</details>
<h4 id="compute_output_shape">compute_output_shape</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_shape</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes the output shape of the layer.</p>
<p>This method will cause the layer's state to be built, if that has not
happened before. This requires that the layer will later be used with
inputs that match the input shape provided here.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_shape</td>
<td>None</td>
<td>Shape tuple (tuple of integers) or <code>tf.TensorShape</code>,<br>or structure of shape tuples / <code>tf.TensorShape</code> instances<br>(one per output tensor of the layer).<br>Shape tuples can include None for free dimensions,<br>instead of an integer.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A <code>tf.TensorShape</code> instance<br>or structure of <code>tf.TensorShape</code> instances.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">compute_output_shape</span>(<span class="nb">self</span>, <span class="n">input_shape</span>):

        <span class="s">&quot;&quot;&quot;Computes the output shape of the layer.</span>

<span class="s">        This method will cause the layer&#39;s state to be built, if that has not</span>

<span class="s">        happened before. This requires that the layer will later be used with</span>

<span class="s">        inputs that match the input shape provided here.</span>

<span class="s">        Args:</span>

<span class="s">            input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`,</span>

<span class="s">                or structure of shape tuples / `tf.TensorShape` instances</span>

<span class="s">                (one per output tensor of the layer).</span>

<span class="s">                Shape tuples can include None for free dimensions,</span>

<span class="s">                instead of an integer.</span>

<span class="s">        Returns:</span>

<span class="s">            A `tf.TensorShape` instance</span>

<span class="s">            or structure of `tf.TensorShape` instances.</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">tf</span>.<span class="n">executing_eagerly</span>():

            <span class="c1"># In this case we build the model first in order to do shape</span>

            <span class="c1"># inference.  This is acceptable because the framework only calls</span>

            <span class="c1"># `compute_output_shape` on shape values that the layer would later</span>

            <span class="c1"># be built for. It would however cause issues in case a user</span>

            <span class="c1"># attempts to use `compute_output_shape` manually with shapes that</span>

            <span class="c1"># are incompatible with the shape the Layer will be called on (these</span>

            <span class="c1"># users will have to implement `compute_output_shape` themselves).</span>

            <span class="nb">self</span>.<span class="n">_maybe_build</span>(<span class="n">input_shape</span>)

            <span class="n">graph_name</span> = <span class="n">str</span>(<span class="nb">self</span>.<span class="nb">name</span>) + <span class="s">&quot;_scratch_graph&quot;</span>

            <span class="k">with</span> <span class="n">tf</span>.<span class="n">__internal__</span>.<span class="n">FuncGraph</span>(<span class="n">graph_name</span>).<span class="n">as_default</span>():

                <span class="n">input_shape</span> = <span class="n">tf_utils</span>.<span class="n">convert_shapes</span>(

                    <span class="n">input_shape</span>, <span class="n">to_tuples</span>=<span class="nb">False</span>

                )

                <span class="n">def</span> <span class="n">_make_placeholder_like</span>(<span class="nb">shape</span>):

                    <span class="n">ph</span> = <span class="n">backend</span>.<span class="nb">placeholder</span>(<span class="nb">shape</span>=<span class="nb">shape</span>, <span class="n">dtype</span>=<span class="nb">self</span>.<span class="n">dtype</span>)

                    <span class="n">ph</span>.<span class="n">_keras_mask</span> = <span class="n">None</span>

                    <span class="k">return</span> <span class="n">ph</span>

                <span class="n">inputs</span> = <span class="n">tf</span>.<span class="n">nest</span>.<span class="n">map_structure</span>(

                    <span class="n">_make_placeholder_like</span>, <span class="n">input_shape</span>

                )

                <span class="n">try:</span>

                    <span class="n">outputs</span> = <span class="nb">self</span>(<span class="n">inputs</span>, <span class="n">training</span>=<span class="nb">False</span>)

                <span class="n">except</span> <span class="n">TypeError</span> <span class="n">as</span> <span class="n">e:</span>

                    <span class="n">raise</span> <span class="n">NotImplementedError</span>(

                        <span class="s">&quot;We could not automatically infer the static shape of &quot;</span>

                        <span class="s">&quot;the layer&#39;s output. Please implement the &quot;</span>

                        <span class="s">&quot;`compute_output_shape` method on your layer (%s).&quot;</span>

                        % <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>

                    ) <span class="nb">from</span> <span class="nb">e</span>

            <span class="k">return</span> <span class="n">tf</span>.<span class="n">nest</span>.<span class="n">map_structure</span>(<span class="n">lambda</span> <span class="n">t:</span> <span class="nb">t</span>.<span class="nb">shape</span>, <span class="n">outputs</span>)

        <span class="n">raise</span> <span class="n">NotImplementedError</span>(

            <span class="s">&quot;Please run in eager mode or implement the `compute_output_shape` &quot;</span>

            <span class="s">&quot;method on your layer (%s).&quot;</span> % <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>

        )
</code></pre></div>

</details>
<h4 id="compute_output_signature">compute_output_signature</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_signature</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_signature</span>
<span class="p">)</span>
</code></pre></div>

<p>Compute the output tensor signature of the layer based on the inputs.</p>
<p>Unlike a TensorShape object, a TensorSpec object contains both shape
and dtype information for a tensor. This method allows layers to provide
output dtype information if it is different from the input dtype.
For any layer that doesn't implement this function,
the framework will fall back to use <code>compute_output_shape</code>, and will
assume that the output dtype matches the input dtype.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_signature</td>
<td>None</td>
<td>Single TensorSpec or nested structure of TensorSpec<br>objects, describing a candidate input for the layer.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Single TensorSpec or nested structure of TensorSpec objects,<br>describing how the layer would transform the provided input.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>TypeError</td>
<td>If input_signature contains a non-TensorSpec object.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls.for_subclass_implementers</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">compute_output_signature</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Compute the output tensor signature of the layer based on the inputs.</span>

<span class="s2">        Unlike a TensorShape object, a TensorSpec object contains both shape</span>

<span class="s2">        and dtype information for a tensor. This method allows layers to provide</span>

<span class="s2">        output dtype information if it is different from the input dtype.</span>

<span class="s2">        For any layer that doesn&#39;t implement this function,</span>

<span class="s2">        the framework will fall back to use `compute_output_shape`, and will</span>

<span class="s2">        assume that the output dtype matches the input dtype.</span>

<span class="s2">        Args:</span>

<span class="s2">          input_signature: Single TensorSpec or nested structure of TensorSpec</span>

<span class="s2">            objects, describing a candidate input for the layer.</span>

<span class="s2">        Returns:</span>

<span class="s2">          Single TensorSpec or nested structure of TensorSpec objects,</span>

<span class="s2">            describing how the layer would transform the provided input.</span>

<span class="s2">        Raises:</span>

<span class="s2">          TypeError: If input_signature contains a non-TensorSpec object.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="n">def</span><span class="w"> </span><span class="n">check_type_return_shape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span>

<span class="w">                    </span><span class="s2">&quot;Only TensorSpec signature types are supported. &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s2">&quot;Received: {s}.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">shape</span>

<span class="w">        </span><span class="n">input_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span>

<span class="w">            </span><span class="n">check_type_return_shape</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span>

<span class="w">        </span><span class="p">)</span>

<span class="w">        </span><span class="n">output_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

<span class="w">        </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_compute_dtype</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span>

<span class="w">            </span><span class="n">input_dtypes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[</span><span class="n">s</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_signature</span><span class="p">)</span><span class="err">]</span>

<span class="w">            </span><span class="c1"># Default behavior when self.dtype is None, is to use the first</span>

<span class="w">            </span><span class="c1"># input&#39;s dtype.</span>

<span class="w">            </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_dtypes</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span>

<span class="w">            </span><span class="n">lambda</span><span class="w"> </span><span class="n">s</span><span class="o">:</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="o">=</span><span class="n">s</span><span class="p">),</span><span class="w"> </span><span class="n">output_shape</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="count_params">count_params</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Count the total number of scalars composing the weights.</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>An integer count.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>if the layer isn't yet built<br>(in which case its weights aren't yet defined).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Count the total number of scalars composing the weights.</span>

<span class="s2">        Returns:</span>

<span class="s2">            An integer count.</span>

<span class="s2">        Raises:</span>

<span class="s2">            ValueError: if the layer isn&#39;t yet built</span>

<span class="s2">              (in which case its weights aren&#39;t yet defined).</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">built</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;_is_graph_network&quot;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="k">with</span><span class="w"> </span><span class="n">tf_utils</span><span class="p">.</span><span class="n">maybe_init_scope</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span>

<span class="w">                    </span><span class="n">self</span><span class="p">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inputs</span><span class="p">)</span>

<span class="w">            </span><span class="k">else</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                    </span><span class="s2">&quot;You tried to call `count_params` &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s2">&quot;on layer {self.name}&quot;</span>

<span class="w">                    </span><span class="s2">&quot;, but the layer isn&#39;t built. &quot;</span>

<span class="w">                    </span><span class="s2">&quot;You can build it manually via: &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s2">&quot;`{self.name}.build(batch_input_shape)`.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">layer_utils</span><span class="p">.</span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="finalize_state">finalize_state</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">finalize_state</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Finalizes the layers state after updating layer weights.</p>
<p>This function can be subclassed in a layer and will be called after
updating a layer weights. It can be overridden to finalize any
additional layer state after a weight update.</p>
<p>This function will be called after weights of a layer have been restored
from a loaded model.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="err">@</span><span class="n">doc_controls</span><span class="o">.</span><span class="n">do_not_generate_docs</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">finalize_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;Finalizes the layers state after updating layer weights.</span>

<span class="sd">        This function can be subclassed in a layer and will be called after</span>

<span class="sd">        updating a layer weights. It can be overridden to finalize any</span>

<span class="sd">        additional layer state after a weight update.</span>

<span class="sd">        This function will be called after weights of a layer have been restored</span>

<span class="sd">        from a loaded model.</span>

<span class="sd">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">pass</span>
</code></pre></div>

</details>
<h4 id="get_config">get_config</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Python dictionary.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">def</span><span class="w"> </span><span class="nv">get_config</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

<span class="w">        </span><span class="nv">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">super</span><span class="ss">()</span>.<span class="nv">get_config</span><span class="ss">()</span>

<span class="w">        </span><span class="nv">config</span>.<span class="nv">update</span><span class="ss">(</span>{

<span class="w">            </span><span class="s2">&quot;size&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">size</span>,

<span class="w">            </span><span class="s2">&quot;num_heads&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">num_heads</span>,

<span class="w">            </span><span class="s2">&quot;ff_mul&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">ff_mul</span>,

<span class="w">            </span><span class="s2">&quot;ff_size&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">ff_size</span>,

<span class="w">            </span><span class="s2">&quot;ff_act&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">ff_act</span>,

<span class="w">            </span><span class="s2">&quot;dropout&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">dropout</span>,

<span class="w">            </span><span class="s2">&quot;mha_dropout&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">mha_dropout</span>,

<span class="w">            </span><span class="s2">&quot;return_attention_scores&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">return_attention_scores</span>,

<span class="w">            </span><span class="s2">&quot;ffn&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">ffn</span>

<span class="w">        </span>}<span class="ss">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nv">config</span>
</code></pre></div>

</details>
<h4 id="get_input_at">get_input_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first input node of the layer.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first input node of the layer.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A tensor (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">        Raises:</span>

<span class="ss">          RuntimeError: If called in Eager mode.</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span>

<span class="w">            </span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;input_tensors&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;input&quot;</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_input_mask_at">get_input_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first time the layer was called.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A mask tensor</span>

<span class="ss">            (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_input_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &quot;_keras_mask&quot;, None) for x in inputs</span><span class="o">]</span>

<span class="w">        </span><span class="k">else</span><span class="err">:</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;_keras_mask&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_input_shape_at">get_input_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the input shape(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first time the layer was called.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A shape tuple</span>

<span class="ss">            (or list of shape tuples if the layer has multiple inputs).</span>

<span class="ss">        Raises:</span>

<span class="ss">          RuntimeError: If called in Eager mode.</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span>

<span class="w">            </span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;input_shapes&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;input shape&quot;</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_output_at">get_output_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first output node of the layer.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first output node of the layer.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A tensor (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">        Raises:</span>

<span class="ss">          RuntimeError: If called in Eager mode.</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span>

<span class="w">            </span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;output_tensors&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;output&quot;</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_output_mask_at">get_output_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first time the layer was called.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A mask tensor</span>

<span class="ss">            (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_output_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &quot;_keras_mask&quot;, None) for x in output</span><span class="o">]</span>

<span class="w">        </span><span class="k">else</span><span class="err">:</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;_keras_mask&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_output_shape_at">get_output_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the output shape(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first time the layer was called.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A shape tuple</span>

<span class="ss">            (or list of shape tuples if the layer has multiple outputs).</span>

<span class="ss">        Raises:</span>

<span class="ss">          RuntimeError: If called in Eager mode.</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span>

<span class="w">            </span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;output_shapes&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;output shape&quot;</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_weights">get_weights</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Returns the current weights of the layer, as NumPy arrays.</p>
<p>The weights of a layer represent the state of the layer. This function
returns both trainable and non-trainable weight values associated with
this layer as a list of NumPy arrays, which can in turn be used to load
state into similarly parameterized layers.</p>
<p>For example, a <code>Dense</code> layer returns a list of two values: the kernel
matrix and the bias vector. These can be used to set the weights of
another <code>Dense</code> layer:</p>
<blockquote>
<blockquote>
<blockquote>
<p>layer_a = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Weights values as a list of NumPy arrays.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the current weights of the layer, as NumPy arrays.</span>

<span class="sd">        The weights of a layer represent the state of the layer. This function</span>

<span class="sd">        returns both trainable and non-trainable weight values associated with</span>

<span class="sd">        this layer as a list of NumPy arrays, which can in turn be used to load</span>

<span class="sd">        state into similarly parameterized layers.</span>

<span class="sd">        For example, a `Dense` layer returns a list of two values: the kernel</span>

<span class="sd">        matrix and the bias vector. These can be used to set the weights of</span>

<span class="sd">        another `Dense` layer:</span>

<span class="sd">        &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,</span>

<span class="sd">        ...   kernel_initializer=tf.constant_initializer(1.))</span>

<span class="sd">        &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))</span>

<span class="sd">        &gt;&gt;&gt; layer_a.get_weights()</span>

<span class="sd">        [array([[1.],</span>

<span class="sd">               [1.],</span>

<span class="sd">               [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">        &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,</span>

<span class="sd">        ...   kernel_initializer=tf.constant_initializer(2.))</span>

<span class="sd">        &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))</span>

<span class="sd">        &gt;&gt;&gt; layer_b.get_weights()</span>

<span class="sd">        [array([[2.],</span>

<span class="sd">               [2.],</span>

<span class="sd">               [2.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">        &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())</span>

<span class="sd">        &gt;&gt;&gt; layer_b.get_weights()</span>

<span class="sd">        [array([[1.],</span>

<span class="sd">               [1.],</span>

<span class="sd">               [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">        Returns:</span>

<span class="sd">            Weights values as a list of NumPy arrays.</span>

<span class="sd">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="n">weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

<span class="w">        </span><span class="n">output_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">weights</span><span class="p">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span>

<span class="w">                </span><span class="n">output_weights</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">get_tensors</span><span class="p">())</span>

<span class="w">            </span><span class="k">else</span><span class="p">:</span>

<span class="w">                </span><span class="n">output_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">backend</span><span class="o">.</span><span class="n">batch_get_value</span><span class="p">(</span><span class="n">output_weights</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="set_weights">set_weights</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">weights</span>
<span class="p">)</span>
</code></pre></div>

<p>Sets the weights of the layer, from NumPy arrays.</p>
<p>The weights of a layer represent the state of the layer. This function
sets the weight values from numpy arrays. The weight values should be
passed in the order they are created by the layer. Note that the layer's
weights must be instantiated before calling this function, by calling
the layer.</p>
<p>For example, a <code>Dense</code> layer returns a list of two values: the kernel
matrix and the bias vector. These can be used to set the weights of
another <code>Dense</code> layer:</p>
<blockquote>
<blockquote>
<blockquote>
<p>layer_a = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>weights</td>
<td>None</td>
<td>a list of NumPy arrays. The number<br>of arrays and their shape must match<br>number of the dimensions of the weights<br>of the layer (i.e. it should match the<br>output of <code>get_weights</code>).</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>If the provided weights list does not match the<br>layer's specifications.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">set_weights</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s">&quot;&quot;&quot;Sets the weights of the layer, from NumPy arrays.</span>

<span class="w">        </span><span class="n">The</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">represent</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="p">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">function</span>

<span class="w">        </span><span class="n">sets</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">numpy</span><span class="w"> </span><span class="n">arrays</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span>

<span class="w">        </span><span class="n">passed</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">order</span><span class="w"> </span><span class="n">they</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">created</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="p">.</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="err">&#39;</span><span class="n">s</span>

<span class="w">        </span><span class="n">weights</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">instantiated</span><span class="w"> </span><span class="n">before</span><span class="w"> </span><span class="n">calling</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">function</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">calling</span>

<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="p">.</span>

<span class="w">        </span><span class="n">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="err">`</span><span class="n">Dense</span><span class="err">`</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">returns</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">values</span><span class="o">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">kernel</span>

<span class="w">        </span><span class="n">matrix</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="n">vector</span><span class="p">.</span><span class="w"> </span><span class="n">These</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">of</span>

<span class="w">        </span><span class="n">another</span><span class="w"> </span><span class="err">`</span><span class="n">Dense</span><span class="err">`</span><span class="w"> </span><span class="n">layer</span><span class="o">:</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>

<span class="w">        </span><span class="p">...</span><span class="w">   </span><span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">a_out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">layer_a</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="w"> </span><span class="mf">2.</span><span class="p">,</span><span class="w"> </span><span class="mf">3.</span><span class="p">]]))</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_a</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="w">        </span><span class="p">[</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">1.</span><span class="p">]],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)]</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>

<span class="w">        </span><span class="p">...</span><span class="w">   </span><span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">2.</span><span class="p">))</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">b_out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">layer_b</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">([[</span><span class="mf">10.</span><span class="p">,</span><span class="w"> </span><span class="mf">20.</span><span class="p">,</span><span class="w"> </span><span class="mf">30.</span><span class="p">]]))</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_b</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="w">        </span><span class="p">[</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">2.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">2.</span><span class="p">]],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)]</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_b</span><span class="p">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">layer_a</span><span class="p">.</span><span class="n">get_weights</span><span class="p">())</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_b</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="w">        </span><span class="p">[</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">1.</span><span class="p">]],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)]</span>

<span class="w">        </span><span class="nl">Args</span><span class="p">:</span>

<span class="w">          </span><span class="nl">weights</span><span class="p">:</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">NumPy</span><span class="w"> </span><span class="n">arrays</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">number</span>

<span class="w">            </span><span class="n">of</span><span class="w"> </span><span class="n">arrays</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">match</span>

<span class="w">            </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">weights</span>

<span class="w">            </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">e</span><span class="p">.</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">match</span><span class="w"> </span><span class="n">the</span>

<span class="w">            </span><span class="n">output</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="err">`</span><span class="n">get_weights</span><span class="err">`</span><span class="p">).</span>

<span class="w">        </span><span class="nl">Raises</span><span class="p">:</span>

<span class="w">          </span><span class="nl">ValueError</span><span class="p">:</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">match</span><span class="w"> </span><span class="n">the</span>

<span class="w">            </span><span class="n">layer</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">specifications</span><span class="p">.</span>

<span class="w">        </span><span class="s">&quot;&quot;&quot;</span>

<span class="w">        </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">weights</span>

<span class="w">        </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">params</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">TrackableWeightHandler</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">param</span><span class="p">.</span><span class="n">num_tensors</span>

<span class="w">            </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                </span><span class="err">&#39;</span><span class="n">You</span><span class="w"> </span><span class="n">called</span><span class="w"> </span><span class="err">`</span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="err">`</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="s">&quot;%s&quot;</span><span class="w"> </span><span class="err">&#39;</span>

<span class="w">                </span><span class="s">&quot;with a weight list of length %s, but the layer was &quot;</span>

<span class="w">                </span><span class="s">&quot;expecting %s weights. Provided weights: %s...&quot;</span>

<span class="w">                </span><span class="o">%</span><span class="w"> </span><span class="p">(</span>

<span class="w">                    </span><span class="nb">self</span><span class="p">.</span><span class="n">name</span><span class="p">,</span>

<span class="w">                    </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span>

<span class="w">                    </span><span class="n">expected_num_weights</span><span class="p">,</span>

<span class="w">                    </span><span class="n">str</span><span class="p">(</span><span class="n">weights</span><span class="p">)[</span><span class="o">:</span><span class="mi">50</span><span class="p">],</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="n">weight_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>

<span class="w">        </span><span class="n">weight_value_tuples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">params</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">TrackableWeightHandler</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="n">num_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="p">.</span><span class="n">num_tensors</span>

<span class="w">                </span><span class="n">tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">num_tensors</span><span class="p">]</span>

<span class="w">                </span><span class="n">param</span><span class="p">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>

<span class="w">                </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">num_tensors</span>

<span class="w">            </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                </span><span class="n">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">]</span>

<span class="w">                </span><span class="n">weight_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">hasattr</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;shape&quot;</span><span class="p">)</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">()</span>

<span class="w">                </span><span class="n">ref_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="p">.</span><span class="n">shape</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">ref_shape</span><span class="p">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">)</span><span class="o">:</span>

<span class="w">                    </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                        </span><span class="n">f</span><span class="s">&quot;Layer {self.name} weight shape {ref_shape} &quot;</span>

<span class="w">                        </span><span class="s">&quot;is not compatible with provided weight &quot;</span>

<span class="w">                        </span><span class="n">f</span><span class="s">&quot;shape {weight_shape}.&quot;</span>

<span class="w">                    </span><span class="p">)</span>

<span class="w">                </span><span class="n">weight_value_tuples</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">))</span>

<span class="w">                </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>

<span class="w">        </span><span class="n">backend</span><span class="p">.</span><span class="n">batch_set_value</span><span class="p">(</span><span class="n">weight_value_tuples</span><span class="p">)</span>

<span class="w">        </span><span class="cp"># Perform any layer defined finalization of the layer state.</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_flatten_layers</span><span class="p">()</span><span class="o">:</span>

<span class="w">            </span><span class="n">layer</span><span class="p">.</span><span class="n">finalize_state</span><span class="p">()</span>
</code></pre></div>

</details>
<h3 id="transformerblockbase">TransformerBlockBase</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">TransformerBlockBase</span><span class="p">(</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ff_mul</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">ff_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mha_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">ff_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
    <span class="n">ffn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">return_attention_scores</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>

<p>A transformer block as defined in the "Attention Is All You Need" paper by Google.</p>
<p>It consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="n">TransformerBlockBase</span>(<span class="n">Layer</span>):

    <span class="s">&quot;&quot;&quot;</span>

<span class="s">    A transformer block as defined in the &quot;</span><span class="n">Attention</span> <span class="n">Is</span> <span class="n">All</span> <span class="n">You</span> <span class="n">Need</span><span class="s">&quot; paper by Google.</span>

<span class="s">    It consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network.</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">num_heads</span>=<span class="mi">8</span>, <span class="n">size</span>=<span class="n">None</span>, <span class="n">ff_mul</span>=<span class="mi">4</span>, <span class="n">ff_size</span>=<span class="n">None</span>, <span class="n">dropout</span>=<span class="mf">0.1</span>, <span class="n">mha_dropout</span>=<span class="mf">0.1</span>, <span class="n">ff_act</span>=<span class="s">&quot;gelu&quot;</span>,

                 <span class="n">ffn</span>=<span class="n">None</span>, <span class="n">return_attention_scores</span>=<span class="nb">False</span>):

        <span class="s">&quot;&quot;&quot;</span>

<span class="s">        Initialize the transformer block.</span>

<span class="s">        :param num_heads: Number of heads in the multi-head self-attention mechanism</span>

<span class="s">        :param size: Dimension of the input/output vectors. If None, it will be inferred from input_shape during build()</span>

<span class="s">        :param ff_mul: Multiplier for the feed-forward network size</span>

<span class="s">        :param ff_size: Size of the feed-forward network. If None, it will be inferred from size and ff_mul</span>

<span class="s">        :param dropout: Dropout rate for the feed-forward network</span>

<span class="s">        :param mha_dropout: Dropout rate for the multi-head self-attention mechanism</span>

<span class="s">        :param ff_act: Activation function for the feed-forward network</span>

<span class="s">        :param ffn: A user-specified feed-forward network. If None, a default one will be created</span>

<span class="s">        :param return_attention_scores: Whether to return attention scores</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="n">super</span>(<span class="n">TransformerBlockBase</span>, <span class="nb">self</span>).<span class="n">__init__</span>()

        <span class="nb">self</span>.<span class="n">size</span> = <span class="n">size</span>

        <span class="nb">self</span>.<span class="n">num_heads</span> = <span class="n">num_heads</span>

        <span class="nb">self</span>.<span class="n">ff_mul</span> = <span class="n">ff_mul</span>

        <span class="nb">self</span>.<span class="n">ff_size</span> = <span class="n">ff_size</span>

        <span class="nb">self</span>.<span class="n">ff_act</span> = <span class="n">ff_act</span>

        <span class="nb">self</span>.<span class="n">dropout</span> = <span class="n">dropout</span>

        <span class="nb">self</span>.<span class="n">mha_dropout</span> = <span class="n">mha_dropout</span>

        <span class="nb">self</span>.<span class="n">return_attention_scores</span>= <span class="n">return_attention_scores</span>

        <span class="nb">self</span>.<span class="n">ffn</span> = <span class="n">ffn</span>

    <span class="n">def</span> <span class="n">build</span>(<span class="nb">self</span>, <span class="n">input_shape</span>):

        <span class="s">&quot;&quot;&quot;</span>

<span class="s">        Build the transformer block.</span>

<span class="s">        :param input_shape: Shape of the input tensor</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">self</span>.<span class="n">size</span> <span class="k">is</span> <span class="n">None:</span>

            <span class="nb">self</span>.<span class="n">size</span> = <span class="n">input_shape</span>[-<span class="mi">1</span>]

        <span class="nb">self</span>.<span class="n">att</span> = <span class="n">MultiHeadAttention</span>(<span class="n">num_heads</span>=<span class="nb">self</span>.<span class="n">num_heads</span>, <span class="n">key_dim</span>=<span class="nb">self</span>.<span class="n">size</span>, <span class="n">dropout</span>=<span class="nb">self</span>.<span class="n">mha_dropout</span>, )

        <span class="k">if</span> <span class="nb">self</span>.<span class="n">ff_size</span> <span class="k">is</span> <span class="n">None:</span>

            <span class="nb">self</span>.<span class="n">ff_size</span> = <span class="nb">self</span>.<span class="n">size</span> * <span class="nb">self</span>.<span class="n">ff_mul</span>

        <span class="c1"># self.ffn = self.ffn or build_dense_model(</span>

        <span class="c1">#     [</span>

        <span class="c1">#         self.ff_size,</span>

        <span class="c1">#         Dropout(self.dropout),</span>

        <span class="c1">#         self.size,</span>

        <span class="c1">#         Dropout(self.dropout)</span>

        <span class="c1">#     ],</span>

        <span class="c1">#     activation=self.ff_act,</span>

        <span class="c1">#     last_activation=None)</span>

        <span class="nb">self</span>.<span class="n">ffn</span> = <span class="nb">self</span>.<span class="n">ffn</span> <span class="o">or</span> <span class="n">Sequential</span>([

            <span class="n">Dense</span>(<span class="nb">self</span>.<span class="n">ff_size</span>, <span class="n">activation</span>=<span class="nb">self</span>.<span class="n">ff_act</span>),

            <span class="n">Dropout</span>(<span class="nb">self</span>.<span class="n">dropout</span>),

            <span class="n">Dense</span>(<span class="nb">self</span>.<span class="n">size</span>),

            <span class="n">Dropout</span>(<span class="nb">self</span>.<span class="n">dropout</span>)

        ])

        <span class="nb">self</span>.<span class="n">layernorm1</span> = <span class="n">LayerNormalization</span>(<span class="n">epsilon</span>=<span class="mf">1e-6</span>)

        <span class="nb">self</span>.<span class="n">layernorm2</span> = <span class="n">LayerNormalization</span>(<span class="n">epsilon</span>=<span class="mf">1e-6</span>)

    <span class="n">def</span> <span class="n">call</span>(<span class="nb">self</span>, <span class="n">inputs</span>):

        <span class="s">&quot;&quot;&quot;</span>

<span class="s">        Perform the forward pass of the transformer block.</span>

<span class="s">        :param inputs: Input tensor</span>

<span class="s">        :return: Output tensor and attention scores if return_attention_scores is True</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">self</span>.<span class="n">return_attention_scores:</span>

            <span class="nb">x</span>, <span class="n">scores</span> = <span class="nb">self</span>.<span class="n">att</span>(<span class="n">inputs</span>, <span class="n">inputs</span>, <span class="n">return_attention_scores</span>=<span class="nb">True</span>)

        <span class="n">else:</span>

            <span class="nb">x</span> = <span class="nb">self</span>.<span class="n">att</span>(<span class="n">inputs</span>, <span class="n">inputs</span>)

        <span class="nb">x</span> = <span class="nb">self</span>.<span class="n">layernorm1</span>(<span class="nb">x</span> + <span class="n">inputs</span>)

        <span class="n">y</span> = <span class="nb">self</span>.<span class="n">ffn</span>(<span class="nb">x</span>)

        <span class="k">if</span> <span class="nb">self</span>.<span class="n">return_attention_scores:</span>

            <span class="k">return</span> <span class="nb">self</span>.<span class="n">layernorm2</span>(<span class="nb">x</span> + <span class="n">y</span>), <span class="n">scores</span>

        <span class="k">return</span> <span class="nb">self</span>.<span class="n">layernorm2</span>(<span class="nb">x</span> + <span class="n">y</span>)

    <span class="n">def</span> <span class="n">get_config</span>(<span class="nb">self</span>):

        <span class="nb">config</span> = <span class="n">super</span>().<span class="n">get_config</span>()

        <span class="nb">config</span>.<span class="n">update</span>({

            <span class="s">&quot;size&quot;</span>: <span class="nb">self</span>.<span class="n">size</span>,

            <span class="s">&quot;num_heads&quot;</span>: <span class="nb">self</span>.<span class="n">num_heads</span>,

            <span class="s">&quot;ff_mul&quot;</span>: <span class="nb">self</span>.<span class="n">ff_mul</span>,

            <span class="s">&quot;ff_size&quot;</span>: <span class="nb">self</span>.<span class="n">ff_size</span>,

            <span class="s">&quot;ff_act&quot;</span>: <span class="nb">self</span>.<span class="n">ff_act</span>,

            <span class="s">&quot;dropout&quot;</span>: <span class="nb">self</span>.<span class="n">dropout</span>,

            <span class="s">&quot;mha_dropout&quot;</span>: <span class="nb">self</span>.<span class="n">mha_dropout</span>,

            <span class="s">&quot;return_attention_scores&quot;</span>: <span class="nb">self</span>.<span class="n">return_attention_scores</span>,

            <span class="s">&quot;ffn&quot;</span>: <span class="nb">self</span>.<span class="n">ffn</span>

        })

        <span class="k">return</span> <span class="nb">config</span>
</code></pre></div>

</details>
<hr />
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h4 id="descendants">Descendants</h4>
<ul>
<li>grid_transformer.transformer_block.TransformerBlock</li>
</ul>
<h4 id="static-methods_1">Static methods</h4>
<h4 id="from_config_1">from_config</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">from_config</span><span class="p">(</span>
    <span class="n">config</span>
<span class="p">)</span>
</code></pre></div>

<p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <code>set_weights</code>).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>config</td>
<td>None</td>
<td>A Python dictionary, typically the<br>output of get_config.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A layer instance.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@classmethod</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">from_config</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Creates a layer from its config.</span>

<span class="s2">        This method is the reverse of `get_config`,</span>

<span class="s2">        capable of instantiating the same layer from the config</span>

<span class="s2">        dictionary. It does not handle layer connectivity</span>

<span class="s2">        (handled by Network), nor weights (handled by `set_weights`).</span>

<span class="s2">        Args:</span>

<span class="s2">            config: A Python dictionary, typically the</span>

<span class="s2">                output of get_config.</span>

<span class="s2">        Returns:</span>

<span class="s2">            A layer instance.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">cls</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="with_name_scope_1">with_name_scope</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">with_name_scope</span><span class="p">(</span>
    <span class="n">method</span>
<span class="p">)</span>
</code></pre></div>

<p>Decorator to automatically enter the module name scope.</p>
<blockquote>
<blockquote>
<blockquote>
<p>class MyModule(tf.Module):
...   @tf.Module.with_name_scope
...   def <strong>call</strong>(self, x):
...     if not hasattr(self, 'w'):
...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))
...     return tf.matmul(x, self.w)</p>
</blockquote>
</blockquote>
</blockquote>
<p>Using the above module would produce <code>tf.Variable</code>s and <code>tf.Tensor</code>s whose
names included the module name:</p>
<blockquote>
<blockquote>
<blockquote>
<p>mod = MyModule()
mod(tf.ones([1, 2]))
<tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>
mod.w
<tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,
numpy=..., dtype=float32)></p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>method</td>
<td>None</td>
<td>The method to wrap.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>The original method wrapped such that it enters the module's name scope.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@classmethod</span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">with_name_scope</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span><span class="w"> </span><span class="k">method</span><span class="p">)</span><span class="err">:</span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Decorator to automatically enter the module name scope.</span>

<span class="ss">    &gt;&gt;&gt; class MyModule(tf.Module):</span>

<span class="ss">    ...   @tf.Module.with_name_scope</span>

<span class="ss">    ...   def __call__(self, x):</span>

<span class="ss">    ...     if not hasattr(self, &#39;w&#39;):</span>

<span class="ss">    ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))</span>

<span class="ss">    ...     return tf.matmul(x, self.w)</span>

<span class="ss">    Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose</span>

<span class="ss">    names included the module name:</span>

<span class="ss">    &gt;&gt;&gt; mod = MyModule()</span>

<span class="ss">    &gt;&gt;&gt; mod(tf.ones([1, 2]))</span>

<span class="ss">    &lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)&gt;</span>

<span class="ss">    &gt;&gt;&gt; mod.w</span>

<span class="ss">    &lt;tf.Variable &#39;my_module/Variable:0&#39; shape=(2, 3) dtype=float32,</span>

<span class="ss">    numpy=..., dtype=float32)&gt;</span>

<span class="ss">    Args:</span>

<span class="ss">      method: The method to wrap.</span>

<span class="ss">    Returns:</span>

<span class="ss">      The original method wrapped such that it enters the module&#39;s name scope.</span>

<span class="ss">    &quot;&quot;&quot;</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">method_with_name_scope</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="err">:</span>

<span class="w">      </span><span class="k">with</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">name_scope</span><span class="p">:</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">method</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">tf_decorator</span><span class="p">.</span><span class="n">make_decorator</span><span class="p">(</span><span class="k">method</span><span class="p">,</span><span class="w"> </span><span class="n">method_with_name_scope</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="instance-variables_1">Instance variables</h4>
<div class="codehilite"><pre><span></span><code><span class="n">activity_regularizer</span>
</code></pre></div>

<p>Optional regularizer function for the output of this layer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">compute_dtype</span>
</code></pre></div>

<p>The dtype of the layer's computations.</p>
<p>This is equivalent to <code>Layer.dtype_policy.compute_dtype</code>. Unless
mixed precision is used, this is the same as <code>Layer.dtype</code>, the dtype of
the weights.</p>
<p>Layers automatically cast their inputs to the compute dtype, which
causes computations and the output to be in the compute dtype as well.
This is done by the base Layer class in <code>Layer.__call__</code>, so you do not
have to insert these casts if implementing your own layer.</p>
<p>Layers often perform certain internal computations in higher precision
when <code>compute_dtype</code> is float16 or bfloat16 for numeric stability. The
output will still typically be float16 or bfloat16 in such cases.</p>
<div class="codehilite"><pre><span></span><code><span class="n">dtype</span>
</code></pre></div>

<p>The dtype of the layer weights.</p>
<p>This is equivalent to <code>Layer.dtype_policy.variable_dtype</code>. Unless
mixed precision is used, this is the same as <code>Layer.compute_dtype</code>, the
dtype of the layer's computations.</p>
<div class="codehilite"><pre><span></span><code><span class="n">dtype_policy</span>
</code></pre></div>

<p>The dtype policy associated with this layer.</p>
<p>This is an instance of a <code>tf.keras.mixed_precision.Policy</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">dynamic</span>
</code></pre></div>

<p>Whether the layer is dynamic (eager-only); set in the constructor.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inbound_nodes</span>
</code></pre></div>

<p>Return Functional API nodes upstream of this layer.</p>
<div class="codehilite"><pre><span></span><code><span class="nb">input</span>
</code></pre></div>

<p>Retrieves the input tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one input,
i.e. if it is connected to one incoming layer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">input_mask</span>
</code></pre></div>

<p>Retrieves the input mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<p>Returns:
    Input mask tensor (potentially None) or list of input
    mask tensors.</p>
<p>Raises:
    AttributeError: if the layer is connected to
    more than one incoming layers.</p>
<div class="codehilite"><pre><span></span><code><span class="n">input_shape</span>
</code></pre></div>

<p>Retrieves the input shape(s) of a layer.</p>
<p>Only applicable if the layer has exactly one input,
i.e. if it is connected to one incoming layer, or if all inputs
have the same shape.</p>
<div class="codehilite"><pre><span></span><code><span class="n">input_spec</span>
</code></pre></div>

<p><code>InputSpec</code> instance(s) describing the input format for this layer.</p>
<p>When you create a layer subclass, you can set <code>self.input_spec</code> to
enable the layer to run input compatibility checks when it is called.
Consider a <code>Conv2D</code> layer: it can only be called on a single input
tensor of rank 4. As such, you can set, in <code>__init__()</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputSpec</span><span class="p">(</span><span class="n">ndim</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>

<p>Now, if you try to call the layer on an input that isn't rank 4
(for instance, an input of shape <code>(2,)</code>, it will raise a
nicely-formatted error:</p>
<div class="codehilite"><pre><span></span><code><span class="n">ValueError</span><span class="o">:</span><span class="w"> </span><span class="n">Input</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">conv2d</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">incompatible</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="o">:</span>
<span class="n">expected</span><span class="w"> </span><span class="n">ndim</span><span class="o">=</span><span class="mi">4</span><span class="o">,</span><span class="w"> </span><span class="n">found</span><span class="w"> </span><span class="n">ndim</span><span class="o">=</span><span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="n">Full</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">received</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="mi">2</span><span class="o">]</span>
</code></pre></div>

<p>Input checks that can be specified via <code>input_spec</code> include:
- Structure (e.g. a single input, a list of 2 inputs, etc)
- Shape
- Rank (ndim)
- Dtype</p>
<p>For more information, see <code>tf.keras.layers.InputSpec</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">losses</span>
</code></pre></div>

<p>List of losses added using the <code>add_loss()</code> API.</p>
<p>Variable regularization tensors are created when this property is
accessed, so it is eager safe: accessing <code>losses</code> under a
<code>tf.GradientTape</code> will propagate gradients back to the corresponding
variables.</p>
<div class="codehilite"><pre><span></span><code><span class="n">metrics</span>
</code></pre></div>

<p>List of metrics added using the <code>add_metric()</code> API.</p>
<div class="codehilite"><pre><span></span><code><span class="n">name</span>
</code></pre></div>

<p>Name of the layer (string), set in the constructor.</p>
<div class="codehilite"><pre><span></span><code><span class="n">name_scope</span>
</code></pre></div>

<p>Returns a <code>tf.name_scope</code> instance for this class.</p>
<div class="codehilite"><pre><span></span><code><span class="n">non_trainable_variables</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">non_trainable_weights</span>
</code></pre></div>

<p>List of all non-trainable weights tracked by this layer.</p>
<p>Non-trainable weights are <em>not</em> updated during training. They are
expected to be updated manually in <code>call()</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">outbound_nodes</span>
</code></pre></div>

<p>Return Functional API nodes downstream of this layer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">output</span>
</code></pre></div>

<p>Retrieves the output tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one output,
i.e. if it is connected to one incoming layer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">output_mask</span>
</code></pre></div>

<p>Retrieves the output mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<p>Returns:
    Output mask tensor (potentially None) or list of output
    mask tensors.</p>
<p>Raises:
    AttributeError: if the layer is connected to
    more than one incoming layers.</p>
<div class="codehilite"><pre><span></span><code><span class="n">output_shape</span>
</code></pre></div>

<p>Retrieves the output shape(s) of a layer.</p>
<p>Only applicable if the layer has one output,
or if all outputs have the same shape.</p>
<div class="codehilite"><pre><span></span><code><span class="n">stateful</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">submodules</span>
</code></pre></div>

<p>Sequence of all sub-modules.</p>
<p>Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).</p>
<blockquote>
<blockquote>
<blockquote>
<p>a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
list(a.submodules) == [b, c]
True
list(b.submodules) == [c]
True
list(c.submodules) == []
True</p>
</blockquote>
</blockquote>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">supports_masking</span>
</code></pre></div>

<p>Whether this layer supports computing a mask using <code>compute_mask</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">trainable</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">trainable_variables</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">trainable_weights</span>
</code></pre></div>

<p>List of all trainable weights tracked by this layer.</p>
<p>Trainable weights are updated via gradient descent during training.</p>
<div class="codehilite"><pre><span></span><code><span class="n">updates</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">variable_dtype</span>
</code></pre></div>

<p>Alias of <code>Layer.dtype</code>, the dtype of the weights.</p>
<div class="codehilite"><pre><span></span><code><span class="n">variables</span>
</code></pre></div>

<p>Returns the list of all layer variables/weights.</p>
<p>Alias of <code>self.weights</code>.</p>
<p>Note: This will not track the weights of nested <code>tf.Modules</code> that are
not themselves Keras layers.</p>
<div class="codehilite"><pre><span></span><code><span class="n">weights</span>
</code></pre></div>

<p>Returns the list of all layer variables/weights.</p>
<h4 id="methods_1">Methods</h4>
<h4 id="add_loss_1">add_loss</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">losses</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be
dependent on the inputs passed when calling a layer. Hence, when reusing
the same layer on different inputs <code>a</code> and <code>b</code>, some entries in
<code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method
automatically keeps track of dependencies.</p>
<p>This method can be used inside a subclassed layer or model's <code>call</code>
function, in which case <code>losses</code> should be a Tensor or list of Tensors.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>losses</td>
<td>None</td>
<td>Loss tensor, or list/tuple of tensors. Rather than tensors,<br>losses may also be zero-argument callables which create a loss<br>tensor.</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>None</td>
<td>Used for backwards compatibility only.</td>
<td>None</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">add_loss</span>(<span class="nb">self</span>, <span class="n">losses</span>, **<span class="n">kwargs</span>):

        <span class="s">&quot;&quot;&quot;Add loss tensor(s), potentially dependent on layer inputs.</span>

<span class="s">        Some losses (for instance, activity regularization losses) may be</span>

<span class="s">        dependent on the inputs passed when calling a layer. Hence, when reusing</span>

<span class="s">        the same layer on different inputs `a` and `b`, some entries in</span>

<span class="s">        `layer.losses` may be dependent on `a` and some on `b`. This method</span>

<span class="s">        automatically keeps track of dependencies.</span>

<span class="s">        This method can be used inside a subclassed layer or model&#39;s `call`</span>

<span class="s">        function, in which case `losses` should be a Tensor or list of Tensors.</span>

<span class="s">        Example:</span>

<span class="s">        ```python</span>

<span class="s">        class MyLayer(tf.keras.layers.Layer):</span>

<span class="s">          def call(self, inputs):</span>

<span class="s">            self.add_loss(tf.abs(tf.reduce_mean(inputs)))</span>

<span class="s">            return inputs</span>

<span class="s">        ```</span>

<span class="s">        This method can also be called directly on a Functional Model during</span>

<span class="s">        construction. In this case, any loss Tensors passed to this Model must</span>

<span class="s">        be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s">        losses become part of the model&#39;s topology and are tracked in</span>

<span class="s">        `get_config`.</span>

<span class="s">        Example:</span>

<span class="s">        ```python</span>

<span class="s">        inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">        x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s">        outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">        model = tf.keras.Model(inputs, outputs)</span>

<span class="s">        # Activity regularization.</span>

<span class="s">        model.add_loss(tf.abs(tf.reduce_mean(x)))</span>

<span class="s">        ```</span>

<span class="s">        If this is not the case for your loss (if, for example, your loss</span>

<span class="s">        references a `Variable` of one of the model&#39;s layers), you can wrap your</span>

<span class="s">        loss in a zero-argument lambda. These losses are not tracked as part of</span>

<span class="s">        the model&#39;s topology since they can&#39;t be serialized.</span>

<span class="s">        Example:</span>

<span class="s">        ```python</span>

<span class="s">        inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">        d = tf.keras.layers.Dense(10)</span>

<span class="s">        x = d(inputs)</span>

<span class="s">        outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">        model = tf.keras.Model(inputs, outputs)</span>

<span class="s">        # Weight regularization.</span>

<span class="s">        model.add_loss(lambda: tf.reduce_mean(d.kernel))</span>

<span class="s">        ```</span>

<span class="s">        Args:</span>

<span class="s">          losses: Loss tensor, or list/tuple of tensors. Rather than tensors,</span>

<span class="s">            losses may also be zero-argument callables which create a loss</span>

<span class="s">            tensor.</span>

<span class="s">          **kwargs: Used for backwards compatibility only.</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="n">kwargs</span>.<span class="nb">pop</span>(<span class="s">&quot;inputs&quot;</span>, <span class="n">None</span>)

        <span class="k">if</span> <span class="n">kwargs:</span>

            <span class="n">raise</span> <span class="n">TypeError</span>(<span class="nb">f</span><span class="s">&quot;Unknown keyword arguments: {kwargs.keys()}&quot;</span>)

        <span class="n">def</span> <span class="n">_tag_callable</span>(<span class="n">loss</span>):

            <span class="s">&quot;&quot;&quot;Tags callable loss tensor as `_unconditional_loss`.&quot;&quot;&quot;</span>

            <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

                <span class="c1"># We run the loss without autocasting, as regularizers are often</span>

                <span class="c1"># numerically unstable in float16.</span>

                <span class="k">with</span> <span class="n">autocast_variable</span>.<span class="n">enable_auto_cast_variables</span>(<span class="n">None</span>):

                    <span class="n">loss</span> = <span class="n">loss</span>()

            <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

                <span class="c1"># Will be filtered out when computing the .losses property</span>

                <span class="k">return</span> <span class="n">None</span>

            <span class="k">if</span> <span class="nb">not</span> <span class="n">tf</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

                <span class="n">loss</span> = <span class="n">tf</span>.<span class="n">convert_to_tensor</span>(<span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

            <span class="n">loss</span>.<span class="n">_unconditional_loss</span> = <span class="nb">True</span>

            <span class="k">return</span> <span class="n">loss</span>

        <span class="n">losses</span> = <span class="n">tf</span>.<span class="n">nest</span>.<span class="n">flatten</span>(<span class="n">losses</span>)

        <span class="n">callable_losses</span> = []

        <span class="n">eager_losses</span> = []

        <span class="n">symbolic_losses</span> = []

        <span class="k">for</span> <span class="n">loss</span> <span class="nb">in</span> <span class="n">losses:</span>

            <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

                <span class="n">callable_losses</span>.<span class="nb">append</span>(<span class="n">functools</span>.<span class="n">partial</span>(<span class="n">_tag_callable</span>, <span class="n">loss</span>))

                <span class="n">continue</span>

            <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

                <span class="n">continue</span>

            <span class="k">if</span> <span class="nb">not</span> <span class="n">tf</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>) <span class="o">and</span> <span class="nb">not</span> <span class="n">isinstance</span>(

                <span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>

            ):

                <span class="n">loss</span> = <span class="n">tf</span>.<span class="n">convert_to_tensor</span>(<span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

            <span class="c1"># TF Functions should take the eager path.</span>

            <span class="k">if</span> (

                <span class="n">tf_utils</span>.<span class="n">is_symbolic_tensor</span>(<span class="n">loss</span>)

                <span class="o">or</span> <span class="n">isinstance</span>(<span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>)

            ) <span class="o">and</span> <span class="nb">not</span> <span class="n">base_layer_utils</span>.<span class="n">is_in_tf_function</span>():

                <span class="n">symbolic_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

            <span class="n">elif</span> <span class="n">tf</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

                <span class="n">eager_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

        <span class="nb">self</span>.<span class="n">_callable_losses</span>.<span class="n">extend</span>(<span class="n">callable_losses</span>)

        <span class="n">in_call_context</span> = <span class="n">base_layer_utils</span>.<span class="n">call_context</span>().<span class="n">in_call</span>

        <span class="k">if</span> <span class="n">eager_losses</span> <span class="o">and</span> <span class="nb">not</span> <span class="n">in_call_context:</span>

            <span class="n">raise</span> <span class="n">ValueError</span>(

                <span class="s">&quot;Expected a symbolic Tensors or a callable for the loss value. &quot;</span>

                <span class="s">&quot;Please wrap your loss computation in a zero argument `lambda`.&quot;</span>

            )

        <span class="nb">self</span>.<span class="n">_eager_losses</span>.<span class="n">extend</span>(<span class="n">eager_losses</span>)

        <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="nb">in</span> <span class="n">symbolic_losses:</span>

            <span class="k">if</span> <span class="n">getattr</span>(<span class="nb">self</span>, <span class="s">&quot;_is_graph_network&quot;</span>, <span class="nb">False</span>):

                <span class="nb">self</span>.<span class="n">_graph_network_add_loss</span>(<span class="n">symbolic_loss</span>)

            <span class="n">else:</span>

                <span class="c1"># Possible a loss was added in a Layer&#39;s `build`.</span>

                <span class="nb">self</span>.<span class="n">_losses</span>.<span class="nb">append</span>(<span class="n">symbolic_loss</span>)
</code></pre></div>

</details>
<h4 id="add_metric_1">add_metric</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">value</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds metric tensor to the layer.</p>
<p>This method can be used inside the <code>call()</code> method of a subclassed layer
or model.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MyMetricLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyMetricLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;my_metric_layer&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_2&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div>

<p>This method can also be called directly on a Functional Model during
construction. In this case, any tensor passed to this Model must
be symbolic and be able to be traced back to the model's <code>Input</code>s. These
metrics become part of the model's topology and are tracked when you
save the model via <code>save()</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p>Note: Calling <code>add_metric()</code> with the result of a metric object on a
Functional Model, as shown in the example below, is not supported. This
is because we cannot trace the metric result tensor back to the model's
inputs.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>value</td>
<td>None</td>
<td>Metric tensor.</td>
<td>None</td>
</tr>
<tr>
<td>name</td>
<td>None</td>
<td>String metric name.</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>None</td>
<td>Additional keyword arguments for backward compatibility.<br>Accepted values:<br><code>aggregation</code> - When the <code>value</code> tensor provided is not the result<br>of calling a <code>keras.Metric</code> instance, it will be aggregated by<br>default using a <code>keras.Metric.Mean</code>.</td>
<td>None</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_metric</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Adds metric tensor to the layer.</span>

<span class="s2">        This method can be used inside the `call()` method of a subclassed layer</span>

<span class="s2">        or model.</span>

<span class="s2">        ```python</span>

<span class="s2">        class MyMetricLayer(tf.keras.layers.Layer):</span>

<span class="s2">          def __init__(self):</span>

<span class="s2">            super(MyMetricLayer, self).__init__(name=&#39;my_metric_layer&#39;)</span>

<span class="s2">            self.mean = tf.keras.metrics.Mean(name=&#39;metric_1&#39;)</span>

<span class="s2">          def call(self, inputs):</span>

<span class="s2">            self.add_metric(self.mean(inputs))</span>

<span class="s2">            self.add_metric(tf.reduce_sum(inputs), name=&#39;metric_2&#39;)</span>

<span class="s2">            return inputs</span>

<span class="s2">        ```</span>

<span class="s2">        This method can also be called directly on a Functional Model during</span>

<span class="s2">        construction. In this case, any tensor passed to this Model must</span>

<span class="s2">        be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s2">        metrics become part of the model&#39;s topology and are tracked when you</span>

<span class="s2">        save the model via `save()`.</span>

<span class="s2">        ```python</span>

<span class="s2">        inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">        x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">        outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">        model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">        model.add_metric(math_ops.reduce_sum(x), name=&#39;metric_1&#39;)</span>

<span class="s2">        ```</span>

<span class="s2">        Note: Calling `add_metric()` with the result of a metric object on a</span>

<span class="s2">        Functional Model, as shown in the example below, is not supported. This</span>

<span class="s2">        is because we cannot trace the metric result tensor back to the model&#39;s</span>

<span class="s2">        inputs.</span>

<span class="s2">        ```python</span>

<span class="s2">        inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">        x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">        outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">        model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">        model.add_metric(tf.keras.metrics.Mean()(x), name=&#39;metric_1&#39;)</span>

<span class="s2">        ```</span>

<span class="s2">        Args:</span>

<span class="s2">          value: Metric tensor.</span>

<span class="s2">          name: String metric name.</span>

<span class="s2">          **kwargs: Additional keyword arguments for backward compatibility.</span>

<span class="s2">            Accepted values:</span>

<span class="s2">            `aggregation` - When the `value` tensor provided is not the result</span>

<span class="s2">            of calling a `keras.Metric` instance, it will be aggregated by</span>

<span class="s2">            default using a `keras.Metric.Mean`.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="n">kwargs_keys</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">list</span><span class="p">(</span><span class="n">kwargs</span><span class="p">.</span><span class="k">keys</span><span class="p">())</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="p">(</span>

<span class="w">            </span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">kwargs_keys</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s2">&quot;aggregation&quot;</span>

<span class="w">        </span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span>

<span class="w">                </span><span class="n">f</span><span class="s2">&quot;Unknown keyword arguments: {kwargs.keys()}. &quot;</span>

<span class="w">                </span><span class="s2">&quot;Expected `aggregation`.&quot;</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hasattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;_metric_obj&quot;</span><span class="p">)</span>

<span class="w">        </span><span class="n">is_symbolic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">keras_tensor</span><span class="p">.</span><span class="n">KerasTensor</span><span class="p">)</span>

<span class="w">        </span><span class="n">in_call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">().</span><span class="n">in_call</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">name</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span>

<span class="w">            </span><span class="c1"># Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we</span>

<span class="w">            </span><span class="c1"># use metric name to lookup a metric. Without a name, a new Mean</span>

<span class="w">            </span><span class="c1"># metric wrapper will be created on every model/layer call. So, we</span>

<span class="w">            </span><span class="c1"># raise an error when no name is provided. We will do the same for</span>

<span class="w">            </span><span class="c1"># symbolic mode for consistency although a name will be generated if</span>

<span class="w">            </span><span class="c1"># no name is provided.</span>

<span class="w">            </span><span class="c1"># We will not raise this error in the foll use case for the sake of</span>

<span class="w">            </span><span class="c1"># consistency as name in provided in the metric constructor.</span>

<span class="w">            </span><span class="c1"># mean = metrics.Mean(name=&#39;my_metric&#39;)</span>

<span class="w">            </span><span class="c1"># model.add_metric(mean(outputs))</span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                </span><span class="s2">&quot;Please provide a name for your metric like &quot;</span>

<span class="w">                </span><span class="s2">&quot;`self.add_metric(tf.reduce_sum(inputs), &quot;</span>

<span class="w">                </span><span class="s2">&quot;name=&#39;mean_activation&#39;)`&quot;</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span>

<span class="w">            </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">value</span><span class="p">.</span><span class="n">_metric_obj</span><span class="p">.</span><span class="k">name</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">is_symbolic</span><span class="o">:</span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                </span><span class="s2">&quot;Expected a symbolic Tensor for the metric value, received: &quot;</span>

<span class="w">                </span><span class="o">+</span><span class="w"> </span><span class="n">str</span><span class="p">(</span><span class="k">value</span><span class="p">)</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="c1"># If a metric was added in a Layer&#39;s `call` or `build`.</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;_is_graph_network&quot;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="c1"># TF Function path should take the eager path.</span>

<span class="w">            </span><span class="c1"># If the given metric is available in `metrics` list we just update</span>

<span class="w">            </span><span class="c1"># state on it, otherwise we create a new metric instance and</span>

<span class="w">            </span><span class="c1"># add it to the `metrics` list.</span>

<span class="w">            </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;_metric_obj&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span>

<span class="w">            </span><span class="c1"># Tensors that come from a Metric object already updated the Metric</span>

<span class="w">            </span><span class="c1"># state.</span>

<span class="w">            </span><span class="n">should_update_state</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">metric_obj</span>

<span class="w">            </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric_obj</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">name</span>

<span class="w">            </span><span class="k">with</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics_lock</span><span class="o">:</span>

<span class="w">                </span><span class="k">match</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_existing_metric</span><span class="p">(</span><span class="k">name</span><span class="p">)</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="k">match</span><span class="o">:</span>

<span class="w">                    </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">match</span>

<span class="w">                </span><span class="n">elif</span><span class="w"> </span><span class="n">metric_obj</span><span class="o">:</span>

<span class="w">                    </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span>

<span class="w">                </span><span class="k">else</span><span class="o">:</span>

<span class="w">                    </span><span class="c1"># Build the metric object with the value&#39;s dtype if it</span>

<span class="w">                    </span><span class="c1"># defines one</span>

<span class="w">                    </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metrics_mod</span><span class="p">.</span><span class="n">Mean</span><span class="p">(</span>

<span class="w">                        </span><span class="k">name</span><span class="o">=</span><span class="k">name</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span>

<span class="w">                    </span><span class="p">)</span>

<span class="w">                    </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">should_update_state</span><span class="o">:</span>

<span class="w">                </span><span class="n">metric_obj</span><span class="p">(</span><span class="k">value</span><span class="p">)</span>

<span class="w">        </span><span class="k">else</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                    </span><span class="s2">&quot;Using the result of calling a `Metric` object &quot;</span>

<span class="w">                    </span><span class="s2">&quot;when calling `add_metric` on a Functional &quot;</span>

<span class="w">                    </span><span class="s2">&quot;Model is not supported. Please pass the &quot;</span>

<span class="w">                    </span><span class="s2">&quot;Tensor to monitor directly.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="c1"># Insert layers into the Keras Graph Network.</span>

<span class="w">            </span><span class="n">aggregation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s2">&quot;mean&quot;</span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">_graph_network_add_metric</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">aggregation</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="add_update_1">add_update</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_update</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">updates</span>
<span class="p">)</span>
</code></pre></div>

<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and
variance in a BatchNormalization layer) may be dependent on the inputs
passed when calling a layer. Hence, when reusing the same layer on
different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be
dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>This call is ignored when eager execution is enabled (in that case,
variable updates are run on the fly and thus do not need to be tracked
for later execution).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>updates</td>
<td>None</td>
<td>Update op, or list/tuple of update ops, or zero-arg callable<br>that returns an update op. A zero-arg callable should be passed in<br>order to disable running the updates by setting <code>trainable=False</code><br>on this Layer, when executing in Eager mode.</td>
<td>None</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls.do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">updates</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Add update op(s), potentially dependent on layer inputs.</span>

<span class="s2">        Weight updates (for instance, the updates of the moving mean and</span>

<span class="s2">        variance in a BatchNormalization layer) may be dependent on the inputs</span>

<span class="s2">        passed when calling a layer. Hence, when reusing the same layer on</span>

<span class="s2">        different inputs `a` and `b`, some entries in `layer.updates` may be</span>

<span class="s2">        dependent on `a` and some on `b`. This method automatically keeps track</span>

<span class="s2">        of dependencies.</span>

<span class="s2">        This call is ignored when eager execution is enabled (in that case,</span>

<span class="s2">        variable updates are run on the fly and thus do not need to be tracked</span>

<span class="s2">        for later execution).</span>

<span class="s2">        Args:</span>

<span class="s2">          updates: Update op, or list/tuple of update ops, or zero-arg callable</span>

<span class="s2">            that returns an update op. A zero-arg callable should be passed in</span>

<span class="s2">            order to disable running the updates by setting `trainable=False`</span>

<span class="s2">            on this Layer, when executing in Eager mode.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="n">call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">()</span>

<span class="w">        </span><span class="c1"># No need to run updates during Functional API construction.</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">in_keras_graph</span><span class="o">:</span>

<span class="w">            </span><span class="k">return</span>

<span class="w">        </span><span class="c1"># Callable updates are disabled by setting `trainable=False`.</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">frozen</span><span class="o">:</span>

<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="k">update</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">updates</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">callable</span><span class="p">(</span><span class="k">update</span><span class="p">)</span><span class="o">:</span>

<span class="w">                    </span><span class="k">update</span><span class="p">()</span>
</code></pre></div>

</details>
<h4 id="add_variable_1">add_variable</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_variable</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use! Alias for <code>add_weight</code>.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls.do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_variable</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use! Alias for `add_weight`.</span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span>

<span class="w">            </span><span class="s2">&quot;`layer.add_variable` is deprecated and &quot;</span>

<span class="w">            </span><span class="s2">&quot;will be removed in a future version. &quot;</span>

<span class="w">            </span><span class="s2">&quot;Please use the `layer.add_weight()` method instead.&quot;</span><span class="p">,</span>

<span class="w">            </span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>

<span class="w">        </span><span class="p">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="add_weight_1">add_weight</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synchronization</span><span class="o">=&lt;</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="o">=&lt;</span><span class="n">VariableAggregationV2</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds a new variable to the layer.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>None</td>
<td>Variable name.</td>
<td>None</td>
</tr>
<tr>
<td>shape</td>
<td>None</td>
<td>Variable shape. Defaults to scalar if unspecified.</td>
<td>scalar if unspecified</td>
</tr>
<tr>
<td>dtype</td>
<td>None</td>
<td>The type of the variable. Defaults to <code>self.dtype</code>.</td>
<td><code>self.dtype</code></td>
</tr>
<tr>
<td>initializer</td>
<td>None</td>
<td>Initializer instance (callable).</td>
<td>None</td>
</tr>
<tr>
<td>regularizer</td>
<td>None</td>
<td>Regularizer instance (callable).</td>
<td>None</td>
</tr>
<tr>
<td>trainable</td>
<td>None</td>
<td>Boolean, whether the variable should be part of the layer's<br>"trainable_variables" (e.g. variables, biases)<br>or "non_trainable_variables" (e.g. BatchNorm mean and variance).<br>Note that <code>trainable</code> cannot be <code>True</code> if <code>synchronization</code><br>is set to <code>ON_READ</code>.</td>
<td>None</td>
</tr>
<tr>
<td>constraint</td>
<td>None</td>
<td>Constraint instance (callable).</td>
<td>None</td>
</tr>
<tr>
<td>use_resource</td>
<td>None</td>
<td>Whether to use a <code>ResourceVariable</code> or not.<br>See <a href="&lt;br&gt;https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables">this guide</a><br> for more information.</td>
<td>None</td>
</tr>
<tr>
<td>synchronization</td>
<td>None</td>
<td>Indicates when a distributed a variable will be<br>aggregated. Accepted values are constants defined in the class<br><code>tf.VariableSynchronization</code>. By default the synchronization is set<br>to <code>AUTO</code> and the current <code>DistributionStrategy</code> chooses when to<br>synchronize. If <code>synchronization</code> is set to <code>ON_READ</code>, <code>trainable</code><br>must not be set to <code>True</code>.</td>
<td>None</td>
</tr>
<tr>
<td>aggregation</td>
<td>None</td>
<td>Indicates how a distributed variable will be aggregated.<br>Accepted values are constants defined in the class<br><code>tf.VariableAggregation</code>.</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>None</td>
<td>Additional keyword arguments. Accepted values are <code>getter</code>,<br><code>collections</code>, <code>experimental_autocast</code> and <code>caching_device</code>.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>The variable created.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>When giving unsupported dtype and no initializer or when<br>trainable has been set to True with synchronization set as<br><code>ON_READ</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="p">@</span><span class="n">doc_controls</span><span class="p">.</span><span class="n">for_subclass_implementers</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_weight</span><span class="p">(</span>

<span class="w">        </span><span class="nb">self</span><span class="p">,</span>

<span class="w">        </span><span class="n">name</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">shape</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">dtype</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">initializer</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">regularizer</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">trainable</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">constraint</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">use_resource</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>

<span class="w">        </span><span class="n">synchronization</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">AUTO</span><span class="p">,</span>

<span class="w">        </span><span class="n">aggregation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">VariableAggregation</span><span class="p">.</span><span class="n">NONE</span><span class="p">,</span>

<span class="w">        </span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>

<span class="w">    </span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s">&quot;&quot;&quot;Adds a new variable to the layer.</span>

<span class="w">        </span><span class="nl">Args</span><span class="p">:</span>

<span class="w">          </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="n">name</span><span class="p">.</span>

<span class="w">          </span><span class="nl">shape</span><span class="p">:</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="n">shape</span><span class="p">.</span><span class="w"> </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">scalar</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">unspecified</span><span class="p">.</span>

<span class="w">          </span><span class="nl">dtype</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="w"> </span><span class="n">Defaults</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="nb">self</span><span class="p">.</span><span class="n">dtype</span><span class="err">`</span><span class="p">.</span>

<span class="w">          </span><span class="nl">initializer</span><span class="p">:</span><span class="w"> </span><span class="n">Initializer</span><span class="w"> </span><span class="n">instance</span><span class="w"> </span><span class="p">(</span><span class="n">callable</span><span class="p">).</span>

<span class="w">          </span><span class="nl">regularizer</span><span class="p">:</span><span class="w"> </span><span class="n">Regularizer</span><span class="w"> </span><span class="n">instance</span><span class="w"> </span><span class="p">(</span><span class="n">callable</span><span class="p">).</span>

<span class="w">          </span><span class="nl">trainable</span><span class="p">:</span><span class="w"> </span><span class="kt">Boolean</span><span class="p">,</span><span class="w"> </span><span class="n">whether</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">part</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="err">&#39;</span><span class="n">s</span>

<span class="w">            </span><span class="s">&quot;trainable_variables&quot;</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="w"> </span><span class="n">variables</span><span class="p">,</span><span class="w"> </span><span class="n">biases</span><span class="p">)</span>

<span class="w">            </span><span class="n">or</span><span class="w"> </span><span class="s">&quot;non_trainable_variables&quot;</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="w"> </span><span class="n">BatchNorm</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">variance</span><span class="p">).</span>

<span class="w">            </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="err">`</span><span class="n">trainable</span><span class="err">`</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="err">`</span><span class="n">True</span><span class="err">`</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">`</span><span class="n">synchronization</span><span class="err">`</span>

<span class="w">            </span><span class="n">is</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="n">ON_READ</span><span class="err">`</span><span class="p">.</span>

<span class="w">          </span><span class="nl">constraint</span><span class="p">:</span><span class="w"> </span><span class="n">Constraint</span><span class="w"> </span><span class="n">instance</span><span class="w"> </span><span class="p">(</span><span class="n">callable</span><span class="p">).</span>

<span class="w">          </span><span class="nl">use_resource</span><span class="p">:</span><span class="w"> </span><span class="n">Whether</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="err">`</span><span class="n">ResourceVariable</span><span class="err">`</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">not</span><span class="p">.</span>

<span class="w">            </span><span class="n">See</span><span class="w"> </span><span class="p">[</span><span class="n">this</span><span class="w"> </span><span class="n">guide</span><span class="p">](</span>

<span class="w">            </span><span class="nl">https</span><span class="p">:</span><span class="c1">//www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)</span>

<span class="w">             </span><span class="k">for</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">information</span><span class="p">.</span>

<span class="w">          </span><span class="nl">synchronization</span><span class="p">:</span><span class="w"> </span><span class="n">Indicates</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">distributed</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span>

<span class="w">            </span><span class="n">aggregated</span><span class="p">.</span><span class="w"> </span><span class="n">Accepted</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">constants</span><span class="w"> </span><span class="n">defined</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">class</span>

<span class="w">            </span><span class="err">`</span><span class="n">tf</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="err">`</span><span class="p">.</span><span class="w"> </span><span class="n">By</span><span class="w"> </span><span class="k">default</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">synchronization</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">set</span>

<span class="w">            </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="n">AUTO</span><span class="err">`</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="err">`</span><span class="n">DistributionStrategy</span><span class="err">`</span><span class="w"> </span><span class="n">chooses</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">to</span>

<span class="w">            </span><span class="n">synchronize</span><span class="p">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="err">`</span><span class="n">synchronization</span><span class="err">`</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="n">ON_READ</span><span class="err">`</span><span class="p">,</span><span class="w"> </span><span class="err">`</span><span class="n">trainable</span><span class="err">`</span>

<span class="w">            </span><span class="n">must</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="err">`</span><span class="n">True</span><span class="err">`</span><span class="p">.</span>

<span class="w">          </span><span class="nl">aggregation</span><span class="p">:</span><span class="w"> </span><span class="n">Indicates</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">distributed</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">aggregated</span><span class="p">.</span>

<span class="w">            </span><span class="n">Accepted</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">constants</span><span class="w"> </span><span class="n">defined</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">class</span>

<span class="w">            </span><span class="err">`</span><span class="n">tf</span><span class="p">.</span><span class="n">VariableAggregation</span><span class="err">`</span><span class="p">.</span>

<span class="w">          </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Additional</span><span class="w"> </span><span class="n">keyword</span><span class="w"> </span><span class="n">arguments</span><span class="p">.</span><span class="w"> </span><span class="n">Accepted</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="err">`</span><span class="k">getter</span><span class="err">`</span><span class="p">,</span>

<span class="w">            </span><span class="err">`</span><span class="n">collections</span><span class="err">`</span><span class="p">,</span><span class="w"> </span><span class="err">`</span><span class="n">experimental_autocast</span><span class="err">`</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="err">`</span><span class="n">caching_device</span><span class="err">`</span><span class="p">.</span>

<span class="w">        </span><span class="nl">Returns</span><span class="p">:</span>

<span class="w">          </span><span class="n">The</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">created</span><span class="p">.</span>

<span class="w">        </span><span class="nl">Raises</span><span class="p">:</span>

<span class="w">          </span><span class="nl">ValueError</span><span class="p">:</span><span class="w"> </span><span class="n">When</span><span class="w"> </span><span class="n">giving</span><span class="w"> </span><span class="n">unsupported</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">initializer</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">when</span>

<span class="w">            </span><span class="n">trainable</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">synchronization</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">as</span>

<span class="w">            </span><span class="err">`</span><span class="n">ON_READ</span><span class="err">`</span><span class="p">.</span>

<span class="w">        </span><span class="s">&quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span>

<span class="w">        </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;partitioner&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">Ignored</span><span class="p">.</span>

<span class="w">        </span><span class="cp"># Validate optional keyword arguments.</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">kwargs</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="p">[</span>

<span class="w">                </span><span class="s">&quot;collections&quot;</span><span class="p">,</span>

<span class="w">                </span><span class="s">&quot;experimental_autocast&quot;</span><span class="p">,</span>

<span class="w">                </span><span class="s">&quot;caching_device&quot;</span><span class="p">,</span>

<span class="w">                </span><span class="s">&quot;getter&quot;</span><span class="p">,</span>

<span class="w">                </span><span class="s">&quot;layout&quot;</span><span class="p">,</span>

<span class="w">            </span><span class="p">]</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s">&quot;Unknown keyword argument:&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">kwarg</span><span class="p">)</span>

<span class="w">        </span><span class="n">collections_arg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;collections&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>

<span class="w">        </span><span class="cp"># &#39;experimental_autocast&#39; can be set to False by the caller to indicate</span>

<span class="w">        </span><span class="cp"># an AutoCastVariable should never be created.</span>

<span class="w">        </span><span class="n">autocast</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;experimental_autocast&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">True</span><span class="p">)</span>

<span class="w">        </span><span class="cp"># See the docstring for tf.Variable about the details for</span>

<span class="w">        </span><span class="cp"># caching_device.</span>

<span class="w">        </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;caching_device&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>

<span class="w">        </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;layout&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>

<span class="w">        </span><span class="cp"># Specially handling of auto layout fetch, based on the variable name</span>

<span class="w">        </span><span class="cp"># and attribute name. For built-in keras layers, usually the variable</span>

<span class="w">        </span><span class="cp"># name, eg &#39;kernel&#39;, will match with a &#39;kernel_layout&#39; attribute name on</span>

<span class="w">        </span><span class="cp"># the instance. We will try to do this auto fetch if layout is not</span>

<span class="w">        </span><span class="cp"># explicitly specified. This is mainly a quick workaround for not</span>

<span class="w">        </span><span class="cp"># applying too many interface change to built-in layers, until DTensor</span>

<span class="w">        </span><span class="cp"># is a public API.  Also see dtensor.utils.allow_initializer_layout for</span>

<span class="w">        </span><span class="cp"># more details.</span>

<span class="w">        </span><span class="cp"># TODO(scottzhu): Remove this once dtensor is public to end user.</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">layout</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">name</span><span class="o">:</span>

<span class="w">            </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">&quot;_layout&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">backend</span><span class="p">.</span><span class="n">floatx</span><span class="p">()</span>

<span class="w">        </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="cp"># The policy is &quot;_infer&quot;, so we infer the policy from the variable</span>

<span class="w">            </span><span class="cp"># dtype.</span>

<span class="w">            </span><span class="nb">self</span><span class="p">.</span><span class="n">_set_dtype_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">Policy</span><span class="p">(</span><span class="n">dtype</span><span class="p">.</span><span class="n">base_dtype</span><span class="p">.</span><span class="n">name</span><span class="p">))</span>

<span class="w">        </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span>

<span class="w">        </span><span class="n">regularizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">regularizers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">regularizer</span><span class="p">)</span>

<span class="w">        </span><span class="n">constraint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">constraints</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">synchronization</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">ON_READ</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                    </span><span class="s">&quot;Synchronization value can be set to &quot;</span>

<span class="w">                    </span><span class="s">&quot;VariableSynchronization.ON_READ only for non-trainable &quot;</span>

<span class="w">                    </span><span class="s">&quot;variables. You have specified trainable=True and &quot;</span>

<span class="w">                    </span><span class="s">&quot;synchronization=VariableSynchronization.ON_READ.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                </span><span class="cp"># Set trainable to be false when variable is to be synced on</span>

<span class="w">                </span><span class="cp"># read.</span>

<span class="w">                </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">False</span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">trainable</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">True</span>

<span class="w">        </span><span class="cp"># Initialize variable when no initializer provided</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">initializer</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="cp"># If dtype is DT_FLOAT, provide a uniform unit scaling initializer</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span><span class="o">:</span>

<span class="w">                </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;glorot_uniform&quot;</span><span class="p">)</span>

<span class="w">            </span><span class="cp"># If dtype is DT_INT/DT_UINT, provide a default value `zero`</span>

<span class="w">            </span><span class="cp"># If dtype is DT_BOOL, provide a default value `FALSE`</span>

<span class="w">            </span><span class="n">elif</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_integer</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_unsigned</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_bool</span><span class="o">:</span>

<span class="w">                </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;zeros&quot;</span><span class="p">)</span>

<span class="w">            </span><span class="cp"># NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX</span>

<span class="w">            </span><span class="cp"># here?</span>

<span class="w">            </span><span class="n">elif</span><span class="w"> </span><span class="s">&quot;getter&quot;</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">kwargs</span><span class="o">:</span>

<span class="w">                </span><span class="cp"># When `getter` is specified, it&#39;s possibly fine for</span>

<span class="w">                </span><span class="cp"># `initializer` to be None since it&#39;s up to the custom `getter`</span>

<span class="w">                </span><span class="cp"># to raise error in case it indeed needs `initializer`.</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                    </span><span class="n">f</span><span class="s">&quot;An initializer for variable {name} of type &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s">&quot;{dtype.base_dtype} is required for layer &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s">&quot;{self.name}. Received: {initializer}.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">        </span><span class="k">getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&quot;getter&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">make_variable</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span>

<span class="w">            </span><span class="n">autocast</span>

<span class="w">            </span><span class="n">and</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">compute_dtype</span>

<span class="w">            </span><span class="o">!=</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span>

<span class="w">            </span><span class="n">and</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span>

<span class="w">        </span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="n">old_getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">getter</span>

<span class="w">            </span><span class="cp"># Wrap variable constructor to return an AutoCastVariable.</span>

<span class="w">            </span><span class="n">def</span><span class="w"> </span><span class="k">getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="n">autocast_variable</span><span class="p">.</span><span class="n">create_autocast_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

<span class="w">            </span><span class="cp"># Also the caching_device does not work with the mixed precision</span>

<span class="w">            </span><span class="cp"># API, disable it if it is specified.</span>

<span class="w">            </span><span class="cp"># TODO(b/142020079): Re-enable it once the bug is fixed.</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">caching_device</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">                </span><span class="n">tf_logging</span><span class="p">.</span><span class="n">warning</span><span class="p">(</span>

<span class="w">                    </span><span class="s">&quot;`caching_device` does not work with mixed precision API. &quot;</span>

<span class="w">                    </span><span class="s">&quot;Ignoring user specified `caching_device`.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">                </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">layout</span><span class="o">:</span>

<span class="w">            </span><span class="k">getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">functools</span><span class="p">.</span><span class="n">partial</span><span class="p">(</span><span class="k">getter</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="o">=</span><span class="n">layout</span><span class="p">)</span>

<span class="w">        </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_add_variable_with_custom_getter</span><span class="p">(</span>

<span class="w">            </span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>

<span class="w">            </span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>

<span class="w">            </span><span class="cp"># TODO(allenl): a `make_variable` equivalent should be added as a</span>

<span class="w">            </span><span class="cp"># `Trackable` method.</span>

<span class="w">            </span><span class="k">getter</span><span class="o">=</span><span class="k">getter</span><span class="p">,</span>

<span class="w">            </span><span class="cp"># Manage errors in Layer rather than Trackable.</span>

<span class="w">            </span><span class="n">overwrite</span><span class="o">=</span><span class="n">True</span><span class="p">,</span>

<span class="w">            </span><span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>

<span class="w">            </span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>

<span class="w">            </span><span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>

<span class="w">            </span><span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>

<span class="w">            </span><span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>

<span class="w">            </span><span class="n">collections</span><span class="o">=</span><span class="n">collections_arg</span><span class="p">,</span>

<span class="w">            </span><span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>

<span class="w">            </span><span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>

<span class="w">            </span><span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>

<span class="w">        </span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">regularizer</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>

<span class="w">            </span><span class="cp"># TODO(fchollet): in the future, this should be handled at the</span>

<span class="w">            </span><span class="cp"># level of variable creation, and weight regularization losses</span>

<span class="w">            </span><span class="cp"># should be variable attributes.</span>

<span class="w">            </span><span class="n">name_in_scope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="n">name</span><span class="p">[</span><span class="o">:</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="n">name</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">&quot;:&quot;</span><span class="p">)]</span>

<span class="w">            </span><span class="nb">self</span><span class="p">.</span><span class="n">_handle_weight_regularization</span><span class="p">(</span>

<span class="w">                </span><span class="n">name_in_scope</span><span class="p">,</span><span class="w"> </span><span class="n">variable</span><span class="p">,</span><span class="w"> </span><span class="n">regularizer</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">is_split_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">variable</span><span class="o">:</span>

<span class="w">                </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span>

<span class="w">                    </span><span class="nb">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="w">                </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                    </span><span class="nb">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="w">        </span><span class="nl">else</span><span class="p">:</span>

<span class="w">            </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span>

<span class="w">                </span><span class="nb">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

<span class="w">            </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                </span><span class="nb">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">variable</span>
</code></pre></div>

</details>
<h4 id="build_1">build</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">build</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_shape</span>
<span class="p">)</span>
</code></pre></div>

<p>Build the transformer block.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_shape</td>
<td>None</td>
<td>Shape of the input tensor</td>
<td>None</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">def</span><span class="w"> </span><span class="nv">build</span><span class="ss">(</span><span class="nv">self</span>,<span class="w"> </span><span class="nv">input_shape</span><span class="ss">)</span>:

<span class="w">        </span><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>

<span class="err">        Build the transformer block.</span>

<span class="err">        :param input_shape: Shape of the input tensor</span>

<span class="w">        </span><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>

<span class="err">        if self.size is None:</span>

<span class="err">            self.size = input_shape[-1]</span>

<span class="err">        self.att = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.size, dropout=self.mha_dropout, )</span>

<span class="err">        if self.ff_size is None:</span>

<span class="err">            self.ff_size = self.size * self.ff_mul</span>

<span class="err">        # self.ffn = self.ffn or build_dense_model(</span>

<span class="err">        #     [</span>

<span class="err">        #         self.ff_size,</span>

<span class="err">        #         Dropout(self.dropout),</span>

<span class="err">        #         self.size,</span>

<span class="err">        #         Dropout(self.dropout)</span>

<span class="err">        #     ],</span>

<span class="err">        #     activation=self.ff_act,</span>

<span class="err">        #     last_activation=None)</span>

<span class="err">        self.ffn = self.ffn or Sequential([</span>

<span class="err">            Dense(self.ff_size, activation=self.ff_act),</span>

<span class="err">            Dropout(self.dropout),</span>

<span class="err">            Dense(self.size),</span>

<span class="err">            Dropout(self.dropout)</span>

<span class="err">        ])</span>

<span class="err">        self.layernorm1 = LayerNormalization(epsilon=1e-6)</span>

<span class="err">        self.layernorm2 = LayerNormalization(epsilon=1e-6)</span>
</code></pre></div>

</details>
<h4 id="call_1">call</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Perform the forward pass of the transformer block.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>None</td>
<td>Input tensor</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Output tensor and attention scores if return_attention_scores is True</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">def</span><span class="w"> </span><span class="nv">call</span><span class="ss">(</span><span class="nv">self</span>,<span class="w"> </span><span class="nv">inputs</span><span class="ss">)</span>:

<span class="w">        </span><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>

<span class="err">        Perform the forward pass of the transformer block.</span>

<span class="err">        :param inputs: Input tensor</span>

<span class="err">        :return: Output tensor and attention scores if return_attention_scores is True</span>

<span class="w">        </span><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>

<span class="err">        if self.return_attention_scores:</span>

<span class="err">            x, scores = self.att(inputs, inputs, return_attention_scores=True)</span>

<span class="err">        else:</span>

<span class="err">            x = self.att(inputs, inputs)</span>

<span class="err">        x = self.layernorm1(x + inputs)</span>

<span class="err">        y = self.ffn(x)</span>

<span class="err">        if self.return_attention_scores:</span>

<span class="err">            return self.layernorm2(x + y), scores</span>

<span class="err">        return self.layernorm2(x + y)</span>
</code></pre></div>

</details>
<h4 id="compute_mask_1">compute_mask</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes an output mask tensor.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>None</td>
<td>Tensor or list of tensors.</td>
<td>None</td>
</tr>
<tr>
<td>mask</td>
<td>None</td>
<td>Tensor or list of tensors.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None or a tensor (or list of tensors,<br>one per output tensor of the layer).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@generic_utils</span><span class="p">.</span><span class="k">default</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">compute_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">mask</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Computes an output mask tensor.</span>

<span class="ss">        Args:</span>

<span class="ss">            inputs: Tensor or list of tensors.</span>

<span class="ss">            mask: Tensor or list of tensors.</span>

<span class="ss">        Returns:</span>

<span class="ss">            None or a tensor (or list of tensors,</span>

<span class="ss">                one per output tensor of the layer).</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">_supports_masking</span><span class="p">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="p">(</span><span class="n">m</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="err">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span>

<span class="w">                    </span><span class="ss">&quot;Layer &quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="ss">&quot; does not support masking, &quot;</span>

<span class="w">                    </span><span class="ss">&quot;but was passed an input_mask: &quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">str</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="nl">supported</span><span class="p">:</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">mask</span><span class="p">.</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="k">None</span>

<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="n">supported</span><span class="p">,</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="k">default</span>

<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="k">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="n">mask</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">mask</span>
</code></pre></div>

</details>
<h4 id="compute_output_shape_1">compute_output_shape</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_shape</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes the output shape of the layer.</p>
<p>This method will cause the layer's state to be built, if that has not
happened before. This requires that the layer will later be used with
inputs that match the input shape provided here.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_shape</td>
<td>None</td>
<td>Shape tuple (tuple of integers) or <code>tf.TensorShape</code>,<br>or structure of shape tuples / <code>tf.TensorShape</code> instances<br>(one per output tensor of the layer).<br>Shape tuples can include None for free dimensions,<br>instead of an integer.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A <code>tf.TensorShape</code> instance<br>or structure of <code>tf.TensorShape</code> instances.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">compute_output_shape</span>(<span class="nb">self</span>, <span class="n">input_shape</span>):

        <span class="s">&quot;&quot;&quot;Computes the output shape of the layer.</span>

<span class="s">        This method will cause the layer&#39;s state to be built, if that has not</span>

<span class="s">        happened before. This requires that the layer will later be used with</span>

<span class="s">        inputs that match the input shape provided here.</span>

<span class="s">        Args:</span>

<span class="s">            input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`,</span>

<span class="s">                or structure of shape tuples / `tf.TensorShape` instances</span>

<span class="s">                (one per output tensor of the layer).</span>

<span class="s">                Shape tuples can include None for free dimensions,</span>

<span class="s">                instead of an integer.</span>

<span class="s">        Returns:</span>

<span class="s">            A `tf.TensorShape` instance</span>

<span class="s">            or structure of `tf.TensorShape` instances.</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">tf</span>.<span class="n">executing_eagerly</span>():

            <span class="c1"># In this case we build the model first in order to do shape</span>

            <span class="c1"># inference.  This is acceptable because the framework only calls</span>

            <span class="c1"># `compute_output_shape` on shape values that the layer would later</span>

            <span class="c1"># be built for. It would however cause issues in case a user</span>

            <span class="c1"># attempts to use `compute_output_shape` manually with shapes that</span>

            <span class="c1"># are incompatible with the shape the Layer will be called on (these</span>

            <span class="c1"># users will have to implement `compute_output_shape` themselves).</span>

            <span class="nb">self</span>.<span class="n">_maybe_build</span>(<span class="n">input_shape</span>)

            <span class="n">graph_name</span> = <span class="n">str</span>(<span class="nb">self</span>.<span class="nb">name</span>) + <span class="s">&quot;_scratch_graph&quot;</span>

            <span class="k">with</span> <span class="n">tf</span>.<span class="n">__internal__</span>.<span class="n">FuncGraph</span>(<span class="n">graph_name</span>).<span class="n">as_default</span>():

                <span class="n">input_shape</span> = <span class="n">tf_utils</span>.<span class="n">convert_shapes</span>(

                    <span class="n">input_shape</span>, <span class="n">to_tuples</span>=<span class="nb">False</span>

                )

                <span class="n">def</span> <span class="n">_make_placeholder_like</span>(<span class="nb">shape</span>):

                    <span class="n">ph</span> = <span class="n">backend</span>.<span class="nb">placeholder</span>(<span class="nb">shape</span>=<span class="nb">shape</span>, <span class="n">dtype</span>=<span class="nb">self</span>.<span class="n">dtype</span>)

                    <span class="n">ph</span>.<span class="n">_keras_mask</span> = <span class="n">None</span>

                    <span class="k">return</span> <span class="n">ph</span>

                <span class="n">inputs</span> = <span class="n">tf</span>.<span class="n">nest</span>.<span class="n">map_structure</span>(

                    <span class="n">_make_placeholder_like</span>, <span class="n">input_shape</span>

                )

                <span class="n">try:</span>

                    <span class="n">outputs</span> = <span class="nb">self</span>(<span class="n">inputs</span>, <span class="n">training</span>=<span class="nb">False</span>)

                <span class="n">except</span> <span class="n">TypeError</span> <span class="n">as</span> <span class="n">e:</span>

                    <span class="n">raise</span> <span class="n">NotImplementedError</span>(

                        <span class="s">&quot;We could not automatically infer the static shape of &quot;</span>

                        <span class="s">&quot;the layer&#39;s output. Please implement the &quot;</span>

                        <span class="s">&quot;`compute_output_shape` method on your layer (%s).&quot;</span>

                        % <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>

                    ) <span class="nb">from</span> <span class="nb">e</span>

            <span class="k">return</span> <span class="n">tf</span>.<span class="n">nest</span>.<span class="n">map_structure</span>(<span class="n">lambda</span> <span class="n">t:</span> <span class="nb">t</span>.<span class="nb">shape</span>, <span class="n">outputs</span>)

        <span class="n">raise</span> <span class="n">NotImplementedError</span>(

            <span class="s">&quot;Please run in eager mode or implement the `compute_output_shape` &quot;</span>

            <span class="s">&quot;method on your layer (%s).&quot;</span> % <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>

        )
</code></pre></div>

</details>
<h4 id="compute_output_signature_1">compute_output_signature</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_signature</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_signature</span>
<span class="p">)</span>
</code></pre></div>

<p>Compute the output tensor signature of the layer based on the inputs.</p>
<p>Unlike a TensorShape object, a TensorSpec object contains both shape
and dtype information for a tensor. This method allows layers to provide
output dtype information if it is different from the input dtype.
For any layer that doesn't implement this function,
the framework will fall back to use <code>compute_output_shape</code>, and will
assume that the output dtype matches the input dtype.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_signature</td>
<td>None</td>
<td>Single TensorSpec or nested structure of TensorSpec<br>objects, describing a candidate input for the layer.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Single TensorSpec or nested structure of TensorSpec objects,<br>describing how the layer would transform the provided input.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>TypeError</td>
<td>If input_signature contains a non-TensorSpec object.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls.for_subclass_implementers</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">compute_output_signature</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Compute the output tensor signature of the layer based on the inputs.</span>

<span class="s2">        Unlike a TensorShape object, a TensorSpec object contains both shape</span>

<span class="s2">        and dtype information for a tensor. This method allows layers to provide</span>

<span class="s2">        output dtype information if it is different from the input dtype.</span>

<span class="s2">        For any layer that doesn&#39;t implement this function,</span>

<span class="s2">        the framework will fall back to use `compute_output_shape`, and will</span>

<span class="s2">        assume that the output dtype matches the input dtype.</span>

<span class="s2">        Args:</span>

<span class="s2">          input_signature: Single TensorSpec or nested structure of TensorSpec</span>

<span class="s2">            objects, describing a candidate input for the layer.</span>

<span class="s2">        Returns:</span>

<span class="s2">          Single TensorSpec or nested structure of TensorSpec objects,</span>

<span class="s2">            describing how the layer would transform the provided input.</span>

<span class="s2">        Raises:</span>

<span class="s2">          TypeError: If input_signature contains a non-TensorSpec object.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="n">def</span><span class="w"> </span><span class="n">check_type_return_shape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span>

<span class="w">                    </span><span class="s2">&quot;Only TensorSpec signature types are supported. &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s2">&quot;Received: {s}.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">shape</span>

<span class="w">        </span><span class="n">input_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span>

<span class="w">            </span><span class="n">check_type_return_shape</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span>

<span class="w">        </span><span class="p">)</span>

<span class="w">        </span><span class="n">output_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

<span class="w">        </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_compute_dtype</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span>

<span class="w">            </span><span class="n">input_dtypes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[</span><span class="n">s</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_signature</span><span class="p">)</span><span class="err">]</span>

<span class="w">            </span><span class="c1"># Default behavior when self.dtype is None, is to use the first</span>

<span class="w">            </span><span class="c1"># input&#39;s dtype.</span>

<span class="w">            </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_dtypes</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span>

<span class="w">            </span><span class="n">lambda</span><span class="w"> </span><span class="n">s</span><span class="o">:</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="o">=</span><span class="n">s</span><span class="p">),</span><span class="w"> </span><span class="n">output_shape</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="count_params_1">count_params</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Count the total number of scalars composing the weights.</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>An integer count.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>if the layer isn't yet built<br>(in which case its weights aren't yet defined).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Count the total number of scalars composing the weights.</span>

<span class="s2">        Returns:</span>

<span class="s2">            An integer count.</span>

<span class="s2">        Raises:</span>

<span class="s2">            ValueError: if the layer isn&#39;t yet built</span>

<span class="s2">              (in which case its weights aren&#39;t yet defined).</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">built</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;_is_graph_network&quot;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="k">with</span><span class="w"> </span><span class="n">tf_utils</span><span class="p">.</span><span class="n">maybe_init_scope</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span>

<span class="w">                    </span><span class="n">self</span><span class="p">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inputs</span><span class="p">)</span>

<span class="w">            </span><span class="k">else</span><span class="o">:</span>

<span class="w">                </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                    </span><span class="s2">&quot;You tried to call `count_params` &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s2">&quot;on layer {self.name}&quot;</span>

<span class="w">                    </span><span class="s2">&quot;, but the layer isn&#39;t built. &quot;</span>

<span class="w">                    </span><span class="s2">&quot;You can build it manually via: &quot;</span>

<span class="w">                    </span><span class="n">f</span><span class="s2">&quot;`{self.name}.build(batch_input_shape)`.&quot;</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">layer_utils</span><span class="p">.</span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="finalize_state_1">finalize_state</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">finalize_state</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Finalizes the layers state after updating layer weights.</p>
<p>This function can be subclassed in a layer and will be called after
updating a layer weights. It can be overridden to finalize any
additional layer state after a weight update.</p>
<p>This function will be called after weights of a layer have been restored
from a loaded model.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="err">@</span><span class="n">doc_controls</span><span class="o">.</span><span class="n">do_not_generate_docs</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">finalize_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;Finalizes the layers state after updating layer weights.</span>

<span class="sd">        This function can be subclassed in a layer and will be called after</span>

<span class="sd">        updating a layer weights. It can be overridden to finalize any</span>

<span class="sd">        additional layer state after a weight update.</span>

<span class="sd">        This function will be called after weights of a layer have been restored</span>

<span class="sd">        from a loaded model.</span>

<span class="sd">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">pass</span>
</code></pre></div>

</details>
<h4 id="get_config_1">get_config</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of
dict every time it is called. The callers should make a copy of the
returned dict if they want to modify it.</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Python dictionary.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">def</span><span class="w"> </span><span class="nv">get_config</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

<span class="w">        </span><span class="nv">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">super</span><span class="ss">()</span>.<span class="nv">get_config</span><span class="ss">()</span>

<span class="w">        </span><span class="nv">config</span>.<span class="nv">update</span><span class="ss">(</span>{

<span class="w">            </span><span class="s2">&quot;size&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">size</span>,

<span class="w">            </span><span class="s2">&quot;num_heads&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">num_heads</span>,

<span class="w">            </span><span class="s2">&quot;ff_mul&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">ff_mul</span>,

<span class="w">            </span><span class="s2">&quot;ff_size&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">ff_size</span>,

<span class="w">            </span><span class="s2">&quot;ff_act&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">ff_act</span>,

<span class="w">            </span><span class="s2">&quot;dropout&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">dropout</span>,

<span class="w">            </span><span class="s2">&quot;mha_dropout&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">mha_dropout</span>,

<span class="w">            </span><span class="s2">&quot;return_attention_scores&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">return_attention_scores</span>,

<span class="w">            </span><span class="s2">&quot;ffn&quot;</span>:<span class="w"> </span><span class="nv">self</span>.<span class="nv">ffn</span>

<span class="w">        </span>}<span class="ss">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nv">config</span>
</code></pre></div>

</details>
<h4 id="get_input_at_1">get_input_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first input node of the layer.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first input node of the layer.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A tensor (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">        Raises:</span>

<span class="ss">          RuntimeError: If called in Eager mode.</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span>

<span class="w">            </span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;input_tensors&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;input&quot;</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_input_mask_at_1">get_input_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first time the layer was called.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A mask tensor</span>

<span class="ss">            (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_input_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &quot;_keras_mask&quot;, None) for x in inputs</span><span class="o">]</span>

<span class="w">        </span><span class="k">else</span><span class="err">:</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;_keras_mask&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_input_shape_at_1">get_input_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the input shape(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first time the layer was called.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A shape tuple</span>

<span class="ss">            (or list of shape tuples if the layer has multiple inputs).</span>

<span class="ss">        Raises:</span>

<span class="ss">          RuntimeError: If called in Eager mode.</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span>

<span class="w">            </span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;input_shapes&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;input shape&quot;</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_output_at_1">get_output_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first output node of the layer.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first output node of the layer.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A tensor (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">        Raises:</span>

<span class="ss">          RuntimeError: If called in Eager mode.</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span>

<span class="w">            </span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;output_tensors&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;output&quot;</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_output_mask_at_1">get_output_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first time the layer was called.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A mask tensor</span>

<span class="ss">            (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_output_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &quot;_keras_mask&quot;, None) for x in output</span><span class="o">]</span>

<span class="w">        </span><span class="k">else</span><span class="err">:</span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;_keras_mask&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_output_shape_at_1">get_output_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>None</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Retrieves the output shape(s) of a layer at a given node.</span>

<span class="ss">        Args:</span>

<span class="ss">            node_index: Integer, index of the node</span>

<span class="ss">                from which to retrieve the attribute.</span>

<span class="ss">                E.g. `node_index=0` will correspond to the</span>

<span class="ss">                first time the layer was called.</span>

<span class="ss">        Returns:</span>

<span class="ss">            A shape tuple</span>

<span class="ss">            (or list of shape tuples if the layer has multiple outputs).</span>

<span class="ss">        Raises:</span>

<span class="ss">          RuntimeError: If called in Eager mode.</span>

<span class="ss">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span>

<span class="w">            </span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;output_shapes&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;output shape&quot;</span>

<span class="w">        </span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="get_weights_1">get_weights</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Returns the current weights of the layer, as NumPy arrays.</p>
<p>The weights of a layer represent the state of the layer. This function
returns both trainable and non-trainable weight values associated with
this layer as a list of NumPy arrays, which can in turn be used to load
state into similarly parameterized layers.</p>
<p>For example, a <code>Dense</code> layer returns a list of two values: the kernel
matrix and the bias vector. These can be used to set the weights of
another <code>Dense</code> layer:</p>
<blockquote>
<blockquote>
<blockquote>
<p>layer_a = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Weights values as a list of NumPy arrays.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the current weights of the layer, as NumPy arrays.</span>

<span class="sd">        The weights of a layer represent the state of the layer. This function</span>

<span class="sd">        returns both trainable and non-trainable weight values associated with</span>

<span class="sd">        this layer as a list of NumPy arrays, which can in turn be used to load</span>

<span class="sd">        state into similarly parameterized layers.</span>

<span class="sd">        For example, a `Dense` layer returns a list of two values: the kernel</span>

<span class="sd">        matrix and the bias vector. These can be used to set the weights of</span>

<span class="sd">        another `Dense` layer:</span>

<span class="sd">        &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,</span>

<span class="sd">        ...   kernel_initializer=tf.constant_initializer(1.))</span>

<span class="sd">        &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))</span>

<span class="sd">        &gt;&gt;&gt; layer_a.get_weights()</span>

<span class="sd">        [array([[1.],</span>

<span class="sd">               [1.],</span>

<span class="sd">               [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">        &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,</span>

<span class="sd">        ...   kernel_initializer=tf.constant_initializer(2.))</span>

<span class="sd">        &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))</span>

<span class="sd">        &gt;&gt;&gt; layer_b.get_weights()</span>

<span class="sd">        [array([[2.],</span>

<span class="sd">               [2.],</span>

<span class="sd">               [2.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">        &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())</span>

<span class="sd">        &gt;&gt;&gt; layer_b.get_weights()</span>

<span class="sd">        [array([[1.],</span>

<span class="sd">               [1.],</span>

<span class="sd">               [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">        Returns:</span>

<span class="sd">            Weights values as a list of NumPy arrays.</span>

<span class="sd">        &quot;&quot;&quot;</span>

<span class="w">        </span><span class="n">weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

<span class="w">        </span><span class="n">output_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">weights</span><span class="p">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span>

<span class="w">                </span><span class="n">output_weights</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">get_tensors</span><span class="p">())</span>

<span class="w">            </span><span class="k">else</span><span class="p">:</span>

<span class="w">                </span><span class="n">output_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">backend</span><span class="o">.</span><span class="n">batch_get_value</span><span class="p">(</span><span class="n">output_weights</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="set_weights_1">set_weights</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">weights</span>
<span class="p">)</span>
</code></pre></div>

<p>Sets the weights of the layer, from NumPy arrays.</p>
<p>The weights of a layer represent the state of the layer. This function
sets the weight values from numpy arrays. The weight values should be
passed in the order they are created by the layer. Note that the layer's
weights must be instantiated before calling this function, by calling
the layer.</p>
<p>For example, a <code>Dense</code> layer returns a list of two values: the kernel
matrix and the bias vector. These can be used to set the weights of
another <code>Dense</code> layer:</p>
<blockquote>
<blockquote>
<blockquote>
<p>layer_a = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>weights</td>
<td>None</td>
<td>a list of NumPy arrays. The number<br>of arrays and their shape must match<br>number of the dimensions of the weights<br>of the layer (i.e. it should match the<br>output of <code>get_weights</code>).</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>If the provided weights list does not match the<br>layer's specifications.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">set_weights</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">)</span><span class="o">:</span>

<span class="w">        </span><span class="s">&quot;&quot;&quot;Sets the weights of the layer, from NumPy arrays.</span>

<span class="w">        </span><span class="n">The</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">represent</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="p">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">function</span>

<span class="w">        </span><span class="n">sets</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">numpy</span><span class="w"> </span><span class="n">arrays</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span>

<span class="w">        </span><span class="n">passed</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">order</span><span class="w"> </span><span class="n">they</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">created</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="p">.</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="err">&#39;</span><span class="n">s</span>

<span class="w">        </span><span class="n">weights</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">instantiated</span><span class="w"> </span><span class="n">before</span><span class="w"> </span><span class="n">calling</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">function</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">calling</span>

<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="p">.</span>

<span class="w">        </span><span class="n">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="err">`</span><span class="n">Dense</span><span class="err">`</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="n">returns</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">values</span><span class="o">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">kernel</span>

<span class="w">        </span><span class="n">matrix</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="n">vector</span><span class="p">.</span><span class="w"> </span><span class="n">These</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">of</span>

<span class="w">        </span><span class="n">another</span><span class="w"> </span><span class="err">`</span><span class="n">Dense</span><span class="err">`</span><span class="w"> </span><span class="n">layer</span><span class="o">:</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>

<span class="w">        </span><span class="p">...</span><span class="w">   </span><span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">a_out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">layer_a</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="w"> </span><span class="mf">2.</span><span class="p">,</span><span class="w"> </span><span class="mf">3.</span><span class="p">]]))</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_a</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="w">        </span><span class="p">[</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">1.</span><span class="p">]],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)]</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>

<span class="w">        </span><span class="p">...</span><span class="w">   </span><span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">2.</span><span class="p">))</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">b_out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">layer_b</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">([[</span><span class="mf">10.</span><span class="p">,</span><span class="w"> </span><span class="mf">20.</span><span class="p">,</span><span class="w"> </span><span class="mf">30.</span><span class="p">]]))</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_b</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="w">        </span><span class="p">[</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">2.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">2.</span><span class="p">]],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)]</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_b</span><span class="p">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">layer_a</span><span class="p">.</span><span class="n">get_weights</span><span class="p">())</span>

<span class="w">        </span><span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="n">layer_b</span><span class="p">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="w">        </span><span class="p">[</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>

<span class="w">               </span><span class="p">[</span><span class="mf">1.</span><span class="p">]],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">],</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)]</span>

<span class="w">        </span><span class="nl">Args</span><span class="p">:</span>

<span class="w">          </span><span class="nl">weights</span><span class="p">:</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">NumPy</span><span class="w"> </span><span class="n">arrays</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">number</span>

<span class="w">            </span><span class="n">of</span><span class="w"> </span><span class="n">arrays</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">match</span>

<span class="w">            </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">weights</span>

<span class="w">            </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">e</span><span class="p">.</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">match</span><span class="w"> </span><span class="n">the</span>

<span class="w">            </span><span class="n">output</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="err">`</span><span class="n">get_weights</span><span class="err">`</span><span class="p">).</span>

<span class="w">        </span><span class="nl">Raises</span><span class="p">:</span>

<span class="w">          </span><span class="nl">ValueError</span><span class="p">:</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">match</span><span class="w"> </span><span class="n">the</span>

<span class="w">            </span><span class="n">layer</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">specifications</span><span class="p">.</span>

<span class="w">        </span><span class="s">&quot;&quot;&quot;</span>

<span class="w">        </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">weights</span>

<span class="w">        </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">params</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">TrackableWeightHandler</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">param</span><span class="p">.</span><span class="n">num_tensors</span>

<span class="w">            </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">:</span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                </span><span class="err">&#39;</span><span class="n">You</span><span class="w"> </span><span class="n">called</span><span class="w"> </span><span class="err">`</span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="err">`</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="s">&quot;%s&quot;</span><span class="w"> </span><span class="err">&#39;</span>

<span class="w">                </span><span class="s">&quot;with a weight list of length %s, but the layer was &quot;</span>

<span class="w">                </span><span class="s">&quot;expecting %s weights. Provided weights: %s...&quot;</span>

<span class="w">                </span><span class="o">%</span><span class="w"> </span><span class="p">(</span>

<span class="w">                    </span><span class="nb">self</span><span class="p">.</span><span class="n">name</span><span class="p">,</span>

<span class="w">                    </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span>

<span class="w">                    </span><span class="n">expected_num_weights</span><span class="p">,</span>

<span class="w">                    </span><span class="n">str</span><span class="p">(</span><span class="n">weights</span><span class="p">)[</span><span class="o">:</span><span class="mi">50</span><span class="p">],</span>

<span class="w">                </span><span class="p">)</span>

<span class="w">            </span><span class="p">)</span>

<span class="w">        </span><span class="n">weight_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>

<span class="w">        </span><span class="n">weight_value_tuples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">params</span><span class="o">:</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">TrackableWeightHandler</span><span class="p">)</span><span class="o">:</span>

<span class="w">                </span><span class="n">num_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="p">.</span><span class="n">num_tensors</span>

<span class="w">                </span><span class="n">tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">num_tensors</span><span class="p">]</span>

<span class="w">                </span><span class="n">param</span><span class="p">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>

<span class="w">                </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">num_tensors</span>

<span class="w">            </span><span class="nl">else</span><span class="p">:</span>

<span class="w">                </span><span class="n">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">]</span>

<span class="w">                </span><span class="n">weight_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">hasattr</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;shape&quot;</span><span class="p">)</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">()</span>

<span class="w">                </span><span class="n">ref_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="p">.</span><span class="n">shape</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">ref_shape</span><span class="p">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">)</span><span class="o">:</span>

<span class="w">                    </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span>

<span class="w">                        </span><span class="n">f</span><span class="s">&quot;Layer {self.name} weight shape {ref_shape} &quot;</span>

<span class="w">                        </span><span class="s">&quot;is not compatible with provided weight &quot;</span>

<span class="w">                        </span><span class="n">f</span><span class="s">&quot;shape {weight_shape}.&quot;</span>

<span class="w">                    </span><span class="p">)</span>

<span class="w">                </span><span class="n">weight_value_tuples</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">))</span>

<span class="w">                </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>

<span class="w">        </span><span class="n">backend</span><span class="p">.</span><span class="n">batch_set_value</span><span class="p">(</span><span class="n">weight_value_tuples</span><span class="p">)</span>

<span class="w">        </span><span class="cp"># Perform any layer defined finalization of the layer state.</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nb">self</span><span class="p">.</span><span class="n">_flatten_layers</span><span class="p">()</span><span class="o">:</span>

<span class="w">            </span><span class="n">layer</span><span class="p">.</span><span class="n">finalize_state</span><span class="p">()</span>
</code></pre></div>

</details>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        
<footer class="md-footer">
    
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../transformer/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Transformer" rel="prev">
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <div class="md-ellipsis">
                <span class="md-footer__direction">
                  Previous
                </span>
                Transformer
              </div>
            </div>
          </a>
        
        
      </nav>
    
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
            
            Powered by
            <a href="http://timothycrosley.github.io/portray">portray.</a>
            You too can
            <a href="http://timothycrosley.github.io/portray">
              portray</a>
            your Python project well using automatic documentation.
          </div>
        
      </div>
    </div>
  </footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.078830c0.min.js"></script>
      
    
  </body>
</html>