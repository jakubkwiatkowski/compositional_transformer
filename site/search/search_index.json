{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Grid Transformer The Grid Transformer is a research library that offers utilities for building transformer models for grid-like input data. This includes data samples that consist of sets of objects with relational relationships, such as Raven Matrices or Sudoku puzzles. It was developed for research purposes by the Neurosymbolic Systems Lab at Poznan University of Technology and the Pozna\u0144 Supercomputing and Networking Center . For usage examples, please refer to the: .... This library includes a few utilities that were either copied or modified from code examples published by the Keras community. The original code can be found on the Keras website at keras.io/examples . There are many useful resources on building transformers using Keras available on their website. Below are links to some of these helpful examples: https://keras.io/examples/nlp/text_classification_with_transformer/ https://keras.io/examples/vision/image_classification_with_vision_transformer/ https://keras.io/examples/vision/deit/ https://keras.io/examples/vision/object_detection_using_vision_transformer/ Getting started Installation To get started with the tool, follow these installation instructions: Usage ... Projects/Publications Research projects that use this tool: ... Comments The documentation was partly created by ChatGPT, a language model developed by OpenAI. This tool is licensed under the MIT License. Please see the LICENSE file for more information.","title":"Home"},{"location":"#grid-transformer","text":"The Grid Transformer is a research library that offers utilities for building transformer models for grid-like input data. This includes data samples that consist of sets of objects with relational relationships, such as Raven Matrices or Sudoku puzzles. It was developed for research purposes by the Neurosymbolic Systems Lab at Poznan University of Technology and the Pozna\u0144 Supercomputing and Networking Center . For usage examples, please refer to the: .... This library includes a few utilities that were either copied or modified from code examples published by the Keras community. The original code can be found on the Keras website at keras.io/examples . There are many useful resources on building transformers using Keras available on their website. Below are links to some of these helpful examples: https://keras.io/examples/nlp/text_classification_with_transformer/ https://keras.io/examples/vision/image_classification_with_vision_transformer/ https://keras.io/examples/vision/deit/ https://keras.io/examples/vision/object_detection_using_vision_transformer/","title":"Grid Transformer"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#installation","text":"To get started with the tool, follow these installation instructions:","title":"Installation"},{"location":"#usage","text":"...","title":"Usage"},{"location":"#projectspublications","text":"Research projects that use this tool: ...","title":"Projects/Publications"},{"location":"#comments","text":"The documentation was partly created by ChatGPT, a language model developed by OpenAI. This tool is licensed under the MIT License. Please see the LICENSE file for more information.","title":"Comments"},{"location":"docs/raven/","text":"RAVEN","title":"Raven"},{"location":"docs/raven/#raven","text":"","title":"RAVEN"},{"location":"docs/sudoku/","text":"Sudoku","title":"Sudoku"},{"location":"docs/sudoku/#sudoku","text":"","title":"Sudoku"},{"location":"reference/grid_transformer/","text":"Module grid_transformer View Source from grid_transformer.transformer_block import TransformerBlock , TransformerBlockBase from grid_transformer.parameters import TransformerParameters , SmallTransformerParameters , BigTransformerParameters , LargeTransformerParameters from grid_transformer.transformer import transformer from grid_transformer.augmented_transformer import augmented_transformer from grid_transformer.grid_transformer import grid_transformer Sub-modules grid_transformer.augmentation grid_transformer.augmented_transformer grid_transformer.grid_transformer grid_transformer.layers grid_transformer.mask grid_transformer.parameters grid_transformer.position_embedding grid_transformer.preprocessing grid_transformer.simple_transformer grid_transformer.transformer grid_transformer.transformer_block","title":"Index"},{"location":"reference/grid_transformer/#module-grid_transformer","text":"View Source from grid_transformer.transformer_block import TransformerBlock , TransformerBlockBase from grid_transformer.parameters import TransformerParameters , SmallTransformerParameters , BigTransformerParameters , LargeTransformerParameters from grid_transformer.transformer import transformer from grid_transformer.augmented_transformer import augmented_transformer from grid_transformer.grid_transformer import grid_transformer","title":"Module grid_transformer"},{"location":"reference/grid_transformer/#sub-modules","text":"grid_transformer.augmentation grid_transformer.augmented_transformer grid_transformer.grid_transformer grid_transformer.layers grid_transformer.mask grid_transformer.parameters grid_transformer.position_embedding grid_transformer.preprocessing grid_transformer.simple_transformer grid_transformer.transformer grid_transformer.transformer_block","title":"Sub-modules"},{"location":"reference/grid_transformer/augmentation/","text":"Module grid_transformer.augmentation This module contains a collection of functions and classes for working with tensors in TensorFlow. Functions: `shuffle_fn : A wrapper function for shuffle layer that allows for customization of the axis along which to shuffle. shuffle : A callable function that applies the shuffle operation on a given tensor with default axis 0. shuffle_col : A callable function that applies the shuffle operation on a given tensor with axis 1. transpose : A partial function that returns a transpose layer, which can be used to transpose the dimensions of a tensor. reshape_static : A function to reshape a tensor in a specific shape and apply a given function to it. Classes: Transpose(Layer) : Returns a transpose layer, which can be used to transpose the dimensions of a tensor. PartialModel(Layer) : Returns a PartialModel layer that allows for applying a function to a subset of the input tensor. Example usage: import tensorflow as tf from models_utils import reshape_static , transpose # Create a tensor of shape (2,3) x = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) # Reshape the tensor to shape (2, 2, 3) and apply transpose function y = reshape_static ([ 2 , 2 , 3 ], transpose )( x ) print ( y ) # Output: # [[[1 3] # [2 4]] # [[5 6] # [3 2]]] import numpy as np from models_utils import PartialModel # Create a tensor of shape (2, 2, 3) x = np . random . rand ( 2 , 2 , 3 ) # Create a partial model layer with indices [0, 1] and function sum pm = PartialModel ( fn = tf . reduce_sum , indices = [ 0 , 1 ]) # Apply the partial model layer on input tensor y = pm ( x ) print ( y ) # Output: # [[1.7268854 1.80438087 1.35692811] # [1.74081294 1.70722026 1.45006824]] In this example, the function tf.reduce_sum is applied to the first two indices of the input tensor x and the resulting tensor is returned. View Source \"\"\" This module contains a collection of functions and classes for working with tensors in TensorFlow. Functions: - `shuffle_fn : A wrapper function for shuffle layer that allows for customization of the axis along which to shuffle. - `shuffle`: A callable function that applies the shuffle operation on a given tensor with default axis 0. - `shuffle_col`: A callable function that applies the shuffle operation on a given tensor with axis 1. - `transpose`: A partial function that returns a transpose layer, which can be used to transpose the dimensions of a tensor. - `reshape_static`: A function to reshape a tensor in a specific shape and apply a given function to it. Classes: - `Transpose(Layer)`: Returns a transpose layer, which can be used to transpose the dimensions of a tensor. - `PartialModel(Layer)`: Returns a PartialModel layer that allows for applying a function to a subset of the input tensor. Example usage: import tensorflow as tf from models_utils import reshape_static, transpose # Create a tensor of shape (2,3) x = tf.constant([[1,2,3], [4,5,6]]) # Reshape the tensor to shape (2, 2, 3) and apply transpose function y = reshape_static([2, 2, 3], transpose)(x) print(y) # Output: # [[[1 3] # [2 4]] # [[5 6] # [3 2]]] import numpy as np from models_utils import PartialModel # Create a tensor of shape (2, 2, 3) x = np.random.rand(2, 2, 3) # Create a partial model layer with indices [0, 1] and function sum pm = PartialModel(fn=tf.reduce_sum, indices=[0, 1]) # Apply the partial model layer on input tensor y = pm(x) print(y) # Output: # [[1.7268854 1.80438087 1.35692811] # [1.74081294 1.70722026 1.45006824]] In this example, the function `tf.reduce_sum` is applied to the first two indices of the input tensor `x` and the resulting tensor is returned. \"\"\" from functools import partial import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.layers import Layer , Lambda from ml_utils import il , dict_from_list , lw import models_utils.ops.list_ import models_utils.ops.rand from models_utils import ops as K , InferencePass , random_update import math from typing import List , Union , Callable , Optional , Tuple def shuffle_fn ( axis : int = 0 ) -> Callable : \"\"\" A wrapper function for Keras's shuffle layer that allows for customization of the axis along which to shuffle. :param axis: The axis along which to shuffle. Default is 0. :type axis: int :return: A callable function that applies the shuffle operation on a given tensor. :rtype: Callable \"\"\" return K . vec ( K . shuffle , axis = axis ) shuffle = shuffle_fn () shuffle_col = shuffle_fn ( axis = 1 ) transpose = partial ( tf . transpose , perm = ( 0 , 2 , 1 , 3 , 4 )) def reshape_static ( shape : Union [ List [ int ], tf . TensorShape ], model : Callable = shuffle , row : Union [ None , int ] = None ) -> Callable : \"\"\" A function to reshape a tensor in a specific shape and apply a given function to it. :param shape: The shape of the reshaped tensor. It can be a list of integers or a tf.TensorShape object. :type shape: Union[List[int], tf.TensorShape] :param model: A callable function to apply on the reshaped tensor. Default is the shuffle function. :type model: Callable :param row: If provided, number of row in reshaped tensor, use to calculate the reshape shape. :type row: Union[None, int] :return: A callable function that reshape the tensor and apply the given function. :rtype: Callable \"\"\" if not il ( shape ): shape = tuple ( shape . shape ) if row is None : row = math . floor ( math . sqrt ( shape [ 1 ])) newshape = [ - 1 ] + [ row , row ] + list ( shape [ 2 :]) shape = ( - 1 ,) + shape [ 1 :] def fn ( x ): x = tf . reshape ( x , newshape ) x = model ( x ) x = tf . reshape ( x , shape ) return x return fn class Transpose ( Layer ): \"\"\" This class returns a transpose layer, which can be used to transpose the dimensions of a tensor. \"\"\" def __init__ ( self , axis : tuple = ( 1 , 0 )): \"\"\" Initialize the Transpose layer :param axis: The axis along which to transpose the tensor. Defaults to (1, 0). :type axis: tuple \"\"\" super () . __init__ () self . axis = axis def build ( self , input_shape : tuple ) -> None : \"\"\" Constructs the transpose layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" dim = len ( input_shape ) axis_len = len ( self . axis ) if axis_len < dim : self . axis = tuple ( self . axis ) + tuple ( range ( dim ))[ axis_len :] def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the transposed tensor. :param inputs: Input tensor to be transposed. :type inputs: tf.Tensor :return: Transposed tensor. :rtype: tf.Tensor \"\"\" return tf . transpose ( inputs , self . axis ) class PartialModel ( Layer ): \"\"\" This class returns a PartialModel layer, which can be used to apply a model on a subset of the input tensor. \"\"\" def __init__ ( self , model : Model , last_axis : int = 8 ): \"\"\" Initialize the PartialModel layer :param model: The model to be applied to the subset of the input tensor :type model: Model :param last_axis: The last axis on which the model should be applied. Defaults to 8. :type last_axis: int \"\"\" super () . __init__ () self . model = model self . last_axis = last_axis def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the combined output of model and the original tensor. :param inputs: Input tensor to be processed. :type inputs: tf.Tensor :return: combined output tensor of model and the original tensor. :rtype: tf.Tensor \"\"\" x = self . model ( inputs [:, : self . last_axis ]) return tf . concat ([ x , inputs [:, self . last_axis :]], axis = 1 ) def noise_seq ( inputs : tf . Tensor , row : Optional [ int ] = None , col : Optional [ int ] = None , sample_from : str = \"sample\" ) -> tf . Tensor : \"\"\" This function adds noise to a sequence by updating some of its elements. :param inputs: the input sequence to be manipulated. :type inputs: tf.Tensor :param row: the number of rows of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type row: int :param col: the number of columns of the input sequence. If not provided, it defaults to the third dimension of the input tensor shape. :type col: int :param sample_from: a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. :type sample_from: str :return: a tensor with the same shape as the input but with some of its elements updated. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) row = row if row else shape [ 1 ] col = col if col else shape [ 2 ] batch = shape [ 0 ] indexes = K . tensor ([ tf . range ( batch ), K . init . label ( max = row , shape = batch [ None ]), K . init . label ( max = col , shape = batch [ None ]) ]) . T updates = ([ K . init . label ( max = batch , shape = batch [ None ]) if sample_from == \"batch\" else tf . range ( batch ), K . init . label ( max = row , shape = batch [ None ]), K . init . label ( max = col , shape = batch [ None ]) ]) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) class Noise ( Layer ): def __init__ ( self , last_index : Optional [ int ] = None , sample_from : str = \"sample\" , prob : float = 1.0 ): \"\"\" Initialize the Noise layer :param last_index: the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type last_index: int :param sample_from: a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. :type sample_from: str :param prob: a float value between 0 and 1 that represents the probability of applying noise to the input. :type prob: float \"\"\" super () . __init__ () self . last_index = last_index self . sample_from = sample_from self . prob = prob def build ( self , input_shape : Tuple [ int ]) -> None : \"\"\" Constructs the noise layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" if not self . last_index : self . last_index = input_shape [ 1 ] def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) switch_indexes = tf . range ( shape [ 0 ]) if self . prob < 1.0 : switch_indexes = switch_indexes [ K . categorical_like ( inputs , prob = self . prob , dtype = \"bool\" )[ 0 ]] switch_no = tf . shape ( switch_indexes )[ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ([ switch_indexes , K . init . label ( max = self . last_index , shape = switch_no [ None ]), ]), perm = ( 1 , 0 )) updates = ([ K . init . label ( max = switch_no , shape = switch_no [ None ]) if self . sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = self . last_index , shape = switch_no [ None ]), ]) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) BatchNoise = partial ( Noise , sample_from = \"batch\" ) def noise ( inputs : tf . Tensor , last_index : Optional [ int ] = None , sample_from : str = \"sample\" , prob : float = 1.0 ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :param last_index: the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type last_index: int :param sample_from: a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. :type sample_from: str :param prob: a float value between 0 and 1 that represents the probability of applying noise to the input. :type prob: float :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) last_index = last_index if last_index else shape [ 1 ] switch_indexes = tf . range ( shape [ 0 ]) if prob < 1.0 : switch_indexes = switch_indexes [ K . categorical_like ( inputs , prob = prob , dtype = \"bool\" )[ 0 ]] switch_no = tf . shape ( switch_indexes )[ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ([ switch_indexes , K . init . label ( max = last_index , shape = switch_no [ None ]), ]), perm = ( 1 , 0 )) updates = ([ K . init . label ( max = switch_no , shape = switch_no [ None ]) if sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = last_index , shape = switch_no [ None ]), ]) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) def rand ( augmentation : Union [ float , int , List [ Union [ float , Tuple [ float , callable ]]], Tuple [ float , callable ]], root : int = 2 , transpose : bool = True ) -> Callable : \"\"\" This function will take augmentation,root and transpose as input and it will return a callable function :param augmentation: a list of floats or tuples of floats and callable functions that represents the probability of applying noise to the input. :type augmentation: Union[float, int, List[Union[float, Tuple[float, callable]]], Tuple[float, callable]] :param root: an integer that represents the root of the number of the augmentation functions. :type root: int :param transpose: a boolean value that represents whether the input should be transposed or not. :type transpose: bool :return: callable function :rtype: callable \"\"\" augmentation_fn = [ ( lambda x = m : K . list_ . DivideDim ( x , root = root )) for m in ( [ shuffle_fn ( i ) for i in range ( root )] + ([ Transpose (( 0 , 2 , 1 ))] if transpose else []) ) ] if isinstance ( augmentation , ( float , int )): augmentation = [ augmentation ] * len ( augmentation_fn ) aug = [] for i , a in enumerate ( lw ( augmentation )): if len ( lw ( a )) == 2 : aug . append ( a ) else : aug . append (( a , augmentation_fn [ i ]())) def apply ( * args ): indexes = Lambda ( K . range_like )( args [ 0 ]) # aug_indexes = K.range_like(inputs) for k , v in aug : if k > 0 : indexes = random_update ( indexes , model = v , prob = k ) outputs = [ K . gather ( a , indexes ) for a in args ] return tuple ( outputs ) return apply def rand_aug4 ( * args , prob : Optional [ float ] = None , vec : Optional [ bool ] = True ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: boolean, whether to use vectorized version of layers or not :return: list of augmented inputs \"\"\" layers = [ partial ( K . shuffle , axis = 0 ), K . shuffle , partial ( tf . transpose , perm = ( 1 , 0 ))] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec ) def rand_aug2 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = K . list_ . run ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: function to use for vectorizing the input list, defaults to `K.list_.run` :return: list of augmented inputs \"\"\" layers = [ partial ( K . list_ . shuffle , axis = 0 ), K . list_ . shuffle , partial ( K . list_ . transpose )] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec ) def rand_aug3 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = K . list_ . run ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: function to use for vectorizing the input list, defaults to `K.list_.run` :return: list of augmented inputs \"\"\" layers = [ DivideShape ( f , axis = 0 , replace_first = False ) for f in [ partial ( K . list_ . shuffle , axis = 0 ), K . list_ . shuffle , partial ( K . list_ . transpose )]] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec ) batch_noise_seq = partial ( noise_seq , sample_from = 'batch' ) batch_noise = partial ( noise , sample_from = 'batch' ) Variables BatchNoise batch_noise batch_noise_seq transpose Functions noise def noise ( inputs : tensorflow . python . framework . ops . Tensor , last_index : Optional [ int ] = None , sample_from : str = 'sample' , prob : float = 1.0 ) -> tensorflow . python . framework . ops . Tensor This function will take inputs tensor as input and it will return the noise tensor. Parameters: Name Type Description Default inputs tf.Tensor Input tensor to be manipulated. None last_index int the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. the second dimension of the input tensor shape sample_from str a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. None prob float a float value between 0 and 1 that represents the probability of applying noise to the input. None Returns: Type Description tf.Tensor Tensor with noise added. View Source def noise ( inputs : tf . Tensor , last_index : Optional [ int ] = None , sample_from : str = \"sample\" , prob : float = 1.0 ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :param last_index: the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type last_index: int :param sample_from: a string that specifies the noise source. \" sample \" (default) will sample from the input sequence while \" batch \" will sample from the batch. :type sample_from: str :param prob: a float value between 0 and 1 that represents the probability of applying noise to the input. :type prob: float :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) last_index = last_index if last_index else shape [ 1 ] switch_indexes = tf . range ( shape [ 0 ] ) if prob < 1.0 : switch_indexes = switch_indexes [ K.categorical_like(inputs, prob=prob, dtype=\"bool\")[0 ] ] switch_no = tf . shape ( switch_indexes ) [ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ( [ switch_indexes, K.init.label(max=last_index, shape=switch_no[None ] ), ] ), perm = ( 1 , 0 )) updates = ( [ K.init.label(max=switch_no, shape=switch_no[None ] ) if sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = last_index , shape = switch_no [ None ] ), ] ) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) noise_seq def noise_seq ( inputs : tensorflow . python . framework . ops . Tensor , row : Optional [ int ] = None , col : Optional [ int ] = None , sample_from : str = 'sample' ) -> tensorflow . python . framework . ops . Tensor This function adds noise to a sequence by updating some of its elements. Parameters: Name Type Description Default inputs tf.Tensor the input sequence to be manipulated. None row int the number of rows of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. the second dimension of the input tensor shape col int the number of columns of the input sequence. If not provided, it defaults to the third dimension of the input tensor shape. the third dimension of the input tensor shape sample_from str a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. None Returns: Type Description tf.Tensor a tensor with the same shape as the input but with some of its elements updated. View Source def noise_seq ( inputs : tf . Tensor , row : Optional [ int ] = None , col : Optional [ int ] = None , sample_from : str = \"sample\" ) -> tf . Tensor : \"\"\" This function adds noise to a sequence by updating some of its elements. :param inputs: the input sequence to be manipulated. :type inputs: tf.Tensor :param row: the number of rows of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type row: int :param col: the number of columns of the input sequence. If not provided, it defaults to the third dimension of the input tensor shape. :type col: int :param sample_from: a string that specifies the noise source. \" sample \" (default) will sample from the input sequence while \" batch \" will sample from the batch. :type sample_from: str :return: a tensor with the same shape as the input but with some of its elements updated. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) row = row if row else shape [ 1 ] col = col if col else shape [ 2 ] batch = shape [ 0 ] indexes = K . tensor ( [ tf.range(batch), K.init.label(max=row, shape=batch[None ] ), K . init . label ( max = col , shape = batch [ None ] ) ] ). T updates = ( [ K.init.label(max=batch, shape=batch[None ] ) if sample_from == \"batch\" else tf . range ( batch ), K . init . label ( max = row , shape = batch [ None ] ), K . init . label ( max = col , shape = batch [ None ] ) ] ) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) rand def rand ( augmentation : Union [ float , int , List [ Union [ float , Tuple [ float , < built - in function callable > ]]], Tuple [ float , < built - in function callable > ]], root : int = 2 , transpose : bool = True ) -> Callable This function will take augmentation,root and transpose as input and it will return a callable function Parameters: Name Type Description Default augmentation Union[float, int, List[Union[float, Tuple[float, callable]]], Tuple[float, callable]] a list of floats or tuples of floats and callable functions that represents the probability of applying noise to the input. None root int an integer that represents the root of the number of the augmentation functions. None transpose bool a boolean value that represents whether the input should be transposed or not. None Returns: Type Description callable callable function View Source def rand ( augmentation : Union [ float, int, List[Union[float, Tuple[float, callable ] ]] , Tuple [ float, callable ] ] , root : int = 2 , transpose : bool = True ) -> Callable : \"\"\" This function will take augmentation,root and transpose as input and it will return a callable function :param augmentation: a list of floats or tuples of floats and callable functions that represents the probability of applying noise to the input. :type augmentation: Union[float, int, List[Union[float, Tuple[float, callable]]], Tuple[float, callable]] :param root: an integer that represents the root of the number of the augmentation functions. :type root: int :param transpose: a boolean value that represents whether the input should be transposed or not. :type transpose: bool :return: callable function :rtype: callable \"\"\" augmentation_fn = [ (lambda x=m: K.list_.DivideDim(x, root=root)) for m in ( [shuffle_fn(i) for i in range(root) ] + ( [ Transpose((0, 2, 1)) ] if transpose else [] ) ) ] if isinstance ( augmentation , ( float , int )) : augmentation = [ augmentation ] * len ( augmentation_fn ) aug = [] for i , a in enumerate ( lw ( augmentation )) : if len ( lw ( a )) == 2 : aug . append ( a ) else : aug . append (( a , augmentation_fn [ i ] ())) def apply ( * args ) : indexes = Lambda ( K . range_like )( args [ 0 ] ) # aug_indexes = K . range_like ( inputs ) for k , v in aug : if k > 0 : indexes = random_update ( indexes , model = v , prob = k ) outputs = [ K.gather(a, indexes) for a in args ] return tuple ( outputs ) return apply rand_aug2 def rand_aug2 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = < function run at 0x7ffa73f46d40 > ) -> List Apply random augmentation to a list of inputs with the specified probability. Parameters: Name Type Description Default args None list of inputs to be augmented None prob None probability of applying augmentation to each input. If None, it will be set to 1/len(args) None vec None function to use for vectorizing the input list, defaults to K.list_.run K.list_.run Returns: Type Description None list of augmented inputs View Source def rand_aug2 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = K . list_ . run ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: function to use for vectorizing the input list, defaults to `K.list_.run` :return: list of augmented inputs \"\"\" layers = [ partial(K.list_.shuffle, axis=0), K.list_.shuffle, partial(K.list_.transpose) ] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec ) rand_aug3 def rand_aug3 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = < function run at 0x7ffa73f46d40 > ) -> List Apply random augmentation to a list of inputs with the specified probability. Parameters: Name Type Description Default args None list of inputs to be augmented None prob None probability of applying augmentation to each input. If None, it will be set to 1/len(args) None vec None function to use for vectorizing the input list, defaults to K.list_.run K.list_.run Returns: Type Description None list of augmented inputs View Source def rand_aug3 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = K . list_ . run ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: function to use for vectorizing the input list, defaults to `K.list_.run` :return: list of augmented inputs \"\"\" layers = [ DivideShape(f, axis=0, replace_first=False) for f in [partial(K.list_.shuffle, axis=0), K.list_.shuffle, partial(K.list_.transpose) ] ] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec ) rand_aug4 def rand_aug4 ( * args , prob : Optional [ float ] = None , vec : Optional [ bool ] = True ) -> List Apply random augmentation to a list of inputs with the specified probability. Parameters: Name Type Description Default args None list of inputs to be augmented None prob None probability of applying augmentation to each input. If None, it will be set to 1/len(args) None vec None boolean, whether to use vectorized version of layers or not None Returns: Type Description None list of augmented inputs View Source def rand_aug4 ( * args , prob : Optional [ float ] = None , vec : Optional [ bool ] = True ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: boolean, whether to use vectorized version of layers or not :return: list of augmented inputs \"\"\" layers = [ partial(K.shuffle, axis=0), K.shuffle, partial(tf.transpose, perm=(1, 0)) ] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec ) reshape_static def reshape_static ( shape : Union [ List [ int ], tensorflow . python . framework . tensor_shape . TensorShape ], model : Callable = < function vec .< locals >. wrapper at 0x7ff9a690aef0 > , row : Optional [ int ] = None ) -> Callable A function to reshape a tensor in a specific shape and apply a given function to it. Parameters: Name Type Description Default shape Union[List[int], tf.TensorShape] The shape of the reshaped tensor. It can be a list of integers or a tf.TensorShape object. None model Callable A callable function to apply on the reshaped tensor. Default is the shuffle function. None row Union[None, int] If provided, number of row in reshaped tensor, use to calculate the reshape shape. None Returns: Type Description Callable A callable function that reshape the tensor and apply the given function. View Source def reshape_static ( shape : Union [ List[int ] , tf . TensorShape ] , model : Callable = shuffle , row : Union [ None, int ] = None ) -> Callable : \"\"\" A function to reshape a tensor in a specific shape and apply a given function to it. :param shape: The shape of the reshaped tensor. It can be a list of integers or a tf.TensorShape object. :type shape: Union[List[int], tf.TensorShape] :param model: A callable function to apply on the reshaped tensor. Default is the shuffle function. :type model: Callable :param row: If provided, number of row in reshaped tensor, use to calculate the reshape shape. :type row: Union[None, int] :return: A callable function that reshape the tensor and apply the given function. :rtype: Callable \"\"\" if not il ( shape ) : shape = tuple ( shape . shape ) if row is None : row = math . floor ( math . sqrt ( shape [ 1 ] )) newshape = [ -1 ] + [ row, row ] + list ( shape [ 2: ] ) shape = ( - 1 ,) + shape [ 1: ] def fn ( x ) : x = tf . reshape ( x , newshape ) x = model ( x ) x = tf . reshape ( x , shape ) return x return fn shuffle def shuffle ( inputs ) View Source def wrapper ( inputs ) : # tf . print ( f \"Run: {fn.keywords}\" ) return tf . vectorized_map ( fn , inputs ) shuffle_col def shuffle_col ( inputs ) View Source def wrapper ( inputs ) : # tf . print ( f \"Run: {fn.keywords}\" ) return tf . vectorized_map ( fn , inputs ) shuffle_fn def shuffle_fn ( axis : int = 0 ) -> Callable A wrapper function for Keras's shuffle layer that allows for customization of the axis along which to shuffle. Parameters: Name Type Description Default axis int The axis along which to shuffle. Default is 0. None Returns: Type Description Callable A callable function that applies the shuffle operation on a given tensor. View Source def shuffle_fn ( axis : int = 0 ) -> Callable : \"\"\" A wrapper function for Keras' s shuffle layer that allows for customization of the axis along which to shuffle . : param axis : The axis along which to shuffle . Default is 0. : type axis : int : return : A callable function that applies the shuffle operation on a given tensor . : rtype : Callable \"\"\" return K.vec(K.shuffle, axis=axis) Classes Noise class Noise ( last_index : Optional [ int ] = None , sample_from : str = 'sample' , prob : float = 1.0 ) This is the class from which all layers inherit. A layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves computation , defined in the call() method, and a state (weight variables). State can be created in various places, at the convenience of the subclass implementer: in __init__() ; in the optional build() method, which is invoked by the first __call__() to the layer, and supplies the shape(s) of the input(s), which may not have been known at initialization time; in the first invocation of call() , with some caveats discussed below. Layers are recursively composable: If you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer. Nested layers should be instantiated in the __init__() method. Users will just instantiate a layer and then treat it as a callable. Attributes Name Type Description Default trainable None Boolean, whether the layer's variables should be trainable. None name None String name of the layer. None dtype None The dtype of the layer's computations and weights. Can also be a tf.keras.mixed_precision.Policy , which allows the computation and weight dtype to differ. Default of None means to use tf.keras.mixed_precision.global_policy() , which is a float32 policy unless set to different value. None dynamic None Set this to True if your layer should only be run eagerly, and should not be used to generate a static computation graph. This would be the case for a Tree-RNN or a recursive network, for example, or generally for any layer that manipulates tensors using Python control flow. If False , we assume that the layer can safely be used to generate a static computation graph. None name None The name of the layer (string). None dtype None The dtype of the layer's weights. None variable_dtype None Alias of dtype . None compute_dtype None The dtype of the layer's computations. Layers automatically cast inputs to this dtype which causes the computations and output to also be in this dtype. When mixed precision is used with a tf.keras.mixed_precision.Policy , this will be different than variable_dtype . None dtype_policy None The layer's dtype policy. See the tf.keras.mixed_precision.Policy documentation for details. None trainable_weights None List of variables to be included in backprop. None non_trainable_weights None List of variables that should not be included in backprop. None weights None The concatenation of the lists trainable_weights and non_trainable_weights (in this order). None trainable None Whether the layer should be trained (boolean), i.e. whether its potentially-trainable weights should be returned as part of layer.trainable_weights . None input_spec None Optional (list of) InputSpec object(s) specifying the constraints on inputs that can be accepted by the layer. None View Source class Noise ( Layer ) : def __init__ ( self , last_index : Optional [ int ] = None , sample_from : str = \"sample\" , prob : float = 1.0 ) : \"\"\" Initialize the Noise layer :param last_index: the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type last_index: int :param sample_from: a string that specifies the noise source. \" sample \" (default) will sample from the input sequence while \" batch \" will sample from the batch. :type sample_from: str :param prob: a float value between 0 and 1 that represents the probability of applying noise to the input. :type prob: float \"\"\" super (). __init__ () self . last_index = last_index self . sample_from = sample_from self . prob = prob def build ( self , input_shape : Tuple [ int ] ) -> None : \"\"\" Constructs the noise layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" if not self . last_index : self . last_index = input_shape [ 1 ] def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) switch_indexes = tf . range ( shape [ 0 ] ) if self . prob < 1.0 : switch_indexes = switch_indexes [ K.categorical_like(inputs, prob=self.prob, dtype=\"bool\")[0 ] ] switch_no = tf . shape ( switch_indexes ) [ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ( [ switch_indexes, K.init.label(max=self.last_index, shape=switch_no[None ] ), ] ), perm = ( 1 , 0 )) updates = ( [ K.init.label(max=switch_no, shape=switch_no[None ] ) if self . sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = self . last_index , shape = switch_no [ None ] ), ] ) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape : Tuple [ int ] ) -> None Constructs the noise layer, it will take input_shape as input and it will not return any output. Parameters: Name Type Description Default input_shape tuple Tensor shape of the input. None View Source def build ( self , input_shape : Tuple [ int ] ) -> None : \"\"\" Constructs the noise layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" if not self . last_index : self . last_index = input_shape [ 1 ] call def call ( self , inputs : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor This function will take inputs tensor as input and it will return the noise tensor. Parameters: Name Type Description Default inputs tf.Tensor Input tensor to be manipulated. None Returns: Type Description tf.Tensor Tensor with noise added. View Source def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) switch_indexes = tf . range ( shape [ 0 ] ) if self . prob < 1.0 : switch_indexes = switch_indexes [ K.categorical_like(inputs, prob=self.prob, dtype=\"bool\")[0 ] ] switch_no = tf . shape ( switch_indexes ) [ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ( [ switch_indexes, K.init.label(max=self.last_index, shape=switch_no[None ] ), ] ), perm = ( 1 , 0 )) updates = ( [ K.init.label(max=switch_no, shape=switch_no[None ] ) if self . sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = self . last_index , shape = switch_no [ None ] ), ] ) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weights def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state () PartialModel class PartialModel ( model : keras . engine . training . Model , last_axis : int = 8 ) This class returns a PartialModel layer, which can be used to apply a model on a subset of the input tensor. View Source class PartialModel ( Layer ): \"\"\" This class returns a PartialModel layer, which can be used to apply a model on a subset of the input tensor. \"\"\" def __init__ ( self , model: Model , last_axis: int = 8 ): \"\"\" Initialize the PartialModel layer :param model: The model to be applied to the subset of the input tensor :type model: Model :param last_axis: The last axis on which the model should be applied. Defaults to 8. :type last_axis: int \"\"\" super (). __init__ () self . model = model self . last_axis = last_axis def call ( self , inputs: tf . Tensor ) -> tf . Tensor: \"\"\" This function will take inputs tensor as input and it will return the combined output of model and the original tensor. :param inputs: Input tensor to be processed. :type inputs: tf.Tensor :return: combined output tensor of model and the original tensor. :rtype: tf.Tensor \"\"\" x = self . model ( inputs [:, : self . last_axis ]) return tf . concat ([ x , inputs [:, self . last_axis: ]], axis = 1 ) Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of call() . This is typically used to create the weights of Layer subclasses (at the discretion of the subclass implementer). Parameters: Name Type Description Default input_shape None Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). None View Source @tf.__internal__.tracking.no_automatic_dependency_tracking @generic_utils.default def build ( self , input_shape ) : \" \"\" Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of `call()`. This is typically used to create the weights of `Layer` subclasses (at the discretion of the subclass implementer). Args: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). \"\" \" self . _build_input_shape = input_shape self . built = True call def call ( self , inputs : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor This function will take inputs tensor as input and it will return the combined output of model and the original tensor. Parameters: Name Type Description Default inputs tf.Tensor Input tensor to be processed. None Returns: Type Description tf.Tensor combined output tensor of model and the original tensor. View Source def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the combined output of model and the original tensor. :param inputs: Input tensor to be processed. :type inputs: tf.Tensor :return: combined output tensor of model and the original tensor. :rtype: tf.Tensor \"\"\" x = self . model ( inputs [ : , : self . last_axis ]) return tf . concat ([ x , inputs [ : , self . last_axis : ]], axis = 1 ) compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weights def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state () Transpose class Transpose ( axis : tuple = ( 1 , 0 ) ) This class returns a transpose layer, which can be used to transpose the dimensions of a tensor. View Source class Transpose ( Layer ): \"\"\" This class returns a transpose layer, which can be used to transpose the dimensions of a tensor. \"\"\" def __init__ ( self , axis: tuple = ( 1 , 0 )): \"\"\" Initialize the Transpose layer :param axis: The axis along which to transpose the tensor. Defaults to (1, 0). :type axis: tuple \"\"\" super (). __init__ () self . axis = axis def build ( self , input_shape: tuple ) -> None: \"\"\" Constructs the transpose layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" dim = len ( input_shape ) axis_len = len ( self . axis ) if axis_len < dim: self . axis = tuple ( self . axis ) + tuple ( range ( dim ))[ axis_len: ] def call ( self , inputs: tf . Tensor ) -> tf . Tensor: \"\"\" This function will take inputs tensor as input and it will return the transposed tensor. :param inputs: Input tensor to be transposed. :type inputs: tf.Tensor :return: Transposed tensor. :rtype: tf.Tensor \"\"\" return tf . transpose ( inputs , self . axis ) Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape : tuple ) -> None Constructs the transpose layer, it will take input_shape as input and it will not return any output. Parameters: Name Type Description Default input_shape tuple Tensor shape of the input. None View Source def build ( self , input_shape : tuple ) -> None : \"\"\" Constructs the transpose layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" dim = len ( input_shape ) axis_len = len ( self . axis ) if axis_len < dim : self . axis = tuple ( self . axis ) + tuple ( range ( dim ))[ axis_len : ] call def call ( self , inputs : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor This function will take inputs tensor as input and it will return the transposed tensor. Parameters: Name Type Description Default inputs tf.Tensor Input tensor to be transposed. None Returns: Type Description tf.Tensor Transposed tensor. View Source def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the transposed tensor. :param inputs: Input tensor to be transposed. :type inputs: tf.Tensor :return: Transposed tensor. :rtype: tf.Tensor \"\"\" return tf . transpose ( inputs , self . axis ) compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weights def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"Augmentation"},{"location":"reference/grid_transformer/augmentation/#module-grid_transformeraugmentation","text":"This module contains a collection of functions and classes for working with tensors in TensorFlow. Functions: `shuffle_fn : A wrapper function for shuffle layer that allows for customization of the axis along which to shuffle. shuffle : A callable function that applies the shuffle operation on a given tensor with default axis 0. shuffle_col : A callable function that applies the shuffle operation on a given tensor with axis 1. transpose : A partial function that returns a transpose layer, which can be used to transpose the dimensions of a tensor. reshape_static : A function to reshape a tensor in a specific shape and apply a given function to it. Classes: Transpose(Layer) : Returns a transpose layer, which can be used to transpose the dimensions of a tensor. PartialModel(Layer) : Returns a PartialModel layer that allows for applying a function to a subset of the input tensor. Example usage: import tensorflow as tf from models_utils import reshape_static , transpose # Create a tensor of shape (2,3) x = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) # Reshape the tensor to shape (2, 2, 3) and apply transpose function y = reshape_static ([ 2 , 2 , 3 ], transpose )( x ) print ( y ) # Output: # [[[1 3] # [2 4]] # [[5 6] # [3 2]]] import numpy as np from models_utils import PartialModel # Create a tensor of shape (2, 2, 3) x = np . random . rand ( 2 , 2 , 3 ) # Create a partial model layer with indices [0, 1] and function sum pm = PartialModel ( fn = tf . reduce_sum , indices = [ 0 , 1 ]) # Apply the partial model layer on input tensor y = pm ( x ) print ( y ) # Output: # [[1.7268854 1.80438087 1.35692811] # [1.74081294 1.70722026 1.45006824]] In this example, the function tf.reduce_sum is applied to the first two indices of the input tensor x and the resulting tensor is returned. View Source \"\"\" This module contains a collection of functions and classes for working with tensors in TensorFlow. Functions: - `shuffle_fn : A wrapper function for shuffle layer that allows for customization of the axis along which to shuffle. - `shuffle`: A callable function that applies the shuffle operation on a given tensor with default axis 0. - `shuffle_col`: A callable function that applies the shuffle operation on a given tensor with axis 1. - `transpose`: A partial function that returns a transpose layer, which can be used to transpose the dimensions of a tensor. - `reshape_static`: A function to reshape a tensor in a specific shape and apply a given function to it. Classes: - `Transpose(Layer)`: Returns a transpose layer, which can be used to transpose the dimensions of a tensor. - `PartialModel(Layer)`: Returns a PartialModel layer that allows for applying a function to a subset of the input tensor. Example usage: import tensorflow as tf from models_utils import reshape_static, transpose # Create a tensor of shape (2,3) x = tf.constant([[1,2,3], [4,5,6]]) # Reshape the tensor to shape (2, 2, 3) and apply transpose function y = reshape_static([2, 2, 3], transpose)(x) print(y) # Output: # [[[1 3] # [2 4]] # [[5 6] # [3 2]]] import numpy as np from models_utils import PartialModel # Create a tensor of shape (2, 2, 3) x = np.random.rand(2, 2, 3) # Create a partial model layer with indices [0, 1] and function sum pm = PartialModel(fn=tf.reduce_sum, indices=[0, 1]) # Apply the partial model layer on input tensor y = pm(x) print(y) # Output: # [[1.7268854 1.80438087 1.35692811] # [1.74081294 1.70722026 1.45006824]] In this example, the function `tf.reduce_sum` is applied to the first two indices of the input tensor `x` and the resulting tensor is returned. \"\"\" from functools import partial import tensorflow as tf from tensorflow.keras import Model from tensorflow.keras.layers import Layer , Lambda from ml_utils import il , dict_from_list , lw import models_utils.ops.list_ import models_utils.ops.rand from models_utils import ops as K , InferencePass , random_update import math from typing import List , Union , Callable , Optional , Tuple def shuffle_fn ( axis : int = 0 ) -> Callable : \"\"\" A wrapper function for Keras's shuffle layer that allows for customization of the axis along which to shuffle. :param axis: The axis along which to shuffle. Default is 0. :type axis: int :return: A callable function that applies the shuffle operation on a given tensor. :rtype: Callable \"\"\" return K . vec ( K . shuffle , axis = axis ) shuffle = shuffle_fn () shuffle_col = shuffle_fn ( axis = 1 ) transpose = partial ( tf . transpose , perm = ( 0 , 2 , 1 , 3 , 4 )) def reshape_static ( shape : Union [ List [ int ], tf . TensorShape ], model : Callable = shuffle , row : Union [ None , int ] = None ) -> Callable : \"\"\" A function to reshape a tensor in a specific shape and apply a given function to it. :param shape: The shape of the reshaped tensor. It can be a list of integers or a tf.TensorShape object. :type shape: Union[List[int], tf.TensorShape] :param model: A callable function to apply on the reshaped tensor. Default is the shuffle function. :type model: Callable :param row: If provided, number of row in reshaped tensor, use to calculate the reshape shape. :type row: Union[None, int] :return: A callable function that reshape the tensor and apply the given function. :rtype: Callable \"\"\" if not il ( shape ): shape = tuple ( shape . shape ) if row is None : row = math . floor ( math . sqrt ( shape [ 1 ])) newshape = [ - 1 ] + [ row , row ] + list ( shape [ 2 :]) shape = ( - 1 ,) + shape [ 1 :] def fn ( x ): x = tf . reshape ( x , newshape ) x = model ( x ) x = tf . reshape ( x , shape ) return x return fn class Transpose ( Layer ): \"\"\" This class returns a transpose layer, which can be used to transpose the dimensions of a tensor. \"\"\" def __init__ ( self , axis : tuple = ( 1 , 0 )): \"\"\" Initialize the Transpose layer :param axis: The axis along which to transpose the tensor. Defaults to (1, 0). :type axis: tuple \"\"\" super () . __init__ () self . axis = axis def build ( self , input_shape : tuple ) -> None : \"\"\" Constructs the transpose layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" dim = len ( input_shape ) axis_len = len ( self . axis ) if axis_len < dim : self . axis = tuple ( self . axis ) + tuple ( range ( dim ))[ axis_len :] def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the transposed tensor. :param inputs: Input tensor to be transposed. :type inputs: tf.Tensor :return: Transposed tensor. :rtype: tf.Tensor \"\"\" return tf . transpose ( inputs , self . axis ) class PartialModel ( Layer ): \"\"\" This class returns a PartialModel layer, which can be used to apply a model on a subset of the input tensor. \"\"\" def __init__ ( self , model : Model , last_axis : int = 8 ): \"\"\" Initialize the PartialModel layer :param model: The model to be applied to the subset of the input tensor :type model: Model :param last_axis: The last axis on which the model should be applied. Defaults to 8. :type last_axis: int \"\"\" super () . __init__ () self . model = model self . last_axis = last_axis def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the combined output of model and the original tensor. :param inputs: Input tensor to be processed. :type inputs: tf.Tensor :return: combined output tensor of model and the original tensor. :rtype: tf.Tensor \"\"\" x = self . model ( inputs [:, : self . last_axis ]) return tf . concat ([ x , inputs [:, self . last_axis :]], axis = 1 ) def noise_seq ( inputs : tf . Tensor , row : Optional [ int ] = None , col : Optional [ int ] = None , sample_from : str = \"sample\" ) -> tf . Tensor : \"\"\" This function adds noise to a sequence by updating some of its elements. :param inputs: the input sequence to be manipulated. :type inputs: tf.Tensor :param row: the number of rows of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type row: int :param col: the number of columns of the input sequence. If not provided, it defaults to the third dimension of the input tensor shape. :type col: int :param sample_from: a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. :type sample_from: str :return: a tensor with the same shape as the input but with some of its elements updated. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) row = row if row else shape [ 1 ] col = col if col else shape [ 2 ] batch = shape [ 0 ] indexes = K . tensor ([ tf . range ( batch ), K . init . label ( max = row , shape = batch [ None ]), K . init . label ( max = col , shape = batch [ None ]) ]) . T updates = ([ K . init . label ( max = batch , shape = batch [ None ]) if sample_from == \"batch\" else tf . range ( batch ), K . init . label ( max = row , shape = batch [ None ]), K . init . label ( max = col , shape = batch [ None ]) ]) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) class Noise ( Layer ): def __init__ ( self , last_index : Optional [ int ] = None , sample_from : str = \"sample\" , prob : float = 1.0 ): \"\"\" Initialize the Noise layer :param last_index: the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type last_index: int :param sample_from: a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. :type sample_from: str :param prob: a float value between 0 and 1 that represents the probability of applying noise to the input. :type prob: float \"\"\" super () . __init__ () self . last_index = last_index self . sample_from = sample_from self . prob = prob def build ( self , input_shape : Tuple [ int ]) -> None : \"\"\" Constructs the noise layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" if not self . last_index : self . last_index = input_shape [ 1 ] def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) switch_indexes = tf . range ( shape [ 0 ]) if self . prob < 1.0 : switch_indexes = switch_indexes [ K . categorical_like ( inputs , prob = self . prob , dtype = \"bool\" )[ 0 ]] switch_no = tf . shape ( switch_indexes )[ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ([ switch_indexes , K . init . label ( max = self . last_index , shape = switch_no [ None ]), ]), perm = ( 1 , 0 )) updates = ([ K . init . label ( max = switch_no , shape = switch_no [ None ]) if self . sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = self . last_index , shape = switch_no [ None ]), ]) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) BatchNoise = partial ( Noise , sample_from = \"batch\" ) def noise ( inputs : tf . Tensor , last_index : Optional [ int ] = None , sample_from : str = \"sample\" , prob : float = 1.0 ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :param last_index: the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type last_index: int :param sample_from: a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. :type sample_from: str :param prob: a float value between 0 and 1 that represents the probability of applying noise to the input. :type prob: float :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) last_index = last_index if last_index else shape [ 1 ] switch_indexes = tf . range ( shape [ 0 ]) if prob < 1.0 : switch_indexes = switch_indexes [ K . categorical_like ( inputs , prob = prob , dtype = \"bool\" )[ 0 ]] switch_no = tf . shape ( switch_indexes )[ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ([ switch_indexes , K . init . label ( max = last_index , shape = switch_no [ None ]), ]), perm = ( 1 , 0 )) updates = ([ K . init . label ( max = switch_no , shape = switch_no [ None ]) if sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = last_index , shape = switch_no [ None ]), ]) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] ) def rand ( augmentation : Union [ float , int , List [ Union [ float , Tuple [ float , callable ]]], Tuple [ float , callable ]], root : int = 2 , transpose : bool = True ) -> Callable : \"\"\" This function will take augmentation,root and transpose as input and it will return a callable function :param augmentation: a list of floats or tuples of floats and callable functions that represents the probability of applying noise to the input. :type augmentation: Union[float, int, List[Union[float, Tuple[float, callable]]], Tuple[float, callable]] :param root: an integer that represents the root of the number of the augmentation functions. :type root: int :param transpose: a boolean value that represents whether the input should be transposed or not. :type transpose: bool :return: callable function :rtype: callable \"\"\" augmentation_fn = [ ( lambda x = m : K . list_ . DivideDim ( x , root = root )) for m in ( [ shuffle_fn ( i ) for i in range ( root )] + ([ Transpose (( 0 , 2 , 1 ))] if transpose else []) ) ] if isinstance ( augmentation , ( float , int )): augmentation = [ augmentation ] * len ( augmentation_fn ) aug = [] for i , a in enumerate ( lw ( augmentation )): if len ( lw ( a )) == 2 : aug . append ( a ) else : aug . append (( a , augmentation_fn [ i ]())) def apply ( * args ): indexes = Lambda ( K . range_like )( args [ 0 ]) # aug_indexes = K.range_like(inputs) for k , v in aug : if k > 0 : indexes = random_update ( indexes , model = v , prob = k ) outputs = [ K . gather ( a , indexes ) for a in args ] return tuple ( outputs ) return apply def rand_aug4 ( * args , prob : Optional [ float ] = None , vec : Optional [ bool ] = True ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: boolean, whether to use vectorized version of layers or not :return: list of augmented inputs \"\"\" layers = [ partial ( K . shuffle , axis = 0 ), K . shuffle , partial ( tf . transpose , perm = ( 1 , 0 ))] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec ) def rand_aug2 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = K . list_ . run ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: function to use for vectorizing the input list, defaults to `K.list_.run` :return: list of augmented inputs \"\"\" layers = [ partial ( K . list_ . shuffle , axis = 0 ), K . list_ . shuffle , partial ( K . list_ . transpose )] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec ) def rand_aug3 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = K . list_ . run ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: function to use for vectorizing the input list, defaults to `K.list_.run` :return: list of augmented inputs \"\"\" layers = [ DivideShape ( f , axis = 0 , replace_first = False ) for f in [ partial ( K . list_ . shuffle , axis = 0 ), K . list_ . shuffle , partial ( K . list_ . transpose )]] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec ) batch_noise_seq = partial ( noise_seq , sample_from = 'batch' ) batch_noise = partial ( noise , sample_from = 'batch' )","title":"Module grid_transformer.augmentation"},{"location":"reference/grid_transformer/augmentation/#variables","text":"BatchNoise batch_noise batch_noise_seq transpose","title":"Variables"},{"location":"reference/grid_transformer/augmentation/#functions","text":"","title":"Functions"},{"location":"reference/grid_transformer/augmentation/#noise","text":"def noise ( inputs : tensorflow . python . framework . ops . Tensor , last_index : Optional [ int ] = None , sample_from : str = 'sample' , prob : float = 1.0 ) -> tensorflow . python . framework . ops . Tensor This function will take inputs tensor as input and it will return the noise tensor. Parameters: Name Type Description Default inputs tf.Tensor Input tensor to be manipulated. None last_index int the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. the second dimension of the input tensor shape sample_from str a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. None prob float a float value between 0 and 1 that represents the probability of applying noise to the input. None Returns: Type Description tf.Tensor Tensor with noise added. View Source def noise ( inputs : tf . Tensor , last_index : Optional [ int ] = None , sample_from : str = \"sample\" , prob : float = 1.0 ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :param last_index: the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type last_index: int :param sample_from: a string that specifies the noise source. \" sample \" (default) will sample from the input sequence while \" batch \" will sample from the batch. :type sample_from: str :param prob: a float value between 0 and 1 that represents the probability of applying noise to the input. :type prob: float :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) last_index = last_index if last_index else shape [ 1 ] switch_indexes = tf . range ( shape [ 0 ] ) if prob < 1.0 : switch_indexes = switch_indexes [ K.categorical_like(inputs, prob=prob, dtype=\"bool\")[0 ] ] switch_no = tf . shape ( switch_indexes ) [ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ( [ switch_indexes, K.init.label(max=last_index, shape=switch_no[None ] ), ] ), perm = ( 1 , 0 )) updates = ( [ K.init.label(max=switch_no, shape=switch_no[None ] ) if sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = last_index , shape = switch_no [ None ] ), ] ) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] )","title":"noise"},{"location":"reference/grid_transformer/augmentation/#noise_seq","text":"def noise_seq ( inputs : tensorflow . python . framework . ops . Tensor , row : Optional [ int ] = None , col : Optional [ int ] = None , sample_from : str = 'sample' ) -> tensorflow . python . framework . ops . Tensor This function adds noise to a sequence by updating some of its elements. Parameters: Name Type Description Default inputs tf.Tensor the input sequence to be manipulated. None row int the number of rows of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. the second dimension of the input tensor shape col int the number of columns of the input sequence. If not provided, it defaults to the third dimension of the input tensor shape. the third dimension of the input tensor shape sample_from str a string that specifies the noise source. \"sample\" (default) will sample from the input sequence while \"batch\" will sample from the batch. None Returns: Type Description tf.Tensor a tensor with the same shape as the input but with some of its elements updated. View Source def noise_seq ( inputs : tf . Tensor , row : Optional [ int ] = None , col : Optional [ int ] = None , sample_from : str = \"sample\" ) -> tf . Tensor : \"\"\" This function adds noise to a sequence by updating some of its elements. :param inputs: the input sequence to be manipulated. :type inputs: tf.Tensor :param row: the number of rows of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type row: int :param col: the number of columns of the input sequence. If not provided, it defaults to the third dimension of the input tensor shape. :type col: int :param sample_from: a string that specifies the noise source. \" sample \" (default) will sample from the input sequence while \" batch \" will sample from the batch. :type sample_from: str :return: a tensor with the same shape as the input but with some of its elements updated. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) row = row if row else shape [ 1 ] col = col if col else shape [ 2 ] batch = shape [ 0 ] indexes = K . tensor ( [ tf.range(batch), K.init.label(max=row, shape=batch[None ] ), K . init . label ( max = col , shape = batch [ None ] ) ] ). T updates = ( [ K.init.label(max=batch, shape=batch[None ] ) if sample_from == \"batch\" else tf . range ( batch ), K . init . label ( max = row , shape = batch [ None ] ), K . init . label ( max = col , shape = batch [ None ] ) ] ) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] )","title":"noise_seq"},{"location":"reference/grid_transformer/augmentation/#rand","text":"def rand ( augmentation : Union [ float , int , List [ Union [ float , Tuple [ float , < built - in function callable > ]]], Tuple [ float , < built - in function callable > ]], root : int = 2 , transpose : bool = True ) -> Callable This function will take augmentation,root and transpose as input and it will return a callable function Parameters: Name Type Description Default augmentation Union[float, int, List[Union[float, Tuple[float, callable]]], Tuple[float, callable]] a list of floats or tuples of floats and callable functions that represents the probability of applying noise to the input. None root int an integer that represents the root of the number of the augmentation functions. None transpose bool a boolean value that represents whether the input should be transposed or not. None Returns: Type Description callable callable function View Source def rand ( augmentation : Union [ float, int, List[Union[float, Tuple[float, callable ] ]] , Tuple [ float, callable ] ] , root : int = 2 , transpose : bool = True ) -> Callable : \"\"\" This function will take augmentation,root and transpose as input and it will return a callable function :param augmentation: a list of floats or tuples of floats and callable functions that represents the probability of applying noise to the input. :type augmentation: Union[float, int, List[Union[float, Tuple[float, callable]]], Tuple[float, callable]] :param root: an integer that represents the root of the number of the augmentation functions. :type root: int :param transpose: a boolean value that represents whether the input should be transposed or not. :type transpose: bool :return: callable function :rtype: callable \"\"\" augmentation_fn = [ (lambda x=m: K.list_.DivideDim(x, root=root)) for m in ( [shuffle_fn(i) for i in range(root) ] + ( [ Transpose((0, 2, 1)) ] if transpose else [] ) ) ] if isinstance ( augmentation , ( float , int )) : augmentation = [ augmentation ] * len ( augmentation_fn ) aug = [] for i , a in enumerate ( lw ( augmentation )) : if len ( lw ( a )) == 2 : aug . append ( a ) else : aug . append (( a , augmentation_fn [ i ] ())) def apply ( * args ) : indexes = Lambda ( K . range_like )( args [ 0 ] ) # aug_indexes = K . range_like ( inputs ) for k , v in aug : if k > 0 : indexes = random_update ( indexes , model = v , prob = k ) outputs = [ K.gather(a, indexes) for a in args ] return tuple ( outputs ) return apply","title":"rand"},{"location":"reference/grid_transformer/augmentation/#rand_aug2","text":"def rand_aug2 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = < function run at 0x7ffa73f46d40 > ) -> List Apply random augmentation to a list of inputs with the specified probability. Parameters: Name Type Description Default args None list of inputs to be augmented None prob None probability of applying augmentation to each input. If None, it will be set to 1/len(args) None vec None function to use for vectorizing the input list, defaults to K.list_.run K.list_.run Returns: Type Description None list of augmented inputs View Source def rand_aug2 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = K . list_ . run ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: function to use for vectorizing the input list, defaults to `K.list_.run` :return: list of augmented inputs \"\"\" layers = [ partial(K.list_.shuffle, axis=0), K.list_.shuffle, partial(K.list_.transpose) ] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec )","title":"rand_aug2"},{"location":"reference/grid_transformer/augmentation/#rand_aug3","text":"def rand_aug3 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = < function run at 0x7ffa73f46d40 > ) -> List Apply random augmentation to a list of inputs with the specified probability. Parameters: Name Type Description Default args None list of inputs to be augmented None prob None probability of applying augmentation to each input. If None, it will be set to 1/len(args) None vec None function to use for vectorizing the input list, defaults to K.list_.run K.list_.run Returns: Type Description None list of augmented inputs View Source def rand_aug3 ( * args , prob : Optional [ float ] = None , vec : Optional [ Callable ] = K . list_ . run ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: function to use for vectorizing the input list, defaults to `K.list_.run` :return: list of augmented inputs \"\"\" layers = [ DivideShape(f, axis=0, replace_first=False) for f in [partial(K.list_.shuffle, axis=0), K.list_.shuffle, partial(K.list_.transpose) ] ] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec )","title":"rand_aug3"},{"location":"reference/grid_transformer/augmentation/#rand_aug4","text":"def rand_aug4 ( * args , prob : Optional [ float ] = None , vec : Optional [ bool ] = True ) -> List Apply random augmentation to a list of inputs with the specified probability. Parameters: Name Type Description Default args None list of inputs to be augmented None prob None probability of applying augmentation to each input. If None, it will be set to 1/len(args) None vec None boolean, whether to use vectorized version of layers or not None Returns: Type Description None list of augmented inputs View Source def rand_aug4 ( * args , prob : Optional [ float ] = None , vec : Optional [ bool ] = True ) -> List : \"\"\" Apply random augmentation to a list of inputs with the specified probability. :param args: list of inputs to be augmented :param prob: probability of applying augmentation to each input. If None, it will be set to 1/len(args) :param vec: boolean, whether to use vectorized version of layers or not :return: list of augmented inputs \"\"\" layers = [ partial(K.shuffle, axis=0), K.shuffle, partial(tf.transpose, perm=(1, 0)) ] return K . rand . switch ( * args , layers = layers , prob = prob , vec = vec )","title":"rand_aug4"},{"location":"reference/grid_transformer/augmentation/#reshape_static","text":"def reshape_static ( shape : Union [ List [ int ], tensorflow . python . framework . tensor_shape . TensorShape ], model : Callable = < function vec .< locals >. wrapper at 0x7ff9a690aef0 > , row : Optional [ int ] = None ) -> Callable A function to reshape a tensor in a specific shape and apply a given function to it. Parameters: Name Type Description Default shape Union[List[int], tf.TensorShape] The shape of the reshaped tensor. It can be a list of integers or a tf.TensorShape object. None model Callable A callable function to apply on the reshaped tensor. Default is the shuffle function. None row Union[None, int] If provided, number of row in reshaped tensor, use to calculate the reshape shape. None Returns: Type Description Callable A callable function that reshape the tensor and apply the given function. View Source def reshape_static ( shape : Union [ List[int ] , tf . TensorShape ] , model : Callable = shuffle , row : Union [ None, int ] = None ) -> Callable : \"\"\" A function to reshape a tensor in a specific shape and apply a given function to it. :param shape: The shape of the reshaped tensor. It can be a list of integers or a tf.TensorShape object. :type shape: Union[List[int], tf.TensorShape] :param model: A callable function to apply on the reshaped tensor. Default is the shuffle function. :type model: Callable :param row: If provided, number of row in reshaped tensor, use to calculate the reshape shape. :type row: Union[None, int] :return: A callable function that reshape the tensor and apply the given function. :rtype: Callable \"\"\" if not il ( shape ) : shape = tuple ( shape . shape ) if row is None : row = math . floor ( math . sqrt ( shape [ 1 ] )) newshape = [ -1 ] + [ row, row ] + list ( shape [ 2: ] ) shape = ( - 1 ,) + shape [ 1: ] def fn ( x ) : x = tf . reshape ( x , newshape ) x = model ( x ) x = tf . reshape ( x , shape ) return x return fn","title":"reshape_static"},{"location":"reference/grid_transformer/augmentation/#shuffle","text":"def shuffle ( inputs ) View Source def wrapper ( inputs ) : # tf . print ( f \"Run: {fn.keywords}\" ) return tf . vectorized_map ( fn , inputs )","title":"shuffle"},{"location":"reference/grid_transformer/augmentation/#shuffle_col","text":"def shuffle_col ( inputs ) View Source def wrapper ( inputs ) : # tf . print ( f \"Run: {fn.keywords}\" ) return tf . vectorized_map ( fn , inputs )","title":"shuffle_col"},{"location":"reference/grid_transformer/augmentation/#shuffle_fn","text":"def shuffle_fn ( axis : int = 0 ) -> Callable A wrapper function for Keras's shuffle layer that allows for customization of the axis along which to shuffle. Parameters: Name Type Description Default axis int The axis along which to shuffle. Default is 0. None Returns: Type Description Callable A callable function that applies the shuffle operation on a given tensor. View Source def shuffle_fn ( axis : int = 0 ) -> Callable : \"\"\" A wrapper function for Keras' s shuffle layer that allows for customization of the axis along which to shuffle . : param axis : The axis along which to shuffle . Default is 0. : type axis : int : return : A callable function that applies the shuffle operation on a given tensor . : rtype : Callable \"\"\" return K.vec(K.shuffle, axis=axis)","title":"shuffle_fn"},{"location":"reference/grid_transformer/augmentation/#classes","text":"","title":"Classes"},{"location":"reference/grid_transformer/augmentation/#noise_1","text":"class Noise ( last_index : Optional [ int ] = None , sample_from : str = 'sample' , prob : float = 1.0 ) This is the class from which all layers inherit. A layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves computation , defined in the call() method, and a state (weight variables). State can be created in various places, at the convenience of the subclass implementer: in __init__() ; in the optional build() method, which is invoked by the first __call__() to the layer, and supplies the shape(s) of the input(s), which may not have been known at initialization time; in the first invocation of call() , with some caveats discussed below. Layers are recursively composable: If you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer. Nested layers should be instantiated in the __init__() method. Users will just instantiate a layer and then treat it as a callable.","title":"Noise"},{"location":"reference/grid_transformer/augmentation/#attributes","text":"Name Type Description Default trainable None Boolean, whether the layer's variables should be trainable. None name None String name of the layer. None dtype None The dtype of the layer's computations and weights. Can also be a tf.keras.mixed_precision.Policy , which allows the computation and weight dtype to differ. Default of None means to use tf.keras.mixed_precision.global_policy() , which is a float32 policy unless set to different value. None dynamic None Set this to True if your layer should only be run eagerly, and should not be used to generate a static computation graph. This would be the case for a Tree-RNN or a recursive network, for example, or generally for any layer that manipulates tensors using Python control flow. If False , we assume that the layer can safely be used to generate a static computation graph. None name None The name of the layer (string). None dtype None The dtype of the layer's weights. None variable_dtype None Alias of dtype . None compute_dtype None The dtype of the layer's computations. Layers automatically cast inputs to this dtype which causes the computations and output to also be in this dtype. When mixed precision is used with a tf.keras.mixed_precision.Policy , this will be different than variable_dtype . None dtype_policy None The layer's dtype policy. See the tf.keras.mixed_precision.Policy documentation for details. None trainable_weights None List of variables to be included in backprop. None non_trainable_weights None List of variables that should not be included in backprop. None weights None The concatenation of the lists trainable_weights and non_trainable_weights (in this order). None trainable None Whether the layer should be trained (boolean), i.e. whether its potentially-trainable weights should be returned as part of layer.trainable_weights . None input_spec None Optional (list of) InputSpec object(s) specifying the constraints on inputs that can be accepted by the layer. None View Source class Noise ( Layer ) : def __init__ ( self , last_index : Optional [ int ] = None , sample_from : str = \"sample\" , prob : float = 1.0 ) : \"\"\" Initialize the Noise layer :param last_index: the last index of the input sequence. If not provided, it defaults to the second dimension of the input tensor shape. :type last_index: int :param sample_from: a string that specifies the noise source. \" sample \" (default) will sample from the input sequence while \" batch \" will sample from the batch. :type sample_from: str :param prob: a float value between 0 and 1 that represents the probability of applying noise to the input. :type prob: float \"\"\" super (). __init__ () self . last_index = last_index self . sample_from = sample_from self . prob = prob def build ( self , input_shape : Tuple [ int ] ) -> None : \"\"\" Constructs the noise layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" if not self . last_index : self . last_index = input_shape [ 1 ] def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) switch_indexes = tf . range ( shape [ 0 ] ) if self . prob < 1.0 : switch_indexes = switch_indexes [ K.categorical_like(inputs, prob=self.prob, dtype=\"bool\")[0 ] ] switch_no = tf . shape ( switch_indexes ) [ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ( [ switch_indexes, K.init.label(max=self.last_index, shape=switch_no[None ] ), ] ), perm = ( 1 , 0 )) updates = ( [ K.init.label(max=switch_no, shape=switch_no[None ] ) if self . sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = self . last_index , shape = switch_no [ None ] ), ] ) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] )","title":"Attributes"},{"location":"reference/grid_transformer/augmentation/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/augmentation/#static-methods","text":"","title":"Static methods"},{"location":"reference/grid_transformer/augmentation/#from_config","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/grid_transformer/augmentation/#with_name_scope","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/augmentation/#instance-variables","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights.","title":"Instance variables"},{"location":"reference/grid_transformer/augmentation/#methods","text":"","title":"Methods"},{"location":"reference/grid_transformer/augmentation/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/augmentation/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/augmentation/#add_update","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/augmentation/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/augmentation/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/augmentation/#build","text":"def build ( self , input_shape : Tuple [ int ] ) -> None Constructs the noise layer, it will take input_shape as input and it will not return any output. Parameters: Name Type Description Default input_shape tuple Tensor shape of the input. None View Source def build ( self , input_shape : Tuple [ int ] ) -> None : \"\"\" Constructs the noise layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" if not self . last_index : self . last_index = input_shape [ 1 ]","title":"build"},{"location":"reference/grid_transformer/augmentation/#call","text":"def call ( self , inputs : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor This function will take inputs tensor as input and it will return the noise tensor. Parameters: Name Type Description Default inputs tf.Tensor Input tensor to be manipulated. None Returns: Type Description tf.Tensor Tensor with noise added. View Source def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the noise tensor. :param inputs: Input tensor to be manipulated. :type inputs: tf.Tensor :return: Tensor with noise added. :rtype: tf.Tensor \"\"\" shape = tf . shape ( inputs ) switch_indexes = tf . range ( shape [ 0 ] ) if self . prob < 1.0 : switch_indexes = switch_indexes [ K.categorical_like(inputs, prob=self.prob, dtype=\"bool\")[0 ] ] switch_no = tf . shape ( switch_indexes ) [ 0 ] else : switch_no = shape [ 0 ] indexes = tf . transpose ( K . tensor ( [ switch_indexes, K.init.label(max=self.last_index, shape=switch_no[None ] ), ] ), perm = ( 1 , 0 )) updates = ( [ K.init.label(max=switch_no, shape=switch_no[None ] ) if self . sample_from == \"batch\" else tf . range ( switch_no ), K . init . label ( max = self . last_index , shape = switch_no [ None ] ), ] ) return tf . tensor_scatter_nd_update ( tensor = inputs , indices = indexes , updates = inputs [ updates ] )","title":"call"},{"location":"reference/grid_transformer/augmentation/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/augmentation/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/augmentation/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/augmentation/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/augmentation/#finalize_state","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/augmentation/#get_config","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) )","title":"get_config"},{"location":"reference/grid_transformer/augmentation/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/augmentation/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/augmentation/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/augmentation/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/augmentation/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/augmentation/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/augmentation/#get_weights","text":"def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/grid_transformer/augmentation/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/augmentation/#partialmodel","text":"class PartialModel ( model : keras . engine . training . Model , last_axis : int = 8 ) This class returns a PartialModel layer, which can be used to apply a model on a subset of the input tensor. View Source class PartialModel ( Layer ): \"\"\" This class returns a PartialModel layer, which can be used to apply a model on a subset of the input tensor. \"\"\" def __init__ ( self , model: Model , last_axis: int = 8 ): \"\"\" Initialize the PartialModel layer :param model: The model to be applied to the subset of the input tensor :type model: Model :param last_axis: The last axis on which the model should be applied. Defaults to 8. :type last_axis: int \"\"\" super (). __init__ () self . model = model self . last_axis = last_axis def call ( self , inputs: tf . Tensor ) -> tf . Tensor: \"\"\" This function will take inputs tensor as input and it will return the combined output of model and the original tensor. :param inputs: Input tensor to be processed. :type inputs: tf.Tensor :return: combined output tensor of model and the original tensor. :rtype: tf.Tensor \"\"\" x = self . model ( inputs [:, : self . last_axis ]) return tf . concat ([ x , inputs [:, self . last_axis: ]], axis = 1 )","title":"PartialModel"},{"location":"reference/grid_transformer/augmentation/#ancestors-in-mro_1","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/augmentation/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/grid_transformer/augmentation/#from_config_1","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/grid_transformer/augmentation/#with_name_scope_1","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/augmentation/#instance-variables_1","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights.","title":"Instance variables"},{"location":"reference/grid_transformer/augmentation/#methods_1","text":"","title":"Methods"},{"location":"reference/grid_transformer/augmentation/#add_loss_1","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/augmentation/#add_metric_1","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/augmentation/#add_update_1","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/augmentation/#add_variable_1","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/augmentation/#add_weight_1","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/augmentation/#build_1","text":"def build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of call() . This is typically used to create the weights of Layer subclasses (at the discretion of the subclass implementer). Parameters: Name Type Description Default input_shape None Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). None View Source @tf.__internal__.tracking.no_automatic_dependency_tracking @generic_utils.default def build ( self , input_shape ) : \" \"\" Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of `call()`. This is typically used to create the weights of `Layer` subclasses (at the discretion of the subclass implementer). Args: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). \"\" \" self . _build_input_shape = input_shape self . built = True","title":"build"},{"location":"reference/grid_transformer/augmentation/#call_1","text":"def call ( self , inputs : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor This function will take inputs tensor as input and it will return the combined output of model and the original tensor. Parameters: Name Type Description Default inputs tf.Tensor Input tensor to be processed. None Returns: Type Description tf.Tensor combined output tensor of model and the original tensor. View Source def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the combined output of model and the original tensor. :param inputs: Input tensor to be processed. :type inputs: tf.Tensor :return: combined output tensor of model and the original tensor. :rtype: tf.Tensor \"\"\" x = self . model ( inputs [ : , : self . last_axis ]) return tf . concat ([ x , inputs [ : , self . last_axis : ]], axis = 1 )","title":"call"},{"location":"reference/grid_transformer/augmentation/#compute_mask_1","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/augmentation/#compute_output_shape_1","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/augmentation/#compute_output_signature_1","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/augmentation/#count_params_1","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/augmentation/#finalize_state_1","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/augmentation/#get_config_1","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) )","title":"get_config"},{"location":"reference/grid_transformer/augmentation/#get_input_at_1","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/augmentation/#get_input_mask_at_1","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/augmentation/#get_input_shape_at_1","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/augmentation/#get_output_at_1","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/augmentation/#get_output_mask_at_1","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/augmentation/#get_output_shape_at_1","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/augmentation/#get_weights_1","text":"def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/grid_transformer/augmentation/#set_weights_1","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/augmentation/#transpose","text":"class Transpose ( axis : tuple = ( 1 , 0 ) ) This class returns a transpose layer, which can be used to transpose the dimensions of a tensor. View Source class Transpose ( Layer ): \"\"\" This class returns a transpose layer, which can be used to transpose the dimensions of a tensor. \"\"\" def __init__ ( self , axis: tuple = ( 1 , 0 )): \"\"\" Initialize the Transpose layer :param axis: The axis along which to transpose the tensor. Defaults to (1, 0). :type axis: tuple \"\"\" super (). __init__ () self . axis = axis def build ( self , input_shape: tuple ) -> None: \"\"\" Constructs the transpose layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" dim = len ( input_shape ) axis_len = len ( self . axis ) if axis_len < dim: self . axis = tuple ( self . axis ) + tuple ( range ( dim ))[ axis_len: ] def call ( self , inputs: tf . Tensor ) -> tf . Tensor: \"\"\" This function will take inputs tensor as input and it will return the transposed tensor. :param inputs: Input tensor to be transposed. :type inputs: tf.Tensor :return: Transposed tensor. :rtype: tf.Tensor \"\"\" return tf . transpose ( inputs , self . axis )","title":"Transpose"},{"location":"reference/grid_transformer/augmentation/#ancestors-in-mro_2","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/augmentation/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/grid_transformer/augmentation/#from_config_2","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/grid_transformer/augmentation/#with_name_scope_2","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/augmentation/#instance-variables_2","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights.","title":"Instance variables"},{"location":"reference/grid_transformer/augmentation/#methods_2","text":"","title":"Methods"},{"location":"reference/grid_transformer/augmentation/#add_loss_2","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/augmentation/#add_metric_2","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/augmentation/#add_update_2","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/augmentation/#add_variable_2","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/augmentation/#add_weight_2","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/augmentation/#build_2","text":"def build ( self , input_shape : tuple ) -> None Constructs the transpose layer, it will take input_shape as input and it will not return any output. Parameters: Name Type Description Default input_shape tuple Tensor shape of the input. None View Source def build ( self , input_shape : tuple ) -> None : \"\"\" Constructs the transpose layer, it will take input_shape as input and it will not return any output. :param input_shape: Tensor shape of the input. :type input_shape: tuple \"\"\" dim = len ( input_shape ) axis_len = len ( self . axis ) if axis_len < dim : self . axis = tuple ( self . axis ) + tuple ( range ( dim ))[ axis_len : ]","title":"build"},{"location":"reference/grid_transformer/augmentation/#call_2","text":"def call ( self , inputs : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor This function will take inputs tensor as input and it will return the transposed tensor. Parameters: Name Type Description Default inputs tf.Tensor Input tensor to be transposed. None Returns: Type Description tf.Tensor Transposed tensor. View Source def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" This function will take inputs tensor as input and it will return the transposed tensor. :param inputs: Input tensor to be transposed. :type inputs: tf.Tensor :return: Transposed tensor. :rtype: tf.Tensor \"\"\" return tf . transpose ( inputs , self . axis )","title":"call"},{"location":"reference/grid_transformer/augmentation/#compute_mask_2","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/augmentation/#compute_output_shape_2","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/augmentation/#compute_output_signature_2","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/augmentation/#count_params_2","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/augmentation/#finalize_state_2","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/augmentation/#get_config_2","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) )","title":"get_config"},{"location":"reference/grid_transformer/augmentation/#get_input_at_2","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/augmentation/#get_input_mask_at_2","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/augmentation/#get_input_shape_at_2","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/augmentation/#get_output_at_2","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/augmentation/#get_output_mask_at_2","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/augmentation/#get_output_shape_at_2","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/augmentation/#get_weights_2","text":"def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/grid_transformer/augmentation/#set_weights_2","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/augmented_transformer/","text":"Module grid_transformer.augmented_transformer This module contains functions and classes for creating grid transformer with augmentation and preprocessing. Functions: augmented_transformer : This function creates grid transformer with augmentation and preprocessing. It takes several parameters such as last_index, pre, noise, mask, augmentation, root, transpose, print_, output_names, return_attention_scores and return_extractor_input and returns a constructor for grid transformer with augmentation. View Source \"\"\" This module contains functions and classes for creating grid transformer with augmentation and preprocessing. Functions: - `augmented_transformer`: This function creates grid transformer with augmentation and preprocessing. It takes several parameters such as last_index, pre, noise, mask, augmentation, root, transpose, print_, output_names, return_attention_scores and return_extractor_input and returns a constructor for grid transformer with augmentation. \"\"\" from functools import partial from typing import Union , List , Callable from ml_utils import lw , il from models_utils import SubClassing , InferencePass from data_utils import TARGET , OUTPUT , INDEX , MASK , ATTENTION , IMAGE , LABELS from models_utils import INPUTS from grid_transformer.grid_transformer import grid_transformer from grid_transformer import augmentation from grid_transformer.mask import random_mask , constant_mask from grid_transformer.preprocessing import get_images , get_images_with_index , random_last , get_images_no_answer , \\ repeat_last def augmented_transformer ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable : \"\"\" This function creates grid transformer with augmentation and preprocessing. Args: last_index (int): The last index of the dataset. Default is 8. pre (str): The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. noise (Union[Tuple[str, float], float, None]): The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. mask (Union[str, int, Callable, None]): The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. augmentation (Union[str, List[str], None]): The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. root (int): The root value to apply to the augmentation. Default is 2. transpose (bool): Whether to transpose the images before applying the augmentation. Default is True. print_ (bool): Whether to print the progress of the augmentation. Default is False. output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. Default is False. return_extractor_input (bool): Whether to return the input to the extractor or not. Default is False. kwargs: Additional keyword arguments to pass to the grid_trans function. Returns: A constructor for grid transformer with augmentation. \"\"\" if pre == \"images\" : pre = partial ( get_images , last_index = last_index ) elif pre == \"index\" : pre = partial ( get_images_with_index , index = 0 , last_index = last_index ) elif pre == \"no_answer\" : pre = partial ( get_images_no_answer , last_index = last_index ) elif pre == \"last\" : pre = partial ( repeat_last , last_index = last_index ) elif pre == \"random_last\" : pre = partial ( random_last , last_index = last_index ) elif il ( pre ): pre = SubClassing ( pre ) if il ( noise ): noise , noise_prob = noise [ 0 ], noise [ 1 ] elif isinstance ( noise , float ): noise , noise_prob = \"batch\" , noise else : noise_prob = 1.0 if noise == \"batch\" : noise = InferencePass ( aug . BatchNoise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass(partial(aug.batch_noise, last_index=last_index, prob=noise_prob)) elif noise : noise = InferencePass ( aug . Noise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass(partial(aug.Noise, last_index=last_index, prob=noise_prob)) if mask : if mask == \"random\" : mask = partial ( random_mask , last_index = last_index + 1 ) elif mask == \"last\" : mask = partial ( constant_mask , value = last_index ) elif isinstance ( mask , int ): mask = partial ( constant_mask , value = mask ) if augmentation : augmentation = InferencePass ( aug . rand ( augmentation , root = root , transpose = transpose ), print_ = print_ ) output_names = get_output_names ( output_names , return_attention_scores , return_extractor_input ) model = grid_transformer ( return_attention_scores = return_attention_scores , return_extractor_input = return_extractor_input , ** kwargs ) def apply ( x ): inputs = x [ INPUTS ] target = x [ TARGET ] if pre : index = x [ INDEX ] inputs = pre (( inputs , index )) target = pre (( target , index )) if noise : inputs = noise ( inputs ) if mask : mask_ = mask ( inputs ) else : mask_ = x [ MASK ] if augmentation : inputs , target , mask_ = augmentation ( inputs , target , mask_ ) output = lw ( model (( inputs , mask_ ))) return { ** x , INPUTS : inputs , LABELS : x [ TARGET ], # later will be use for metrics calculation TARGET : target , MASK : mask_ , ** { name : output [ i ] for i , name in enumerate ( output_names )} } return apply def get_output_names ( output_names : Union [ str , List [ str ], None ], return_attention_scores : bool , return_extractor_input : bool ) -> List [ str ]: \"\"\" This function returns the output names of the model. Args: output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. return_extractor_input (bool): Whether to return the input to the extractor or not. Returns: A list of names of the outputs. \"\"\" if not output_names : output_names = [ OUTPUT ] if return_attention_scores : output_names += [ ATTENTION ] if return_extractor_input : output_names += [ IMAGE ] return output_names aug_trans = augmented_transformer Variables ATTENTION IMAGE INDEX INPUTS LABELS MASK OUTPUT TARGET Functions aug_trans def aug_trans ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable This function creates grid transformer with augmentation and preprocessing. Parameters: Name Type Description Default last_index int The last index of the dataset. Default is 8. None pre str The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. None noise Union[Tuple[str, float], float, None] The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. None mask Union[str, int, Callable, None] The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. None augmentation Union[str, List[str], None] The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. None root int The root value to apply to the augmentation. Default is 2. None transpose bool Whether to transpose the images before applying the augmentation. Default is True. None print_ bool Whether to print the progress of the augmentation. Default is False. None output_names Union[str, List[str], None] The names of the output. Default is None. None return_attention_scores bool Whether to return attention scores or not. Default is False. None return_extractor_input bool Whether to return the input to the extractor or not. Default is False. None kwargs None Additional keyword arguments to pass to the grid_trans function. None Returns: Type Description None A constructor for grid transformer with augmentation. View Source def augmented_transformer ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable : \"\"\" This function creates grid transformer with augmentation and preprocessing. Args: last_index (int): The last index of the dataset. Default is 8. pre (str): The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. noise (Union[Tuple[str, float], float, None]): The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. mask (Union[str, int, Callable, None]): The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. augmentation (Union[str, List[str], None]): The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. root (int): The root value to apply to the augmentation. Default is 2. transpose (bool): Whether to transpose the images before applying the augmentation. Default is True. print_ (bool): Whether to print the progress of the augmentation. Default is False. output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. Default is False. return_extractor_input (bool): Whether to return the input to the extractor or not. Default is False. kwargs: Additional keyword arguments to pass to the grid_trans function. Returns: A constructor for grid transformer with augmentation. \"\"\" if pre == \"images\" : pre = partial ( get_images , last_index = last_index ) elif pre == \"index\" : pre = partial ( get_images_with_index , index = 0 , last_index = last_index ) elif pre == \"no_answer\" : pre = partial ( get_images_no_answer , last_index = last_index ) elif pre == \"last\" : pre = partial ( repeat_last , last_index = last_index ) elif pre == \"random_last\" : pre = partial ( random_last , last_index = last_index ) elif il ( pre ) : pre = SubClassing ( pre ) if il ( noise ) : noise , noise_prob = noise [ 0 ] , noise [ 1 ] elif isinstance ( noise , float ) : noise , noise_prob = \"batch\" , noise else : noise_prob = 1.0 if noise == \"batch\" : noise = InferencePass ( aug . BatchNoise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass ( partial ( aug . batch_noise , last_index = last_index , prob = noise_prob )) elif noise : noise = InferencePass ( aug . Noise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass ( partial ( aug . Noise , last_index = last_index , prob = noise_prob )) if mask : if mask == \"random\" : mask = partial ( random_mask , last_index = last_index + 1 ) elif mask == \"last\" : mask = partial ( constant_mask , value = last_index ) elif isinstance ( mask , int ) : mask = partial ( constant_mask , value = mask ) if augmentation : augmentation = InferencePass ( aug . rand ( augmentation , root = root , transpose = transpose ), print_ = print_ ) output_names = get_output_names ( output_names , return_attention_scores , return_extractor_input ) model = grid_transformer ( return_attention_scores = return_attention_scores , return_extractor_input = return_extractor_input , ** kwargs ) def apply ( x ) : inputs = x [ INPUTS ] target = x [ TARGET ] if pre : index = x [ INDEX ] inputs = pre (( inputs , index )) target = pre (( target , index )) if noise : inputs = noise ( inputs ) if mask : mask_ = mask ( inputs ) else : mask_ = x [ MASK ] if augmentation : inputs , target , mask_ = augmentation ( inputs , target , mask_ ) output = lw ( model (( inputs , mask_ ))) return { ** x , INPUTS : inputs , LABELS : x [ TARGET ] , # later will be use for metrics calculation TARGET : target , MASK : mask_ , ** { name : output [ i ] for i , name in enumerate ( output_names ) } } return apply augmented_transformer def augmented_transformer ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable This function creates grid transformer with augmentation and preprocessing. Parameters: Name Type Description Default last_index int The last index of the dataset. Default is 8. None pre str The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. None noise Union[Tuple[str, float], float, None] The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. None mask Union[str, int, Callable, None] The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. None augmentation Union[str, List[str], None] The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. None root int The root value to apply to the augmentation. Default is 2. None transpose bool Whether to transpose the images before applying the augmentation. Default is True. None print_ bool Whether to print the progress of the augmentation. Default is False. None output_names Union[str, List[str], None] The names of the output. Default is None. None return_attention_scores bool Whether to return attention scores or not. Default is False. None return_extractor_input bool Whether to return the input to the extractor or not. Default is False. None kwargs None Additional keyword arguments to pass to the grid_trans function. None Returns: Type Description None A constructor for grid transformer with augmentation. View Source def augmented_transformer ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable : \"\"\" This function creates grid transformer with augmentation and preprocessing. Args: last_index (int): The last index of the dataset. Default is 8. pre (str): The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. noise (Union[Tuple[str, float], float, None]): The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. mask (Union[str, int, Callable, None]): The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. augmentation (Union[str, List[str], None]): The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. root (int): The root value to apply to the augmentation. Default is 2. transpose (bool): Whether to transpose the images before applying the augmentation. Default is True. print_ (bool): Whether to print the progress of the augmentation. Default is False. output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. Default is False. return_extractor_input (bool): Whether to return the input to the extractor or not. Default is False. kwargs: Additional keyword arguments to pass to the grid_trans function. Returns: A constructor for grid transformer with augmentation. \"\"\" if pre == \"images\" : pre = partial ( get_images , last_index = last_index ) elif pre == \"index\" : pre = partial ( get_images_with_index , index = 0 , last_index = last_index ) elif pre == \"no_answer\" : pre = partial ( get_images_no_answer , last_index = last_index ) elif pre == \"last\" : pre = partial ( repeat_last , last_index = last_index ) elif pre == \"random_last\" : pre = partial ( random_last , last_index = last_index ) elif il ( pre ) : pre = SubClassing ( pre ) if il ( noise ) : noise , noise_prob = noise [ 0 ] , noise [ 1 ] elif isinstance ( noise , float ) : noise , noise_prob = \"batch\" , noise else : noise_prob = 1.0 if noise == \"batch\" : noise = InferencePass ( aug . BatchNoise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass ( partial ( aug . batch_noise , last_index = last_index , prob = noise_prob )) elif noise : noise = InferencePass ( aug . Noise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass ( partial ( aug . Noise , last_index = last_index , prob = noise_prob )) if mask : if mask == \"random\" : mask = partial ( random_mask , last_index = last_index + 1 ) elif mask == \"last\" : mask = partial ( constant_mask , value = last_index ) elif isinstance ( mask , int ) : mask = partial ( constant_mask , value = mask ) if augmentation : augmentation = InferencePass ( aug . rand ( augmentation , root = root , transpose = transpose ), print_ = print_ ) output_names = get_output_names ( output_names , return_attention_scores , return_extractor_input ) model = grid_transformer ( return_attention_scores = return_attention_scores , return_extractor_input = return_extractor_input , ** kwargs ) def apply ( x ) : inputs = x [ INPUTS ] target = x [ TARGET ] if pre : index = x [ INDEX ] inputs = pre (( inputs , index )) target = pre (( target , index )) if noise : inputs = noise ( inputs ) if mask : mask_ = mask ( inputs ) else : mask_ = x [ MASK ] if augmentation : inputs , target , mask_ = augmentation ( inputs , target , mask_ ) output = lw ( model (( inputs , mask_ ))) return { ** x , INPUTS : inputs , LABELS : x [ TARGET ] , # later will be use for metrics calculation TARGET : target , MASK : mask_ , ** { name : output [ i ] for i , name in enumerate ( output_names ) } } return apply get_output_names def get_output_names ( output_names : Union [ str , List [ str ], NoneType ], return_attention_scores : bool , return_extractor_input : bool ) -> List [ str ] This function returns the output names of the model. Parameters: Name Type Description Default output_names Union[str, List[str], None] The names of the output. Default is None. None return_attention_scores bool Whether to return attention scores or not. None return_extractor_input bool Whether to return the input to the extractor or not. None Returns: Type Description None A list of names of the outputs. View Source def get_output_names ( output_names : Union [ str, List[str ] , None ] , return_attention_scores : bool , return_extractor_input : bool ) -> List [ str ] : \"\"\" This function returns the output names of the model. Args: output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. return_extractor_input (bool): Whether to return the input to the extractor or not. Returns: A list of names of the outputs. \"\"\" if not output_names : output_names = [ OUTPUT ] if return_attention_scores : output_names += [ ATTENTION ] if return_extractor_input : output_names += [ IMAGE ] return output_names","title":"Augmented Transformer"},{"location":"reference/grid_transformer/augmented_transformer/#module-grid_transformeraugmented_transformer","text":"This module contains functions and classes for creating grid transformer with augmentation and preprocessing. Functions: augmented_transformer : This function creates grid transformer with augmentation and preprocessing. It takes several parameters such as last_index, pre, noise, mask, augmentation, root, transpose, print_, output_names, return_attention_scores and return_extractor_input and returns a constructor for grid transformer with augmentation. View Source \"\"\" This module contains functions and classes for creating grid transformer with augmentation and preprocessing. Functions: - `augmented_transformer`: This function creates grid transformer with augmentation and preprocessing. It takes several parameters such as last_index, pre, noise, mask, augmentation, root, transpose, print_, output_names, return_attention_scores and return_extractor_input and returns a constructor for grid transformer with augmentation. \"\"\" from functools import partial from typing import Union , List , Callable from ml_utils import lw , il from models_utils import SubClassing , InferencePass from data_utils import TARGET , OUTPUT , INDEX , MASK , ATTENTION , IMAGE , LABELS from models_utils import INPUTS from grid_transformer.grid_transformer import grid_transformer from grid_transformer import augmentation from grid_transformer.mask import random_mask , constant_mask from grid_transformer.preprocessing import get_images , get_images_with_index , random_last , get_images_no_answer , \\ repeat_last def augmented_transformer ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable : \"\"\" This function creates grid transformer with augmentation and preprocessing. Args: last_index (int): The last index of the dataset. Default is 8. pre (str): The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. noise (Union[Tuple[str, float], float, None]): The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. mask (Union[str, int, Callable, None]): The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. augmentation (Union[str, List[str], None]): The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. root (int): The root value to apply to the augmentation. Default is 2. transpose (bool): Whether to transpose the images before applying the augmentation. Default is True. print_ (bool): Whether to print the progress of the augmentation. Default is False. output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. Default is False. return_extractor_input (bool): Whether to return the input to the extractor or not. Default is False. kwargs: Additional keyword arguments to pass to the grid_trans function. Returns: A constructor for grid transformer with augmentation. \"\"\" if pre == \"images\" : pre = partial ( get_images , last_index = last_index ) elif pre == \"index\" : pre = partial ( get_images_with_index , index = 0 , last_index = last_index ) elif pre == \"no_answer\" : pre = partial ( get_images_no_answer , last_index = last_index ) elif pre == \"last\" : pre = partial ( repeat_last , last_index = last_index ) elif pre == \"random_last\" : pre = partial ( random_last , last_index = last_index ) elif il ( pre ): pre = SubClassing ( pre ) if il ( noise ): noise , noise_prob = noise [ 0 ], noise [ 1 ] elif isinstance ( noise , float ): noise , noise_prob = \"batch\" , noise else : noise_prob = 1.0 if noise == \"batch\" : noise = InferencePass ( aug . BatchNoise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass(partial(aug.batch_noise, last_index=last_index, prob=noise_prob)) elif noise : noise = InferencePass ( aug . Noise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass(partial(aug.Noise, last_index=last_index, prob=noise_prob)) if mask : if mask == \"random\" : mask = partial ( random_mask , last_index = last_index + 1 ) elif mask == \"last\" : mask = partial ( constant_mask , value = last_index ) elif isinstance ( mask , int ): mask = partial ( constant_mask , value = mask ) if augmentation : augmentation = InferencePass ( aug . rand ( augmentation , root = root , transpose = transpose ), print_ = print_ ) output_names = get_output_names ( output_names , return_attention_scores , return_extractor_input ) model = grid_transformer ( return_attention_scores = return_attention_scores , return_extractor_input = return_extractor_input , ** kwargs ) def apply ( x ): inputs = x [ INPUTS ] target = x [ TARGET ] if pre : index = x [ INDEX ] inputs = pre (( inputs , index )) target = pre (( target , index )) if noise : inputs = noise ( inputs ) if mask : mask_ = mask ( inputs ) else : mask_ = x [ MASK ] if augmentation : inputs , target , mask_ = augmentation ( inputs , target , mask_ ) output = lw ( model (( inputs , mask_ ))) return { ** x , INPUTS : inputs , LABELS : x [ TARGET ], # later will be use for metrics calculation TARGET : target , MASK : mask_ , ** { name : output [ i ] for i , name in enumerate ( output_names )} } return apply def get_output_names ( output_names : Union [ str , List [ str ], None ], return_attention_scores : bool , return_extractor_input : bool ) -> List [ str ]: \"\"\" This function returns the output names of the model. Args: output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. return_extractor_input (bool): Whether to return the input to the extractor or not. Returns: A list of names of the outputs. \"\"\" if not output_names : output_names = [ OUTPUT ] if return_attention_scores : output_names += [ ATTENTION ] if return_extractor_input : output_names += [ IMAGE ] return output_names aug_trans = augmented_transformer","title":"Module grid_transformer.augmented_transformer"},{"location":"reference/grid_transformer/augmented_transformer/#variables","text":"ATTENTION IMAGE INDEX INPUTS LABELS MASK OUTPUT TARGET","title":"Variables"},{"location":"reference/grid_transformer/augmented_transformer/#functions","text":"","title":"Functions"},{"location":"reference/grid_transformer/augmented_transformer/#aug_trans","text":"def aug_trans ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable This function creates grid transformer with augmentation and preprocessing. Parameters: Name Type Description Default last_index int The last index of the dataset. Default is 8. None pre str The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. None noise Union[Tuple[str, float], float, None] The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. None mask Union[str, int, Callable, None] The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. None augmentation Union[str, List[str], None] The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. None root int The root value to apply to the augmentation. Default is 2. None transpose bool Whether to transpose the images before applying the augmentation. Default is True. None print_ bool Whether to print the progress of the augmentation. Default is False. None output_names Union[str, List[str], None] The names of the output. Default is None. None return_attention_scores bool Whether to return attention scores or not. Default is False. None return_extractor_input bool Whether to return the input to the extractor or not. Default is False. None kwargs None Additional keyword arguments to pass to the grid_trans function. None Returns: Type Description None A constructor for grid transformer with augmentation. View Source def augmented_transformer ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable : \"\"\" This function creates grid transformer with augmentation and preprocessing. Args: last_index (int): The last index of the dataset. Default is 8. pre (str): The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. noise (Union[Tuple[str, float], float, None]): The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. mask (Union[str, int, Callable, None]): The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. augmentation (Union[str, List[str], None]): The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. root (int): The root value to apply to the augmentation. Default is 2. transpose (bool): Whether to transpose the images before applying the augmentation. Default is True. print_ (bool): Whether to print the progress of the augmentation. Default is False. output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. Default is False. return_extractor_input (bool): Whether to return the input to the extractor or not. Default is False. kwargs: Additional keyword arguments to pass to the grid_trans function. Returns: A constructor for grid transformer with augmentation. \"\"\" if pre == \"images\" : pre = partial ( get_images , last_index = last_index ) elif pre == \"index\" : pre = partial ( get_images_with_index , index = 0 , last_index = last_index ) elif pre == \"no_answer\" : pre = partial ( get_images_no_answer , last_index = last_index ) elif pre == \"last\" : pre = partial ( repeat_last , last_index = last_index ) elif pre == \"random_last\" : pre = partial ( random_last , last_index = last_index ) elif il ( pre ) : pre = SubClassing ( pre ) if il ( noise ) : noise , noise_prob = noise [ 0 ] , noise [ 1 ] elif isinstance ( noise , float ) : noise , noise_prob = \"batch\" , noise else : noise_prob = 1.0 if noise == \"batch\" : noise = InferencePass ( aug . BatchNoise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass ( partial ( aug . batch_noise , last_index = last_index , prob = noise_prob )) elif noise : noise = InferencePass ( aug . Noise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass ( partial ( aug . Noise , last_index = last_index , prob = noise_prob )) if mask : if mask == \"random\" : mask = partial ( random_mask , last_index = last_index + 1 ) elif mask == \"last\" : mask = partial ( constant_mask , value = last_index ) elif isinstance ( mask , int ) : mask = partial ( constant_mask , value = mask ) if augmentation : augmentation = InferencePass ( aug . rand ( augmentation , root = root , transpose = transpose ), print_ = print_ ) output_names = get_output_names ( output_names , return_attention_scores , return_extractor_input ) model = grid_transformer ( return_attention_scores = return_attention_scores , return_extractor_input = return_extractor_input , ** kwargs ) def apply ( x ) : inputs = x [ INPUTS ] target = x [ TARGET ] if pre : index = x [ INDEX ] inputs = pre (( inputs , index )) target = pre (( target , index )) if noise : inputs = noise ( inputs ) if mask : mask_ = mask ( inputs ) else : mask_ = x [ MASK ] if augmentation : inputs , target , mask_ = augmentation ( inputs , target , mask_ ) output = lw ( model (( inputs , mask_ ))) return { ** x , INPUTS : inputs , LABELS : x [ TARGET ] , # later will be use for metrics calculation TARGET : target , MASK : mask_ , ** { name : output [ i ] for i , name in enumerate ( output_names ) } } return apply","title":"aug_trans"},{"location":"reference/grid_transformer/augmented_transformer/#augmented_transformer","text":"def augmented_transformer ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable This function creates grid transformer with augmentation and preprocessing. Parameters: Name Type Description Default last_index int The last index of the dataset. Default is 8. None pre str The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. None noise Union[Tuple[str, float], float, None] The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. None mask Union[str, int, Callable, None] The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. None augmentation Union[str, List[str], None] The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. None root int The root value to apply to the augmentation. Default is 2. None transpose bool Whether to transpose the images before applying the augmentation. Default is True. None print_ bool Whether to print the progress of the augmentation. Default is False. None output_names Union[str, List[str], None] The names of the output. Default is None. None return_attention_scores bool Whether to return attention scores or not. Default is False. None return_extractor_input bool Whether to return the input to the extractor or not. Default is False. None kwargs None Additional keyword arguments to pass to the grid_trans function. None Returns: Type Description None A constructor for grid transformer with augmentation. View Source def augmented_transformer ( last_index : int = 8 , pre : str = 'images' , noise = None , mask = None , augmentation = None , root : int = 2 , transpose : bool = True , print_ : bool = False , output_names = None , return_attention_scores : bool = False , return_extractor_input : bool = False , ** kwargs ) -> Callable : \"\"\" This function creates grid transformer with augmentation and preprocessing. Args: last_index (int): The last index of the dataset. Default is 8. pre (str): The type of pre-processing to apply to the images. Can be 'images', 'index', 'no_answer', 'last' or 'random_last'. Default is 'images'. noise (Union[Tuple[str, float], float, None]): The type of noise to apply to the images. Can be 'batch' or 'image'. Default is None. mask (Union[str, int, Callable, None]): The type of mask to apply to the images. Can be 'random', 'last', an integer, or a callable. Default is None. augmentation (Union[str, List[str], None]): The type of augmentation to apply to the images. Can be a string or a list of strings. Default is None. root (int): The root value to apply to the augmentation. Default is 2. transpose (bool): Whether to transpose the images before applying the augmentation. Default is True. print_ (bool): Whether to print the progress of the augmentation. Default is False. output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. Default is False. return_extractor_input (bool): Whether to return the input to the extractor or not. Default is False. kwargs: Additional keyword arguments to pass to the grid_trans function. Returns: A constructor for grid transformer with augmentation. \"\"\" if pre == \"images\" : pre = partial ( get_images , last_index = last_index ) elif pre == \"index\" : pre = partial ( get_images_with_index , index = 0 , last_index = last_index ) elif pre == \"no_answer\" : pre = partial ( get_images_no_answer , last_index = last_index ) elif pre == \"last\" : pre = partial ( repeat_last , last_index = last_index ) elif pre == \"random_last\" : pre = partial ( random_last , last_index = last_index ) elif il ( pre ) : pre = SubClassing ( pre ) if il ( noise ) : noise , noise_prob = noise [ 0 ] , noise [ 1 ] elif isinstance ( noise , float ) : noise , noise_prob = \"batch\" , noise else : noise_prob = 1.0 if noise == \"batch\" : noise = InferencePass ( aug . BatchNoise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass ( partial ( aug . batch_noise , last_index = last_index , prob = noise_prob )) elif noise : noise = InferencePass ( aug . Noise ( last_index = last_index , prob = noise_prob ), print_ = print_ ) # noise = InferencePass ( partial ( aug . Noise , last_index = last_index , prob = noise_prob )) if mask : if mask == \"random\" : mask = partial ( random_mask , last_index = last_index + 1 ) elif mask == \"last\" : mask = partial ( constant_mask , value = last_index ) elif isinstance ( mask , int ) : mask = partial ( constant_mask , value = mask ) if augmentation : augmentation = InferencePass ( aug . rand ( augmentation , root = root , transpose = transpose ), print_ = print_ ) output_names = get_output_names ( output_names , return_attention_scores , return_extractor_input ) model = grid_transformer ( return_attention_scores = return_attention_scores , return_extractor_input = return_extractor_input , ** kwargs ) def apply ( x ) : inputs = x [ INPUTS ] target = x [ TARGET ] if pre : index = x [ INDEX ] inputs = pre (( inputs , index )) target = pre (( target , index )) if noise : inputs = noise ( inputs ) if mask : mask_ = mask ( inputs ) else : mask_ = x [ MASK ] if augmentation : inputs , target , mask_ = augmentation ( inputs , target , mask_ ) output = lw ( model (( inputs , mask_ ))) return { ** x , INPUTS : inputs , LABELS : x [ TARGET ] , # later will be use for metrics calculation TARGET : target , MASK : mask_ , ** { name : output [ i ] for i , name in enumerate ( output_names ) } } return apply","title":"augmented_transformer"},{"location":"reference/grid_transformer/augmented_transformer/#get_output_names","text":"def get_output_names ( output_names : Union [ str , List [ str ], NoneType ], return_attention_scores : bool , return_extractor_input : bool ) -> List [ str ] This function returns the output names of the model. Parameters: Name Type Description Default output_names Union[str, List[str], None] The names of the output. Default is None. None return_attention_scores bool Whether to return attention scores or not. None return_extractor_input bool Whether to return the input to the extractor or not. None Returns: Type Description None A list of names of the outputs. View Source def get_output_names ( output_names : Union [ str, List[str ] , None ] , return_attention_scores : bool , return_extractor_input : bool ) -> List [ str ] : \"\"\" This function returns the output names of the model. Args: output_names (Union[str, List[str], None]): The names of the output. Default is None. return_attention_scores (bool): Whether to return attention scores or not. return_extractor_input (bool): Whether to return the input to the extractor or not. Returns: A list of names of the outputs. \"\"\" if not output_names : output_names = [ OUTPUT ] if return_attention_scores : output_names += [ ATTENTION ] if return_extractor_input : output_names += [ IMAGE ] return output_names","title":"get_output_names"},{"location":"reference/grid_transformer/grid_transformer/","text":"Module grid_transformer.grid_transformer This module contains functions for creating transformer models that operate on grid-like inputs. Functions: grid_transformer: returns a callable transformer model constructor that can operate on grid-like inputs. View Source \"\"\" This module contains functions for creating transformer models that operate on grid-like inputs. Functions: - grid_transformer: returns a callable transformer model constructor that can operate on grid-like inputs. \"\"\" import math from typing import Optional , Tuple , Callable , Union import tensorflow as tf from ml_utils import flatten_return from models_utils . models . initial import InitialWeight from tensorflow . keras import Sequential from tensorflow . keras . layers import Lambda from grid_transformer . layers import Vec , Conversion # from tensorflow . python . keras . layers import LayerNormalization , Lambda , Conv2D , Layer , Embedding from grid_transformer . mask import ImageMask , take_left , mix , empty_last , init_weights from grid_transformer . transformer import transformer from models_utils . transfer import get_extractor from models_utils . models . base import BatchModel import models_utils . ops as K from models_utils . image import rgb def grid_transformer ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = \"ef\" , output_size : int = 10 , pos_emd : str = \"cat\" , last : Union [ str , Callable ] = \"start\" , pooling : Optional [ Union [ str , int , Callable ]] = None , model : Optional [ tf . keras . Model ] = None , map_fn : Union [ str , Callable ] = \"batch\" , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , **kwargs ) -> Callable : \"\"\" Applies a grid transformation to an image. Parameters ---------- extractor_shape : Union[int, Tuple[int, int, int]] Shape of the image extractor. batch_no : Optional[int] Number of batches to be used. col : int Number of columns in the grid. row : int Number of rows in the grid. no : int Number of attention heads. extractor : Union[str, Callable] Extractor to be used for extracting features from the image. output_size : int Size of the output of the transformer model. pos_emd : str Type of positional encoding to be used. last : Union[str, Callable] Last layer of the transformer model. pooling : Optional[Union[str, int, Callable]] Pooling function to be used. model : Optional[tf.keras.Model] Transformer model to be used. map_fn : Union[str, Callable] Function to map the extracted features to the transformer model. channel : Optional[str] How channels are handled. last_index : Optional[int] Index of the last layer. return_extractor_input : bool If True, returns the input of the extractor. return_attention_scores : bool If True, returns the attention scores. **kwargs : Additional keyword arguments to be passed to the transformer model. Returns ------- Callable Grid Transformer constructor. \"\"\" extractor_shape = ( extractor_shape , extractor_shape , 3 ) if isinstance ( extractor_shape , int ) else extractor_shape image_shape = ( math . ceil ( extractor_shape [ 0 ] / col ), math . ceil ( extractor_shape [ 0 ] / row )) if last == \"left\" : last = take_left elif last == \"mix\" : last = mix elif last == \"empty\" : last = empty_last elif last == \"start\" : last = Sequential ([ Lambda ( empty_last ), InitialWeight ( initializer = init_weights )]) batch_no = batch_no extractor = extractor pos_emd = pos_emd output_size = output_size pooling = pooling no = no model = model channel = channel last_index = last_index return_extractor_input = return_extractor_input if map_fn == \"vec\" : map_fn = Vec elif callable ( map_fn ) : map_fn = map_fn else : map_fn = BatchModel def apply ( x : tf . Tensor ) -> tf . Tensor : nonlocal batch_no nonlocal extractor nonlocal model nonlocal pooling nonlocal channel nonlocal return_extractor_input x , mask_ = x x = tf . image . resize ( tf . transpose ( x , ( 0 , 2 , 3 , 1 )), image_shape ) x = ImageMask ( last = last , last_index = last_index )(( x , mask_ )) shape = tf . shape ( x ) if channel == \"tile\" : x = tf . transpose ( x [..., None ], [ 0 , 3 , 1 , 2 , 4 ]) x = rgb (( 1 , 1 , 1 , 1 , 3 ))( x ) else : x = tf . reshape ( x , tf . concat ([ shape [ :- 1 ], [ int ( x . shape [ - 1 ] / 3 ), 3 ]], axis = 0 )) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 , 4 ]) if batch_no is None : batch_no = int ( x . shape [ 1 ] / ( row * col )) if row > 1 or col > 1 : x = K . create_image_grid2 ( x , row = row , col = col ) x = x [ : , : , : extractor_shape [ 0 ], : extractor_shape [ 1 ]] # from data_utils import ims , imb # imb ( x [ : 9 ]. transpose (( 0 , 1 , 4 , 2 , 3 ))) if batch_no > 1 : extractor = map_fn ( get_extractor ( data = extractor_shape , batch = False , model = extractor )) if not callable ( pooling ) : pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) else : x = x [ : , 0 ] pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) if model is None : model = transformer ( extractor = extractor , pos_emd = pos_emd , output_size = output_size , pooling = pooling , return_attention_scores = return_attention_scores , no = no , **kwargs ) if return_extractor_input : return flatten_return ( model ( x ), x ) return model ( x ) return apply grid_trans = grid_transformer Functions grid_trans def grid_trans ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = 'ef' , output_size : int = 10 , pos_emd : str = 'cat' , last : Union [ str , Callable ] = 'start' , pooling : Union [ str , int , Callable , NoneType ] = None , model : Optional [ keras . engine . training . Model ] = None , map_fn : Union [ str , Callable ] = 'batch' , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , ** kwargs ) -> Callable Applies a grid transformation to an image. Parameters: Name Type Description Default extractor_shape Union[int, Tuple[int, int, int]] Shape of the image extractor. None batch_no Optional[int] Number of batches to be used. None col int Number of columns in the grid. None row int Number of rows in the grid. None no int Number of attention heads. None extractor Union[str, Callable] Extractor to be used for extracting features from the image. None output_size int Size of the output of the transformer model. None pos_emd str Type of positional encoding to be used. None last Union[str, Callable] Last layer of the transformer model. None pooling Optional[Union[str, int, Callable]] Pooling function to be used. None model Optional[tf.keras.Model] Transformer model to be used. None map_fn Union[str, Callable] Function to map the extracted features to the transformer model. None channel Optional[str] How channels are handled. None last_index Optional[int] Index of the last layer. None return_extractor_input bool If True, returns the input of the extractor. None return_attention_scores bool If True, returns the attention scores. None **kwargs Additional keyword arguments to be passed to the transformer model. None Returns: Type Description Callable Grid Transformer constructor. View Source def grid_transformer ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = \"ef\" , output_size : int = 10 , pos_emd : str = \"cat\" , last : Union [ str , Callable ] = \"start\" , pooling : Optional [ Union [ str , int , Callable ]] = None , model : Optional [ tf . keras . Model ] = None , map_fn : Union [ str , Callable ] = \"batch\" , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , **kwargs ) -> Callable : \"\"\" Applies a grid transformation to an image. Parameters ---------- extractor_shape : Union[int, Tuple[int, int, int]] Shape of the image extractor. batch_no : Optional[int] Number of batches to be used. col : int Number of columns in the grid. row : int Number of rows in the grid. no : int Number of attention heads. extractor : Union[str, Callable] Extractor to be used for extracting features from the image. output_size : int Size of the output of the transformer model. pos_emd : str Type of positional encoding to be used. last : Union[str, Callable] Last layer of the transformer model. pooling : Optional[Union[str, int, Callable]] Pooling function to be used. model : Optional[tf.keras.Model] Transformer model to be used. map_fn : Union[str, Callable] Function to map the extracted features to the transformer model. channel : Optional[str] How channels are handled. last_index : Optional[int] Index of the last layer. return_extractor_input : bool If True, returns the input of the extractor. return_attention_scores : bool If True, returns the attention scores. **kwargs : Additional keyword arguments to be passed to the transformer model. Returns ------- Callable Grid Transformer constructor. \"\"\" extractor_shape = ( extractor_shape , extractor_shape , 3 ) if isinstance ( extractor_shape , int ) else extractor_shape image_shape = ( math . ceil ( extractor_shape [ 0 ] / col ), math . ceil ( extractor_shape [ 0 ] / row )) if last == \"left\" : last = take_left elif last == \"mix\" : last = mix elif last == \"empty\" : last = empty_last elif last == \"start\" : last = Sequential ([ Lambda ( empty_last ), InitialWeight ( initializer = init_weights )]) batch_no = batch_no extractor = extractor pos_emd = pos_emd output_size = output_size pooling = pooling no = no model = model channel = channel last_index = last_index return_extractor_input = return_extractor_input if map_fn == \"vec\" : map_fn = Vec elif callable ( map_fn ) : map_fn = map_fn else : map_fn = BatchModel def apply ( x : tf . Tensor ) -> tf . Tensor : nonlocal batch_no nonlocal extractor nonlocal model nonlocal pooling nonlocal channel nonlocal return_extractor_input x , mask_ = x x = tf . image . resize ( tf . transpose ( x , ( 0 , 2 , 3 , 1 )), image_shape ) x = ImageMask ( last = last , last_index = last_index )(( x , mask_ )) shape = tf . shape ( x ) if channel == \"tile\" : x = tf . transpose ( x [..., None ], [ 0 , 3 , 1 , 2 , 4 ]) x = rgb (( 1 , 1 , 1 , 1 , 3 ))( x ) else : x = tf . reshape ( x , tf . concat ([ shape [ :- 1 ], [ int ( x . shape [ - 1 ] / 3 ), 3 ]], axis = 0 )) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 , 4 ]) if batch_no is None : batch_no = int ( x . shape [ 1 ] / ( row * col )) if row > 1 or col > 1 : x = K . create_image_grid2 ( x , row = row , col = col ) x = x [ : , : , : extractor_shape [ 0 ], : extractor_shape [ 1 ]] # from data_utils import ims , imb # imb ( x [ : 9 ]. transpose (( 0 , 1 , 4 , 2 , 3 ))) if batch_no > 1 : extractor = map_fn ( get_extractor ( data = extractor_shape , batch = False , model = extractor )) if not callable ( pooling ) : pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) else : x = x [ : , 0 ] pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) if model is None : model = transformer ( extractor = extractor , pos_emd = pos_emd , output_size = output_size , pooling = pooling , return_attention_scores = return_attention_scores , no = no , **kwargs ) if return_extractor_input : return flatten_return ( model ( x ), x ) return model ( x ) return apply grid_transformer def grid_transformer ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = 'ef' , output_size : int = 10 , pos_emd : str = 'cat' , last : Union [ str , Callable ] = 'start' , pooling : Union [ str , int , Callable , NoneType ] = None , model : Optional [ keras . engine . training . Model ] = None , map_fn : Union [ str , Callable ] = 'batch' , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , ** kwargs ) -> Callable Applies a grid transformation to an image. Parameters: Name Type Description Default extractor_shape Union[int, Tuple[int, int, int]] Shape of the image extractor. None batch_no Optional[int] Number of batches to be used. None col int Number of columns in the grid. None row int Number of rows in the grid. None no int Number of attention heads. None extractor Union[str, Callable] Extractor to be used for extracting features from the image. None output_size int Size of the output of the transformer model. None pos_emd str Type of positional encoding to be used. None last Union[str, Callable] Last layer of the transformer model. None pooling Optional[Union[str, int, Callable]] Pooling function to be used. None model Optional[tf.keras.Model] Transformer model to be used. None map_fn Union[str, Callable] Function to map the extracted features to the transformer model. None channel Optional[str] How channels are handled. None last_index Optional[int] Index of the last layer. None return_extractor_input bool If True, returns the input of the extractor. None return_attention_scores bool If True, returns the attention scores. None **kwargs Additional keyword arguments to be passed to the transformer model. None Returns: Type Description Callable Grid Transformer constructor. View Source def grid_transformer ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = \"ef\" , output_size : int = 10 , pos_emd : str = \"cat\" , last : Union [ str , Callable ] = \"start\" , pooling : Optional [ Union [ str , int , Callable ]] = None , model : Optional [ tf . keras . Model ] = None , map_fn : Union [ str , Callable ] = \"batch\" , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , **kwargs ) -> Callable : \"\"\" Applies a grid transformation to an image. Parameters ---------- extractor_shape : Union[int, Tuple[int, int, int]] Shape of the image extractor. batch_no : Optional[int] Number of batches to be used. col : int Number of columns in the grid. row : int Number of rows in the grid. no : int Number of attention heads. extractor : Union[str, Callable] Extractor to be used for extracting features from the image. output_size : int Size of the output of the transformer model. pos_emd : str Type of positional encoding to be used. last : Union[str, Callable] Last layer of the transformer model. pooling : Optional[Union[str, int, Callable]] Pooling function to be used. model : Optional[tf.keras.Model] Transformer model to be used. map_fn : Union[str, Callable] Function to map the extracted features to the transformer model. channel : Optional[str] How channels are handled. last_index : Optional[int] Index of the last layer. return_extractor_input : bool If True, returns the input of the extractor. return_attention_scores : bool If True, returns the attention scores. **kwargs : Additional keyword arguments to be passed to the transformer model. Returns ------- Callable Grid Transformer constructor. \"\"\" extractor_shape = ( extractor_shape , extractor_shape , 3 ) if isinstance ( extractor_shape , int ) else extractor_shape image_shape = ( math . ceil ( extractor_shape [ 0 ] / col ), math . ceil ( extractor_shape [ 0 ] / row )) if last == \"left\" : last = take_left elif last == \"mix\" : last = mix elif last == \"empty\" : last = empty_last elif last == \"start\" : last = Sequential ([ Lambda ( empty_last ), InitialWeight ( initializer = init_weights )]) batch_no = batch_no extractor = extractor pos_emd = pos_emd output_size = output_size pooling = pooling no = no model = model channel = channel last_index = last_index return_extractor_input = return_extractor_input if map_fn == \"vec\" : map_fn = Vec elif callable ( map_fn ) : map_fn = map_fn else : map_fn = BatchModel def apply ( x : tf . Tensor ) -> tf . Tensor : nonlocal batch_no nonlocal extractor nonlocal model nonlocal pooling nonlocal channel nonlocal return_extractor_input x , mask_ = x x = tf . image . resize ( tf . transpose ( x , ( 0 , 2 , 3 , 1 )), image_shape ) x = ImageMask ( last = last , last_index = last_index )(( x , mask_ )) shape = tf . shape ( x ) if channel == \"tile\" : x = tf . transpose ( x [..., None ], [ 0 , 3 , 1 , 2 , 4 ]) x = rgb (( 1 , 1 , 1 , 1 , 3 ))( x ) else : x = tf . reshape ( x , tf . concat ([ shape [ :- 1 ], [ int ( x . shape [ - 1 ] / 3 ), 3 ]], axis = 0 )) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 , 4 ]) if batch_no is None : batch_no = int ( x . shape [ 1 ] / ( row * col )) if row > 1 or col > 1 : x = K . create_image_grid2 ( x , row = row , col = col ) x = x [ : , : , : extractor_shape [ 0 ], : extractor_shape [ 1 ]] # from data_utils import ims , imb # imb ( x [ : 9 ]. transpose (( 0 , 1 , 4 , 2 , 3 ))) if batch_no > 1 : extractor = map_fn ( get_extractor ( data = extractor_shape , batch = False , model = extractor )) if not callable ( pooling ) : pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) else : x = x [ : , 0 ] pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) if model is None : model = transformer ( extractor = extractor , pos_emd = pos_emd , output_size = output_size , pooling = pooling , return_attention_scores = return_attention_scores , no = no , **kwargs ) if return_extractor_input : return flatten_return ( model ( x ), x ) return model ( x ) return apply","title":"Grid Transformer"},{"location":"reference/grid_transformer/grid_transformer/#module-grid_transformergrid_transformer","text":"This module contains functions for creating transformer models that operate on grid-like inputs. Functions: grid_transformer: returns a callable transformer model constructor that can operate on grid-like inputs. View Source \"\"\" This module contains functions for creating transformer models that operate on grid-like inputs. Functions: - grid_transformer: returns a callable transformer model constructor that can operate on grid-like inputs. \"\"\" import math from typing import Optional , Tuple , Callable , Union import tensorflow as tf from ml_utils import flatten_return from models_utils . models . initial import InitialWeight from tensorflow . keras import Sequential from tensorflow . keras . layers import Lambda from grid_transformer . layers import Vec , Conversion # from tensorflow . python . keras . layers import LayerNormalization , Lambda , Conv2D , Layer , Embedding from grid_transformer . mask import ImageMask , take_left , mix , empty_last , init_weights from grid_transformer . transformer import transformer from models_utils . transfer import get_extractor from models_utils . models . base import BatchModel import models_utils . ops as K from models_utils . image import rgb def grid_transformer ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = \"ef\" , output_size : int = 10 , pos_emd : str = \"cat\" , last : Union [ str , Callable ] = \"start\" , pooling : Optional [ Union [ str , int , Callable ]] = None , model : Optional [ tf . keras . Model ] = None , map_fn : Union [ str , Callable ] = \"batch\" , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , **kwargs ) -> Callable : \"\"\" Applies a grid transformation to an image. Parameters ---------- extractor_shape : Union[int, Tuple[int, int, int]] Shape of the image extractor. batch_no : Optional[int] Number of batches to be used. col : int Number of columns in the grid. row : int Number of rows in the grid. no : int Number of attention heads. extractor : Union[str, Callable] Extractor to be used for extracting features from the image. output_size : int Size of the output of the transformer model. pos_emd : str Type of positional encoding to be used. last : Union[str, Callable] Last layer of the transformer model. pooling : Optional[Union[str, int, Callable]] Pooling function to be used. model : Optional[tf.keras.Model] Transformer model to be used. map_fn : Union[str, Callable] Function to map the extracted features to the transformer model. channel : Optional[str] How channels are handled. last_index : Optional[int] Index of the last layer. return_extractor_input : bool If True, returns the input of the extractor. return_attention_scores : bool If True, returns the attention scores. **kwargs : Additional keyword arguments to be passed to the transformer model. Returns ------- Callable Grid Transformer constructor. \"\"\" extractor_shape = ( extractor_shape , extractor_shape , 3 ) if isinstance ( extractor_shape , int ) else extractor_shape image_shape = ( math . ceil ( extractor_shape [ 0 ] / col ), math . ceil ( extractor_shape [ 0 ] / row )) if last == \"left\" : last = take_left elif last == \"mix\" : last = mix elif last == \"empty\" : last = empty_last elif last == \"start\" : last = Sequential ([ Lambda ( empty_last ), InitialWeight ( initializer = init_weights )]) batch_no = batch_no extractor = extractor pos_emd = pos_emd output_size = output_size pooling = pooling no = no model = model channel = channel last_index = last_index return_extractor_input = return_extractor_input if map_fn == \"vec\" : map_fn = Vec elif callable ( map_fn ) : map_fn = map_fn else : map_fn = BatchModel def apply ( x : tf . Tensor ) -> tf . Tensor : nonlocal batch_no nonlocal extractor nonlocal model nonlocal pooling nonlocal channel nonlocal return_extractor_input x , mask_ = x x = tf . image . resize ( tf . transpose ( x , ( 0 , 2 , 3 , 1 )), image_shape ) x = ImageMask ( last = last , last_index = last_index )(( x , mask_ )) shape = tf . shape ( x ) if channel == \"tile\" : x = tf . transpose ( x [..., None ], [ 0 , 3 , 1 , 2 , 4 ]) x = rgb (( 1 , 1 , 1 , 1 , 3 ))( x ) else : x = tf . reshape ( x , tf . concat ([ shape [ :- 1 ], [ int ( x . shape [ - 1 ] / 3 ), 3 ]], axis = 0 )) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 , 4 ]) if batch_no is None : batch_no = int ( x . shape [ 1 ] / ( row * col )) if row > 1 or col > 1 : x = K . create_image_grid2 ( x , row = row , col = col ) x = x [ : , : , : extractor_shape [ 0 ], : extractor_shape [ 1 ]] # from data_utils import ims , imb # imb ( x [ : 9 ]. transpose (( 0 , 1 , 4 , 2 , 3 ))) if batch_no > 1 : extractor = map_fn ( get_extractor ( data = extractor_shape , batch = False , model = extractor )) if not callable ( pooling ) : pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) else : x = x [ : , 0 ] pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) if model is None : model = transformer ( extractor = extractor , pos_emd = pos_emd , output_size = output_size , pooling = pooling , return_attention_scores = return_attention_scores , no = no , **kwargs ) if return_extractor_input : return flatten_return ( model ( x ), x ) return model ( x ) return apply grid_trans = grid_transformer","title":"Module grid_transformer.grid_transformer"},{"location":"reference/grid_transformer/grid_transformer/#functions","text":"","title":"Functions"},{"location":"reference/grid_transformer/grid_transformer/#grid_trans","text":"def grid_trans ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = 'ef' , output_size : int = 10 , pos_emd : str = 'cat' , last : Union [ str , Callable ] = 'start' , pooling : Union [ str , int , Callable , NoneType ] = None , model : Optional [ keras . engine . training . Model ] = None , map_fn : Union [ str , Callable ] = 'batch' , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , ** kwargs ) -> Callable Applies a grid transformation to an image. Parameters: Name Type Description Default extractor_shape Union[int, Tuple[int, int, int]] Shape of the image extractor. None batch_no Optional[int] Number of batches to be used. None col int Number of columns in the grid. None row int Number of rows in the grid. None no int Number of attention heads. None extractor Union[str, Callable] Extractor to be used for extracting features from the image. None output_size int Size of the output of the transformer model. None pos_emd str Type of positional encoding to be used. None last Union[str, Callable] Last layer of the transformer model. None pooling Optional[Union[str, int, Callable]] Pooling function to be used. None model Optional[tf.keras.Model] Transformer model to be used. None map_fn Union[str, Callable] Function to map the extracted features to the transformer model. None channel Optional[str] How channels are handled. None last_index Optional[int] Index of the last layer. None return_extractor_input bool If True, returns the input of the extractor. None return_attention_scores bool If True, returns the attention scores. None **kwargs Additional keyword arguments to be passed to the transformer model. None Returns: Type Description Callable Grid Transformer constructor. View Source def grid_transformer ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = \"ef\" , output_size : int = 10 , pos_emd : str = \"cat\" , last : Union [ str , Callable ] = \"start\" , pooling : Optional [ Union [ str , int , Callable ]] = None , model : Optional [ tf . keras . Model ] = None , map_fn : Union [ str , Callable ] = \"batch\" , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , **kwargs ) -> Callable : \"\"\" Applies a grid transformation to an image. Parameters ---------- extractor_shape : Union[int, Tuple[int, int, int]] Shape of the image extractor. batch_no : Optional[int] Number of batches to be used. col : int Number of columns in the grid. row : int Number of rows in the grid. no : int Number of attention heads. extractor : Union[str, Callable] Extractor to be used for extracting features from the image. output_size : int Size of the output of the transformer model. pos_emd : str Type of positional encoding to be used. last : Union[str, Callable] Last layer of the transformer model. pooling : Optional[Union[str, int, Callable]] Pooling function to be used. model : Optional[tf.keras.Model] Transformer model to be used. map_fn : Union[str, Callable] Function to map the extracted features to the transformer model. channel : Optional[str] How channels are handled. last_index : Optional[int] Index of the last layer. return_extractor_input : bool If True, returns the input of the extractor. return_attention_scores : bool If True, returns the attention scores. **kwargs : Additional keyword arguments to be passed to the transformer model. Returns ------- Callable Grid Transformer constructor. \"\"\" extractor_shape = ( extractor_shape , extractor_shape , 3 ) if isinstance ( extractor_shape , int ) else extractor_shape image_shape = ( math . ceil ( extractor_shape [ 0 ] / col ), math . ceil ( extractor_shape [ 0 ] / row )) if last == \"left\" : last = take_left elif last == \"mix\" : last = mix elif last == \"empty\" : last = empty_last elif last == \"start\" : last = Sequential ([ Lambda ( empty_last ), InitialWeight ( initializer = init_weights )]) batch_no = batch_no extractor = extractor pos_emd = pos_emd output_size = output_size pooling = pooling no = no model = model channel = channel last_index = last_index return_extractor_input = return_extractor_input if map_fn == \"vec\" : map_fn = Vec elif callable ( map_fn ) : map_fn = map_fn else : map_fn = BatchModel def apply ( x : tf . Tensor ) -> tf . Tensor : nonlocal batch_no nonlocal extractor nonlocal model nonlocal pooling nonlocal channel nonlocal return_extractor_input x , mask_ = x x = tf . image . resize ( tf . transpose ( x , ( 0 , 2 , 3 , 1 )), image_shape ) x = ImageMask ( last = last , last_index = last_index )(( x , mask_ )) shape = tf . shape ( x ) if channel == \"tile\" : x = tf . transpose ( x [..., None ], [ 0 , 3 , 1 , 2 , 4 ]) x = rgb (( 1 , 1 , 1 , 1 , 3 ))( x ) else : x = tf . reshape ( x , tf . concat ([ shape [ :- 1 ], [ int ( x . shape [ - 1 ] / 3 ), 3 ]], axis = 0 )) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 , 4 ]) if batch_no is None : batch_no = int ( x . shape [ 1 ] / ( row * col )) if row > 1 or col > 1 : x = K . create_image_grid2 ( x , row = row , col = col ) x = x [ : , : , : extractor_shape [ 0 ], : extractor_shape [ 1 ]] # from data_utils import ims , imb # imb ( x [ : 9 ]. transpose (( 0 , 1 , 4 , 2 , 3 ))) if batch_no > 1 : extractor = map_fn ( get_extractor ( data = extractor_shape , batch = False , model = extractor )) if not callable ( pooling ) : pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) else : x = x [ : , 0 ] pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) if model is None : model = transformer ( extractor = extractor , pos_emd = pos_emd , output_size = output_size , pooling = pooling , return_attention_scores = return_attention_scores , no = no , **kwargs ) if return_extractor_input : return flatten_return ( model ( x ), x ) return model ( x ) return apply","title":"grid_trans"},{"location":"reference/grid_transformer/grid_transformer/#grid_transformer","text":"def grid_transformer ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = 'ef' , output_size : int = 10 , pos_emd : str = 'cat' , last : Union [ str , Callable ] = 'start' , pooling : Union [ str , int , Callable , NoneType ] = None , model : Optional [ keras . engine . training . Model ] = None , map_fn : Union [ str , Callable ] = 'batch' , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , ** kwargs ) -> Callable Applies a grid transformation to an image. Parameters: Name Type Description Default extractor_shape Union[int, Tuple[int, int, int]] Shape of the image extractor. None batch_no Optional[int] Number of batches to be used. None col int Number of columns in the grid. None row int Number of rows in the grid. None no int Number of attention heads. None extractor Union[str, Callable] Extractor to be used for extracting features from the image. None output_size int Size of the output of the transformer model. None pos_emd str Type of positional encoding to be used. None last Union[str, Callable] Last layer of the transformer model. None pooling Optional[Union[str, int, Callable]] Pooling function to be used. None model Optional[tf.keras.Model] Transformer model to be used. None map_fn Union[str, Callable] Function to map the extracted features to the transformer model. None channel Optional[str] How channels are handled. None last_index Optional[int] Index of the last layer. None return_extractor_input bool If True, returns the input of the extractor. None return_attention_scores bool If True, returns the attention scores. None **kwargs Additional keyword arguments to be passed to the transformer model. None Returns: Type Description Callable Grid Transformer constructor. View Source def grid_transformer ( extractor_shape : Union [ int , Tuple [ int , int , int ]] = 224 , batch_no : Optional [ int ] = None , col : int = 3 , row : int = 3 , no : int = 4 , extractor : Union [ str , Callable ] = \"ef\" , output_size : int = 10 , pos_emd : str = \"cat\" , last : Union [ str , Callable ] = \"start\" , pooling : Optional [ Union [ str , int , Callable ]] = None , model : Optional [ tf . keras . Model ] = None , map_fn : Union [ str , Callable ] = \"batch\" , channel : Optional [ str ] = None , last_index : Optional [ int ] = None , return_extractor_input : bool = False , return_attention_scores : bool = False , **kwargs ) -> Callable : \"\"\" Applies a grid transformation to an image. Parameters ---------- extractor_shape : Union[int, Tuple[int, int, int]] Shape of the image extractor. batch_no : Optional[int] Number of batches to be used. col : int Number of columns in the grid. row : int Number of rows in the grid. no : int Number of attention heads. extractor : Union[str, Callable] Extractor to be used for extracting features from the image. output_size : int Size of the output of the transformer model. pos_emd : str Type of positional encoding to be used. last : Union[str, Callable] Last layer of the transformer model. pooling : Optional[Union[str, int, Callable]] Pooling function to be used. model : Optional[tf.keras.Model] Transformer model to be used. map_fn : Union[str, Callable] Function to map the extracted features to the transformer model. channel : Optional[str] How channels are handled. last_index : Optional[int] Index of the last layer. return_extractor_input : bool If True, returns the input of the extractor. return_attention_scores : bool If True, returns the attention scores. **kwargs : Additional keyword arguments to be passed to the transformer model. Returns ------- Callable Grid Transformer constructor. \"\"\" extractor_shape = ( extractor_shape , extractor_shape , 3 ) if isinstance ( extractor_shape , int ) else extractor_shape image_shape = ( math . ceil ( extractor_shape [ 0 ] / col ), math . ceil ( extractor_shape [ 0 ] / row )) if last == \"left\" : last = take_left elif last == \"mix\" : last = mix elif last == \"empty\" : last = empty_last elif last == \"start\" : last = Sequential ([ Lambda ( empty_last ), InitialWeight ( initializer = init_weights )]) batch_no = batch_no extractor = extractor pos_emd = pos_emd output_size = output_size pooling = pooling no = no model = model channel = channel last_index = last_index return_extractor_input = return_extractor_input if map_fn == \"vec\" : map_fn = Vec elif callable ( map_fn ) : map_fn = map_fn else : map_fn = BatchModel def apply ( x : tf . Tensor ) -> tf . Tensor : nonlocal batch_no nonlocal extractor nonlocal model nonlocal pooling nonlocal channel nonlocal return_extractor_input x , mask_ = x x = tf . image . resize ( tf . transpose ( x , ( 0 , 2 , 3 , 1 )), image_shape ) x = ImageMask ( last = last , last_index = last_index )(( x , mask_ )) shape = tf . shape ( x ) if channel == \"tile\" : x = tf . transpose ( x [..., None ], [ 0 , 3 , 1 , 2 , 4 ]) x = rgb (( 1 , 1 , 1 , 1 , 3 ))( x ) else : x = tf . reshape ( x , tf . concat ([ shape [ :- 1 ], [ int ( x . shape [ - 1 ] / 3 ), 3 ]], axis = 0 )) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 , 4 ]) if batch_no is None : batch_no = int ( x . shape [ 1 ] / ( row * col )) if row > 1 or col > 1 : x = K . create_image_grid2 ( x , row = row , col = col ) x = x [ : , : , : extractor_shape [ 0 ], : extractor_shape [ 1 ]] # from data_utils import ims , imb # imb ( x [ : 9 ]. transpose (( 0 , 1 , 4 , 2 , 3 ))) if batch_no > 1 : extractor = map_fn ( get_extractor ( data = extractor_shape , batch = False , model = extractor )) if not callable ( pooling ) : pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) else : x = x [ : , 0 ] pooling = lambda y = pooling : Conversion ( y if isinstance ( y , int ) else 9 ) if model is None : model = transformer ( extractor = extractor , pos_emd = pos_emd , output_size = output_size , pooling = pooling , return_attention_scores = return_attention_scores , no = no , **kwargs ) if return_extractor_input : return flatten_return ( model ( x ), x ) return model ( x ) return apply","title":"grid_transformer"},{"location":"reference/grid_transformer/layers/","text":"Module grid_transformer.layers The module contains two classes, Conversion and Vec . Conversion class is a model that converts input sequence of tokens to a sequence of tokens desired size and desired number of tokens. It can be initialized with a desired size and maximum token number. The build method is used to set the token_max and mul attributes based on the input_shape, and the call method performs the conversion by reshaping the inputs tensor. Vec class is a model that applies a given model on each element of the input tensor. It is initialized with a model, and the call method applies the model on each element of the input tensor by first transposing the input tensor and then using tf.vectorized_map to apply the model on each element, and then transposing the output tensor. Example usage: # create an instance of Conversion model conversion_model = Conversion ( size = 5 , max_ = 20 ) conversion_model . build (( None , 30 , 100 )) # create an instance of Vec model vec_model = Vec ( tf . keras . layers . Dense ( units = 64 )) # input tensor x = tf . random . normal (( 10 , 30 , 100 )) # apply conversion model on input tensor converted_x = conversion_model ( x ) # apply vec model on input tensor vec_x = vec_model ( x ) View Source \" \"\" The module contains two classes, `Conversion` and `Vec`. `Conversion` class is a model that converts input sequence of tokens to a sequence of tokens desired size and desired number of tokens. It can be initialized with a desired size and maximum token number. The build method is used to set the token_max and mul attributes based on the input_shape, and the call method performs the conversion by reshaping the inputs tensor. `Vec` class is a model that applies a given model on each element of the input tensor. It is initialized with a model, and the call method applies the model on each element of the input tensor by first transposing the input tensor and then using tf.vectorized_map to apply the model on each element, and then transposing the output tensor. Example usage: ```python # create an instance of Conversion model conversion_model = Conversion(size=5, max_=20) conversion_model.build((None, 30, 100)) # create an instance of Vec model vec_model = Vec(tf.keras.layers.Dense(units=64)) # input tensor x = tf.random.normal((10,30,100)) # apply conversion model on input tensor converted_x = conversion_model(x) # apply vec model on input tensor vec_x = vec_model(x) ``` \"\" \" from typing import Tuple , Optional import tensorflow as tf import math from loguru import logger from tensorflow . keras import Model class Conversion ( Model ) : \" \"\" A model for converting input sequence of tokens to a sequence of tokens desired size and desired number of tokens.. \"\" \" def __init__ ( self , size : int = 9 , max_ : Optional [ int ] = None ) : \" \"\" Initialize the class with desired size and maximum token number. :param size: The desired size of the output tokens. :param max_: The maximum number of tokens in the output shape. \"\" \" super (). __init__ () self . token_max = max_ # number off transformer output token that will be used to create model output self . size = size # number of outputs self . mul = 1 def build ( self , input_shape : Tuple [ int , int , int ] ) -> None : \" \"\" Build the model. :param input_shape: The input shape of the model. \"\" \" if self . token_max is None : ratio = input_shape [ 1 ] / self . size if ratio < 1 : self . mul = math . ceil ( 1 / ratio ) self . token_max = self . size if input_shape [ 2 ] % self . mul != 0 : # todo high rise error logger . error ( f \"To create {self.size} from {input_shape[1]} tokens the size of transformer {input_shape[2]} need to divisible by {self.mul}.\" ) else : self . token_max = math . floor ( ratio ) * self . size def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \" \"\" Perform the conversion. :param inputs: Input tensor of shape (batch_size, tokens, features). :return: Tensor of shape (batch_size, size, features * (token_max / size)). \"\" \" def call ( self , inputs ) : shape = tf . shape ( inputs ) if self . mul > 1 : inputs = tf . reshape ( inputs , ( shape [ 0 ] , int ( inputs . shape [ 1 ] * self . mul ), int ( inputs . shape [ 2 ] / self . mul ))) shape = tf . shape ( inputs ) return tf . reshape ( inputs [ : , : self . token_max ] , tf . stack ( [ shape [ 0 ] , self . size , int ( inputs . shape [ - 1 ] * ( self . token_max / self . size )) ] )) class Vec ( tf . keras . Model ) : def __init__ ( self , model : tf . keras . Model ) : \" \"\" Class that applies a given model on each element of the input tensor. Parameters: model (tf.keras.Model): The model to apply on each element of the input tensor. \"\" \" super (). __init__ () self . model = model def call ( self , x : tf . Tensor ) -> tf . Tensor : \" \"\" Applies the model on each element of the input tensor. Parameters: x (tf.Tensor): The input tensor Returns: tf.Tensor: The output tensor with the model applied on each element. \"\" \" x = tf . transpose ( x , perm = ( 1 , 0 , 2 , 3 , 4 )) x = tf . vectorized_map ( self . model , x ) return tf . transpose ( x , perm = ( 1 , 0 , 2 , 3 , 4 )) Classes Conversion class Conversion ( size : int = 9 , max_ : Optional [ int ] = None ) A model for converting input sequence of tokens to a sequence of tokens desired size and desired number of tokens.. View Source class Conversion ( Model ) : \"\"\" A model for converting input sequence of tokens to a sequence of tokens desired size and desired number of tokens.. \"\"\" def __init__ ( self , size : int = 9 , max_ : Optional [ int ] = None ) : \"\"\" Initialize the class with desired size and maximum token number. :param size: The desired size of the output tokens. :param max_: The maximum number of tokens in the output shape. \"\"\" super (). __init__ () self . token_max = max_ # number off transformer output token that will be used to create model output self . size = size # number of outputs self . mul = 1 def build ( self , input_shape : Tuple [ int, int, int ] ) -> None : \"\"\" Build the model. :param input_shape: The input shape of the model. \"\"\" if self . token_max is None : ratio = input_shape [ 1 ] / self . size if ratio < 1 : self . mul = math . ceil ( 1 / ratio ) self . token_max = self . size if input_shape [ 2 ] % self . mul != 0 : # todo high rise error logger . error ( f \"To create {self.size} from {input_shape[1]} tokens the size of transformer {input_shape[2]} need to divisible by {self.mul}.\" ) else : self . token_max = math . floor ( ratio ) * self . size def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" Perform the conversion. :param inputs: Input tensor of shape (batch_size, tokens, features). :return: Tensor of shape (batch_size, size, features * (token_max / size)). \"\"\" def call ( self , inputs ) : shape = tf . shape ( inputs ) if self . mul > 1 : inputs = tf . reshape ( inputs , ( shape [ 0 ] , int ( inputs . shape [ 1 ] * self . mul ), int ( inputs . shape [ 2 ] / self . mul ))) shape = tf . shape ( inputs ) return tf . reshape ( inputs [ :, :self.token_max ] , tf . stack ( [ shape[0 ] , self . size , int ( inputs . shape [ -1 ] * ( self . token_max / self . size )) ] )) Ancestors (in MRO) keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Static methods from_config def from_config ( config , custom_objects = None ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config , custom_objects = None ): compile_config = config . pop ( \"compile_config\" , None ) build_input_shape = config . pop ( \"build_input_shape\" , None ) # `from_config` assumes `cls` is either `Functional` or a child class of # `Functional`. In the case that `cls` is meant to behave like a child # class of `Functional` but only inherits from the `Model` class, we # have to call `cls(...)` instead of `Functional.from_config`. from keras.engine import functional with serialization . SharedObjectLoadingScope (): functional_model_keys = [ \"name\" , \"layers\" , \"input_layers\" , \"output_layers\" , ] if all ( key in config for key in functional_model_keys ): inputs , outputs , layers = functional . reconstruct_from_config ( config , custom_objects ) model = cls ( inputs = inputs , outputs = outputs , name = config . get ( \"name\" ) ) functional . connect_ancillary_layers ( model , layers ) else : # The config does not contain all the information necessary to # revive a Functional model. This happens when the user creates # subclassed models where `get_config()` is returning # insufficient information to be considered a Functional model. # In this case, we fall back to provide all config into the # constructor of the class. try : model = cls ( ** config ) except TypeError as e : raise TypeError ( \"Unable to revive model from config. When overriding \" \"the `get_config()`, make sure that the returned \" \"config contains all items used as arguments in the \" f \"constructor to { cls } , which is the default behavior. \" \"You can override this default behavior by defining a \" \"`from_config` method to specify how to create an \" f \"instance of { cls . __name__ } from the config. \\n\\n \" f \"Error encountered during deserialization: \\n { e } \" ) if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ): if build_input_shape : model . build ( build_input_shape ) if compile_config is not None : model . _compile_from_config ( compile_config , base_class = Model ) return model with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. distribute_reduction_method The method employed to reduce per-replica values during training. Unless specified, the value \"auto\" will be assumed, indicating that the reduction strategy should be chosen based on the current running environment. See reduce_per_replica function for more details. distribute_strategy The tf.distribute.Strategy this model was created under. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . layers losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics Returns the model's metrics added using compile() , add_metric() APIs. Note: Metrics passed to compile() are available only after a keras.Model has been trained/evaluated on actual data. metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. state_updates Deprecated, do NOT use! Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape : Tuple [ int , int , int ] ) -> None Build the model. Parameters: Name Type Description Default input_shape None The input shape of the model. None View Source def build ( self , input_shape : Tuple [ int , int , int ]) -> None : \"\"\" Build the model. :param input_shape: The input shape of the model. \"\"\" if self . token_max is None : ratio = input_shape [ 1 ] / self . size if ratio < 1 : self . mul = math . ceil ( 1 / ratio ) self . token_max = self . size if input_shape [ 2 ] % self . mul != 0 : # todo high rise error logger . error ( f \"To create {self.size} from {input_shape[1]} tokens the size of transformer {input_shape[2]} need to divisible by {self.mul}.\" ) else : self . token_max = math . floor ( ratio ) * self . size call def call ( self , inputs ) Calls the model on new inputs and returns the outputs as tensors. In this case call() just reapplies all ops in the graph to the new inputs (e.g. build a new computational graph from the provided inputs). Note: This method should not be called directly. It is only meant to be overridden when subclassing tf.keras.Model . To call a model on an input, always use the __call__() method, i.e. model(inputs) , which relies on the underlying call() method. Parameters: Name Type Description Default inputs None Input tensor, or dict/list/tuple of input tensors. None training None Boolean or boolean scalar tensor, indicating whether to run the Network in training mode or inference mode. None mask None A mask or list of masks. A mask can be either a boolean tensor or None (no mask). For more details, check the guide here . None Returns: Type Description None A tensor if there is a single output, or a list of tensors if there are more than one outputs. View Source def call ( self , inputs ) : shape = tf . shape ( inputs ) if self . mul > 1 : inputs = tf . reshape ( inputs , ( shape [ 0 ], int ( inputs . shape [ 1 ] * self . mul ) , int ( inputs . shape [ 2 ] / self . mul ))) shape = tf . shape ( inputs ) return tf . reshape ( inputs [:, : self . token_max ], tf . stack ( [ shape [ 0 ], self . size , int ( inputs . shape [ - 1 ] * ( self . token_max / self . size )) ] )) compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Parameters: Name Type Description Default optimizer None String (name of optimizer) or optimizer instance. See tf.keras.optimizers . None loss None Loss function. May be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true,<br>y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. None metrics None List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true,<br>y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the shapes of the targets and of the model output. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. The metrics passed here are evaluated without sample weighting; if you would like sample weighting to apply, you can specify your metrics via the weighted_metrics argument instead. None loss_weights None Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. None weighted_metrics None List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. None run_eagerly None Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . False steps_per_execution None Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). 1 jit_compile None If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. None **kwargs None Arguments supported for backwards compatibility only. None View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = \"rmsprop\" , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs , ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . May be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = {' output_a ':' accuracy ' , ' output_b ' :[ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the shapes of the targets and of the model output . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . The metrics passed here are evaluated without sample weighting ; if you would like sample weighting to apply , you can specify your metrics via the ` weighted_metrics ` argument instead . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( \"compile\" ). set ( True ) self . _compile_config = generic_utils . Config ( optimizer = optimizer , loss = loss , metrics = metrics , loss_weights = loss_weights , weighted_metrics = weighted_metrics , run_eagerly = run_eagerly , steps_per_execution = steps_per_execution , jit_compile = jit_compile , ) with self . distribute_strategy . scope () : if \"experimental_steps_per_execution\" in kwargs : logging . warning ( \"The argument `steps_per_execution` is no longer \" \"experimental. Pass `steps_per_execution` instead of \" \"`experimental_steps_per_execution`.\" ) if not steps_per_execution : steps_per_execution = kwargs . pop ( \"experimental_steps_per_execution\" ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for # multi-output models, which have prefixes added for each # corresponding output name). from_serialized = kwargs . pop ( \"from_serialized\" , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) if isinstance ( loss , compile_utils . LossesContainer ) : self . compiled_loss = loss else : self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized , ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( \"You cannot enable `run_eagerly` and `jit_compile` \" \"at the same time.\" ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses ) compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model.call(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns # results for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) return self . get_metrics_result () compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 'auto' , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs,<br> targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). None batch_size None Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None sample_weight None Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . None steps None Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None **kwargs None Unused at this time. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = \"auto\" , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs , ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( \"evaluate\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"evaluate\" ) self . _assert_compile_was_called () self . _check_call_args ( \"evaluate\" ) self . _check_sample_weight_warning ( x , sample_weight ) _disallow_inside_tf_function ( \"evaluate\" ) use_cached_eval_dataset = kwargs . pop ( \"_use_cached_eval_dataset\" , False ) if kwargs : raise TypeError ( f \"Invalid keyword arguments: {list(kwargs.keys())}\" ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , \"_eval_data_handler\" , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch # iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( \"test\" , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.evaluate_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.evaluate`, which supports generators.\" , stacklevel = 2 , ) self . _check_call_args ( \"evaluate_generator\" ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include sample_weights as a third component, note that sample weighting applies to the weighted_metrics argument but not the metrics argument in compile() . If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. If both validation_data and validation_split are provided, validation_data will override validation_split . validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . Note that sample weighting does not apply to metrics specified via the metrics argument in compile() . To apply sample weighting to your metrics, you can specify them via the weighted_metrics in compile() instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = \"auto\" , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include `sample_weights` as a third component, note that sample weighting applies to the `weighted_metrics` argument but not the `metrics` argument in `compile()`. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. If both `validation_data` and `validation_split` are provided, `validation_data` will override `validation_split`. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. Note that sample weighting does not apply to metrics specified via the `metrics` argument in `compile()`. To apply sample weighting to your metrics, you can specify them via the `weighted_metrics` in `compile()` instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( \"fit\" ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( \"Model\" , \"fit\" ) self . _assert_compile_was_called () self . _check_call_args ( \"fit\" ) _disallow_inside_tf_function ( \"fit\" ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) if validation_split and validation_data is None : # Create the validation data using the training data. Only supported # for `Tensor` and `NumPy` input. ( x , y , sample_weight , ), validation_data = data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split ) if validation_data : ( val_x , val_y , val_sample_weight , ) = data_adapter . unpack_x_y_sample_weight ( validation_data ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. steps_per_epoch_inferred = ( steps_per_epoch or data_handler . inferred_steps ) ( data_handler . _initial_epoch , data_handler . _initial_step , ) = self . _maybe_load_initial_counters_from_ckpt ( steps_per_epoch_inferred , initial_epoch ) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( \"train\" , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 , ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( \"Unexpected result of `train_function` \" \"(Empty logs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , \"_eval_data_handler\" , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items () } epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break if ( isinstance ( self . optimizer , optimizer_experimental . Optimizer ) and epochs > 0 ): self . optimizer . finalize_variable_values ( self . trainable_variables ) # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , \"_eval_data_handler\" , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 , ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.fit_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.fit`, which supports generators.\" , stacklevel = 2 , ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch , ) get_config def get_config ( self ) Returns the config of the Model . Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a Model . This allows the Model to be be reinstantiated later (without its trained weights) from this configuration. Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed Model are advised to override this method, and continue to update the dict from super(MyModel, self).get_config() to provide the proper configuration of this Model . The default config is an empty dict. Optionally, raise NotImplementedError to allow Keras to attempt a default serialization. Returns: Type Description None Python dictionary containing the configuration of this Model . View Source def get_config ( self ) : \" \"\" Returns the config of the `Model`. Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a `Model`. This allows the `Model` to be be reinstantiated later (without its trained weights) from this configuration. Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed `Model` are advised to override this method, and continue to update the dict from `super(MyModel, self).get_config()` to provide the proper configuration of this `Model`. The default config is an empty dict. Optionally, raise `NotImplementedError` to allow Keras to attempt a default serialization. Returns: Python dictionary containing the configuration of this `Model`. \"\" \" # Return an empty dict here because otherwise Model # subclass developers may see # their model's `__init__()` fed with unexpected keyword arguments, # if their `__init__()` takes no argument for example, and they # don't override `from_config()`, which would use `cls(**config)` # as a result. config = {} if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ) : if self . _is_compiled and hasattr ( self , \"_compile_config\" ) : config [ \"compile_config\" ] = self . _compile_config . serialize () if self . built : config [ \"build_input_shape\" ] = self . _build_input_shape return config get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Type Description Default name None String, name of layer. None index None Integer, index of layer. None Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( \"Provide only a layer name or a layer index. Received: \" f \"index={index}, name={name}.\" ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f \"Was asked to retrieve layer at index {index}\" f \" but model only has {len(self.layers)}\" \" layers.\" ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f \"No such layer: {name}. Existing layers are: \" f \"{list(layer.name for layer in self.layers)}.\" ) raise ValueError ( \"Provide either a layer name or layer index at `get_layer`.\" ) get_metrics_result def get_metrics_result ( self ) Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: Type Description None A dict containing values of the metrics listed in self.metrics . Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def get_metrics_result ( self ) : \" \"\" Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: A `dict` containing values of the metrics listed in `self.metrics`. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weight_paths def get_weight_paths ( self ) Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a tf.Variable instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding tf.Variable instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: Type Description None A dict where keys are variable paths and values are tf.Variable instances. View Source def get_weight_paths ( self ) : \"\"\"Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a `tf.Variable` instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding `tf.Variable` instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: A dict where keys are variable paths and values are `tf.Variable` instances. Example: ```python class SubclassModel(tf.keras.Model): def __init__(self, name=None): super().__init__(name=name) self.d1 = tf.keras.layers.Dense(10) self.d2 = tf.keras.layers.Dense(20) def call(self, inputs): x = self.d1(inputs) return self.d2(x) model = SubclassModel() model(tf.zeros((10, 10))) weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': model.d1.kernel, # 'd1.bias': model.d1.bias, # 'd2.kernel': model.d2.kernel, # 'd2.bias': model.d2.bias, # } # Functional model inputs = tf.keras.Input((10,), batch_size=10) x = tf.keras.layers.Dense(20, name='d1')(inputs) output = tf.keras.layers.Dense(30, name='d2')(x) model = tf.keras.Model(inputs, output) d1 = model.layers[1] d2 = model.layers[2] weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': d1.kernel, # 'd1.bias': d1.bias, # 'd2.kernel': d2.kernel, # 'd2.bias': d2.bias, # } ``` \"\"\" result = {} ( descendants , object_paths_dict , ) = tf . __internal__ . tracking . ObjectGraphView ( self ). breadth_first_traversal () for descendant in descendants : if isinstance ( descendant , tf . Variable ) : trackable_references = object_paths_dict [ descendant ] object_path = \".\" . join ( [ t.name for t in trackable_references ] ) result [ object_path ] = descendant return result get_weights def get_weights ( self ) Retrieves the weights of the model. Returns: Type Description None A flat list of Numpy arrays. View Source def get_weights ( self ) : \"\" \"Retrieves the weights of the model. Returns: A flat list of Numpy arrays. \"\" \" with self.distribute_strategy.scope(): return super().get_weights() load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Type Description Default filepath None String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . None by_name None Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. None skip_mismatch None Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). None options None Optional tf.train.CheckpointOptions object that specifies options for loading weights. None Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ) ): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( \"Load weights is not implemented with TPUStrategy \" \"with `steps_per_run` greater than 1. The \" f \"`steps_per_run` is {spr}\" ) if skip_mismatch and not by_name : raise ValueError ( \"When calling model.load_weights, skip_mismatch can only be \" \"set to True when by_name is True.\" ) filepath , save_format = _detect_save_format ( filepath ) if save_format == \"tf\" : status = self . _checkpoint . read ( filepath , options ) if by_name : raise NotImplementedError ( \"Weights may only be loaded based on topology into Models \" \"when loading TensorFlow-formatted weights \" \"(got by_name=True to load_weights).\" ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( \"`load_weights` requires h5py package when loading weights \" \"from HDF5. Try installing h5py.\" ) if not self . _is_graph_network and not self . built : raise ValueError ( \"Unable to load weights saved in HDF5 format into a \" \"subclassed Model which has not created its variables yet. \" \"Call the Model first, then load the weights.\" ) self . _assert_weights_created () with h5py . File ( filepath , \"r\" ) as f : if \"layer_names\" not in f . attrs and \"model_weights\" in f : f = f [ \"model_weights\" ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the predict function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = \"concat\" ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( outputs , tf . nest . map_structure ( lambda t : tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape , outputs , ), ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , reduce_retracing = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the test function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) if self . _cluster_coordinator : self . test_function = ( lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it ,) ) ) else : self . test_function = test_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it , self . _steps_per_execution . value ()) ) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the train function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = ( lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it ,) ) ) else : self . train_function = train_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it , self . _steps_per_execution . value ()) ) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 'auto' , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Type Description Default x None Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None batch_size None Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None steps None Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = \"auto\" , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks]( https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( \"predict\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"predict\" ) self . _check_call_args ( \"predict\" ) _disallow_inside_tf_function ( \"predict\" ) # TODO(yashkatariya): Cache model on the coordinator for faster # prediction. If running under PSS, then swap it with OneDeviceStrategy # so that execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None verbose = _get_verbosity ( verbose , self . distribute_strategy ) outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy ) ) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = ( data_option ) x = x . with_options ( options ) except ValueError : warnings . warn ( \"Using Model.predict with MultiWorkerMirroredStrategy \" \"or TPUStrategy and AutoShardPolicy.FILE might lead to \" \"out-of-order result. Consider setting it to \" \"AutoShardPolicy.DATA.\" , stacklevel = 2 , ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = ( tmp_batch_outputs # No error, now safe to assign. ) if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs , ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs , ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { \"outputs\" : batch_outputs } ) if batch_outputs is None : raise ValueError ( \"Unexpected result of `predict_function` \" \"(Empty batch_outputs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , potentially_ragged_concat , outputs ) # If originally PSS strategy was used, then replace it back since # predict is running under `OneDeviceStrategy` after the swap and once # its done we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.predict_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.predict`, which supports generators.\" , stacklevel = 2 , ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\" \"Returns predictions for a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`. \"\" \" self . _check_call_args ( \"predict_on_batch\" ) _disallow_inside_tf_function ( \"predict_on_batch\" ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None The result of one inference step, typically the output of calling the Model on data. View Source def predict_step ( self , data ) : \" \"\" The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by `Model.make_predict_function`. This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_predict_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: The result of one inference step, typically the output of calling the `Model` on data. \"\" \" x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) return self ( x , training = False ) reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. View Source def reset_metrics ( self ) : \"\" \"Resets the state of all the metrics in the model. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model . compile ( optimizer = \"Adam\" , loss = \"mse\" , metrics = [ \"mae\" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\" \" for m in self.metrics: m.reset_state() reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , \"reset_states\" ) and getattr ( layer , \"stateful\" , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Parameters: Name Type Description Default filepath None String, PathLike, path to SavedModel or H5 file to save the model. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None include_optimizer None If True, save optimizer's state together. None save_format None Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. None signatures None Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. None options None (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. None save_traces None (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. None View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True , ) : \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide]( https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces , ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Type Description Default dynamic_batch None Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . None Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Type Description Default filepath None String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None save_format None Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. None options None Optional tf.train.CheckpointOptions object that specifies options for saving weights. None Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints]( https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = io_utils . path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = \"h5\" else : save_format = \"tf\" else : user_format = save_format . lower () . strip () if user_format in ( \"tensorflow\" , \"tf\" ): save_format = \"tf\" elif user_format in ( \"hdf5\" , \"h5\" , \"keras\" ): save_format = \"h5\" else : raise ValueError ( f \"Unknown format. Received: `save_format`={save_format}. \" 'Was expecting one of {\"tf\", \"h5\"}.' ) if save_format == \"tf\" and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f \"filepath ({filepath}) looks like an HDF5 file. \" 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == \"h5\" and h5py is None : raise ImportError ( \"`save_weights` requires h5py when saving in hdf5, but h5py is \" \"not available. Try installing h5py package.\" ) if save_format == \"tf\" : check_filepath = filepath + \".index\" else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = io_utils . ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == \"h5\" : with h5py . File ( filepath , \"w\" ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if not tf . executing_eagerly (): # Call `get_session` to initialize any uninitialized variables. backend . get_session () self . _checkpoint . write ( filepath , options = options ) # Record this checkpoint so it's visible from # tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ], ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state () summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None ) Prints a string summary of the network. Parameters: Name Type Description Default line_length None Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). None positions None Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . None print_fn None Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. print expand_nested None Whether to expand the nested models. If not provided, defaults to False . None show_trainable None Whether to show if a layer is trainable. If not provided, defaults to False . None layer_range None a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to layer_range[0] and the end predicate will be the last element it matches to layer_range[1] . By default None which considers all layers of model. None Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None , ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. layer_range: a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to `layer_range[0]` and the end predicate will be the last element it matches to `layer_range[1]`. By default `None` which considers all layers of model. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( \"This model has not yet been built. \" \"Build the model first by calling `build()` or by calling \" \"the model on a batch of data.\" ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable , layer_range = layer_range , ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"test_on_batch\" ) _disallow_inside_tf_function ( \"test_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. View Source def test_step ( self , data ) : \" \"\" The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by `Model.make_test_function`. This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_test_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) # Updates stateful loss metrics. self . compute_loss ( x , y , y_pred , sample_weight ) return self . compute_metrics ( x , y , y_pred , sample_weight ) to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to * json.dumps() . None Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to *`json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to yaml.dump() . None Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( \"Method `model.to_yaml()` has been removed due to security risk of \" \"arbitrary code execution. Please use `model.to_json()` instead.\" ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight None Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"train_on_batch\" ) _disallow_inside_tf_function ( \"train_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see Customizing what happens in fit . This method is called by Model.make_train_function . This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def train_step ( self , data ) : \" \"\" The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see [Customizing what happens in fit]( https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This method is called by `Model.make_train_function`. This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_train_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) # Run forward pass. with tf . GradientTape () as tape : y_pred = self ( x , training = True ) loss = self . compute_loss ( x , y , y_pred , sample_weight ) self . _validate_target_and_loss ( y , loss ) # Run backwards pass. self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return self . compute_metrics ( x , y , y_pred , sample_weight ) Vec class Vec ( model : keras . engine . training . Model ) Model groups layers into an object with training and inference features. Attributes Name Type Description Default inputs None The input(s) of the model: a keras.Input object or a combination of keras.Input objects in a dict, list or tuple. None outputs None The output(s) of the model: a tensor that originated from keras.Input objects or a combination of such tensors in a dict, list or tuple. See Functional API example below. None name None String, the name of the model. None View Source class Vec ( tf . keras . Model ): def __init__ ( self , model: tf . keras . Model ): \"\"\" Class that applies a given model on each element of the input tensor. Parameters: model (tf.keras.Model): The model to apply on each element of the input tensor. \"\"\" super (). __init__ () self . model = model def call ( self , x : tf . Tensor ) -> tf . Tensor: \"\"\" Applies the model on each element of the input tensor. Parameters: x (tf.Tensor): The input tensor Returns: tf.Tensor: The output tensor with the model applied on each element. \"\"\" x = tf . transpose ( x , perm =( 1 , 0 , 2 , 3 , 4 )) x = tf . vectorized_map ( self . model , x ) return tf . transpose ( x , perm =( 1 , 0 , 2 , 3 , 4 )) Ancestors (in MRO) keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Static methods from_config def from_config ( config , custom_objects = None ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config , custom_objects = None ): compile_config = config . pop ( \"compile_config\" , None ) build_input_shape = config . pop ( \"build_input_shape\" , None ) # `from_config` assumes `cls` is either `Functional` or a child class of # `Functional`. In the case that `cls` is meant to behave like a child # class of `Functional` but only inherits from the `Model` class, we # have to call `cls(...)` instead of `Functional.from_config`. from keras.engine import functional with serialization . SharedObjectLoadingScope (): functional_model_keys = [ \"name\" , \"layers\" , \"input_layers\" , \"output_layers\" , ] if all ( key in config for key in functional_model_keys ): inputs , outputs , layers = functional . reconstruct_from_config ( config , custom_objects ) model = cls ( inputs = inputs , outputs = outputs , name = config . get ( \"name\" ) ) functional . connect_ancillary_layers ( model , layers ) else : # The config does not contain all the information necessary to # revive a Functional model. This happens when the user creates # subclassed models where `get_config()` is returning # insufficient information to be considered a Functional model. # In this case, we fall back to provide all config into the # constructor of the class. try : model = cls ( ** config ) except TypeError as e : raise TypeError ( \"Unable to revive model from config. When overriding \" \"the `get_config()`, make sure that the returned \" \"config contains all items used as arguments in the \" f \"constructor to { cls } , which is the default behavior. \" \"You can override this default behavior by defining a \" \"`from_config` method to specify how to create an \" f \"instance of { cls . __name__ } from the config. \\n\\n \" f \"Error encountered during deserialization: \\n { e } \" ) if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ): if build_input_shape : model . build ( build_input_shape ) if compile_config is not None : model . _compile_from_config ( compile_config , base_class = Model ) return model with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. distribute_reduction_method The method employed to reduce per-replica values during training. Unless specified, the value \"auto\" will be assumed, indicating that the reduction strategy should be chosen based on the current running environment. See reduce_per_replica function for more details. distribute_strategy The tf.distribute.Strategy this model was created under. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . layers losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics Returns the model's metrics added using compile() , add_metric() APIs. Note: Metrics passed to compile() are available only after a keras.Model has been trained/evaluated on actual data. metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. state_updates Deprecated, do NOT use! Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape ) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape instance, or list/dict of shapes, where shapes are tuples, integers, or TensorShape instances. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, TensorShape , or dict). 2. If the model requires call arguments that are agnostic to the input shapes (positional or keyword arg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. View Source @generic_utils.default def build ( self , input_shape ) : \" \"\" Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call `model.build()` in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, `TensorShape` instance, or list/dict of shapes, where shapes are tuples, integers, or `TensorShape` instances. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, `TensorShape`, or dict). 2. If the model requires call arguments that are agnostic to the input shapes (positional or keyword arg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. \"\" \" if self . _is_graph_network : super (). build ( input_shape ) return if input_shape is None : raise ValueError ( \"Input shape must be defined when calling `build()` on \" \"a `Model` subclass.\" ) valid_types = ( tuple , list , tf . TensorShape , dict ) if not isinstance ( input_shape , valid_types ) : raise ValueError ( \"Specified input shape is not one of the valid types. \" \"Please specify a batch input shape of type tuple or \" \"list of input shapes. User provided \" \"input type: {}.\" . format ( type ( input_shape )) ) if input_shape and not self . inputs : # We create placeholders for the `None`s in the shape and build the # model in a Graph. Since tf.Variable is compatible with both eager # execution and graph building, the variables created after building # the model in a Graph are still valid when executing eagerly. if tf . executing_eagerly () : graph = tf . __internal__ . FuncGraph ( \"build_graph\" ) else : graph = backend . get_graph () with graph . as_default () : if isinstance ( input_shape , list ) and all ( d is None or isinstance ( d , int ) for d in input_shape ) : input_shape = tuple ( input_shape ) if isinstance ( input_shape , list ) : x = [ base_layer_utils . generate_placeholders_from_shape ( shape ) for shape in input_shape ] elif isinstance ( input_shape , dict ) : x = { k : base_layer_utils . generate_placeholders_from_shape ( shape ) for k , shape in input_shape . items () } else : x = base_layer_utils . generate_placeholders_from_shape ( input_shape ) kwargs = {} call_signature = self . _call_spec . full_argspec call_args = call_signature . args # Exclude `self`, `inputs`, and any argument with a default # value. if len ( call_args ) > 2 : if call_signature . defaults : call_args = call_args [ 2 : - len ( call_signature . defaults ) ] else : call_args = call_args [ 2 : ] for arg in call_args : if arg == \"training\" : # Case where `training` is a positional arg with no # default. kwargs [ \"training\" ] = False else : # Has invalid call signature with unknown positional # arguments. raise ValueError ( \"Currently, you cannot build your model if it \" \"has positional or keyword arguments that are \" \"not inputs to the model, but are required for \" \"its `call()` method. Instead, in order to \" \"instantiate and build your model, `call()` \" \"your model on real tensor data with all \" \"expected call arguments. The argument \" \"for `call()` can be a single list/tuple that \" \"contains multiple inputs.\" ) elif len ( call_args ) < 2 : # Signature without `inputs`. raise ValueError ( \"You can only call `build()` on a model if its \" \"`call()` method accepts an `inputs` argument.\" ) try : self . call ( x , ** kwargs ) except ( tf . errors . InvalidArgumentError , TypeError ) as e : raise ValueError ( \"You cannot build your model by calling `build` \" \"if your layers do not support float type inputs. \" \"Instead, in order to instantiate and build your \" \"model, call your model on real tensor data (of \" \"the correct dtype). \\n\\n The actual error from \" f \"`call` is: {e}.\" ) super (). build ( input_shape ) call def call ( self , x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Applies the model on each element of the input tensor. Parameters: Name Type Description Default x tf.Tensor The input tensor None Returns: Type Description tf.Tensor The output tensor with the model applied on each element. View Source def call ( self , x : tf . Tensor ) -> tf . Tensor : \"\"\" Applies the model on each element of the input tensor. Parameters: x (tf.Tensor): The input tensor Returns: tf.Tensor: The output tensor with the model applied on each element. \"\"\" x = tf . transpose ( x , perm = ( 1 , 0 , 2 , 3 , 4 )) x = tf . vectorized_map ( self . model , x ) return tf . transpose ( x , perm = ( 1 , 0 , 2 , 3 , 4 )) compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Parameters: Name Type Description Default optimizer None String (name of optimizer) or optimizer instance. See tf.keras.optimizers . None loss None Loss function. May be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true,<br>y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. None metrics None List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true,<br>y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the shapes of the targets and of the model output. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. The metrics passed here are evaluated without sample weighting; if you would like sample weighting to apply, you can specify your metrics via the weighted_metrics argument instead. None loss_weights None Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. None weighted_metrics None List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. None run_eagerly None Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . False steps_per_execution None Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). 1 jit_compile None If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. None **kwargs None Arguments supported for backwards compatibility only. None View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = \"rmsprop\" , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs , ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . May be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = {' output_a ':' accuracy ' , ' output_b ' :[ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the shapes of the targets and of the model output . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . The metrics passed here are evaluated without sample weighting ; if you would like sample weighting to apply , you can specify your metrics via the ` weighted_metrics ` argument instead . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( \"compile\" ). set ( True ) self . _compile_config = generic_utils . Config ( optimizer = optimizer , loss = loss , metrics = metrics , loss_weights = loss_weights , weighted_metrics = weighted_metrics , run_eagerly = run_eagerly , steps_per_execution = steps_per_execution , jit_compile = jit_compile , ) with self . distribute_strategy . scope () : if \"experimental_steps_per_execution\" in kwargs : logging . warning ( \"The argument `steps_per_execution` is no longer \" \"experimental. Pass `steps_per_execution` instead of \" \"`experimental_steps_per_execution`.\" ) if not steps_per_execution : steps_per_execution = kwargs . pop ( \"experimental_steps_per_execution\" ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for # multi-output models, which have prefixes added for each # corresponding output name). from_serialized = kwargs . pop ( \"from_serialized\" , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) if isinstance ( loss , compile_utils . LossesContainer ) : self . compiled_loss = loss else : self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized , ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( \"You cannot enable `run_eagerly` and `jit_compile` \" \"at the same time.\" ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses ) compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model.call(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns # results for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) return self . get_metrics_result () compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 'auto' , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs,<br> targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). None batch_size None Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None sample_weight None Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . None steps None Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None **kwargs None Unused at this time. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = \"auto\" , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs , ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( \"evaluate\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"evaluate\" ) self . _assert_compile_was_called () self . _check_call_args ( \"evaluate\" ) self . _check_sample_weight_warning ( x , sample_weight ) _disallow_inside_tf_function ( \"evaluate\" ) use_cached_eval_dataset = kwargs . pop ( \"_use_cached_eval_dataset\" , False ) if kwargs : raise TypeError ( f \"Invalid keyword arguments: {list(kwargs.keys())}\" ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , \"_eval_data_handler\" , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch # iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( \"test\" , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.evaluate_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.evaluate`, which supports generators.\" , stacklevel = 2 , ) self . _check_call_args ( \"evaluate_generator\" ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include sample_weights as a third component, note that sample weighting applies to the weighted_metrics argument but not the metrics argument in compile() . If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. If both validation_data and validation_split are provided, validation_data will override validation_split . validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . Note that sample weighting does not apply to metrics specified via the metrics argument in compile() . To apply sample weighting to your metrics, you can specify them via the weighted_metrics in compile() instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = \"auto\" , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include `sample_weights` as a third component, note that sample weighting applies to the `weighted_metrics` argument but not the `metrics` argument in `compile()`. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. If both `validation_data` and `validation_split` are provided, `validation_data` will override `validation_split`. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. Note that sample weighting does not apply to metrics specified via the `metrics` argument in `compile()`. To apply sample weighting to your metrics, you can specify them via the `weighted_metrics` in `compile()` instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( \"fit\" ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( \"Model\" , \"fit\" ) self . _assert_compile_was_called () self . _check_call_args ( \"fit\" ) _disallow_inside_tf_function ( \"fit\" ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) if validation_split and validation_data is None : # Create the validation data using the training data. Only supported # for `Tensor` and `NumPy` input. ( x , y , sample_weight , ), validation_data = data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split ) if validation_data : ( val_x , val_y , val_sample_weight , ) = data_adapter . unpack_x_y_sample_weight ( validation_data ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. steps_per_epoch_inferred = ( steps_per_epoch or data_handler . inferred_steps ) ( data_handler . _initial_epoch , data_handler . _initial_step , ) = self . _maybe_load_initial_counters_from_ckpt ( steps_per_epoch_inferred , initial_epoch ) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( \"train\" , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 , ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( \"Unexpected result of `train_function` \" \"(Empty logs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , \"_eval_data_handler\" , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items () } epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break if ( isinstance ( self . optimizer , optimizer_experimental . Optimizer ) and epochs > 0 ): self . optimizer . finalize_variable_values ( self . trainable_variables ) # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , \"_eval_data_handler\" , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 , ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.fit_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.fit`, which supports generators.\" , stacklevel = 2 , ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch , ) get_config def get_config ( self ) Returns the config of the Model . Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a Model . This allows the Model to be be reinstantiated later (without its trained weights) from this configuration. Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed Model are advised to override this method, and continue to update the dict from super(MyModel, self).get_config() to provide the proper configuration of this Model . The default config is an empty dict. Optionally, raise NotImplementedError to allow Keras to attempt a default serialization. Returns: Type Description None Python dictionary containing the configuration of this Model . View Source def get_config ( self ) : \" \"\" Returns the config of the `Model`. Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a `Model`. This allows the `Model` to be be reinstantiated later (without its trained weights) from this configuration. Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed `Model` are advised to override this method, and continue to update the dict from `super(MyModel, self).get_config()` to provide the proper configuration of this `Model`. The default config is an empty dict. Optionally, raise `NotImplementedError` to allow Keras to attempt a default serialization. Returns: Python dictionary containing the configuration of this `Model`. \"\" \" # Return an empty dict here because otherwise Model # subclass developers may see # their model's `__init__()` fed with unexpected keyword arguments, # if their `__init__()` takes no argument for example, and they # don't override `from_config()`, which would use `cls(**config)` # as a result. config = {} if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ) : if self . _is_compiled and hasattr ( self , \"_compile_config\" ) : config [ \"compile_config\" ] = self . _compile_config . serialize () if self . built : config [ \"build_input_shape\" ] = self . _build_input_shape return config get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Type Description Default name None String, name of layer. None index None Integer, index of layer. None Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( \"Provide only a layer name or a layer index. Received: \" f \"index={index}, name={name}.\" ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f \"Was asked to retrieve layer at index {index}\" f \" but model only has {len(self.layers)}\" \" layers.\" ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f \"No such layer: {name}. Existing layers are: \" f \"{list(layer.name for layer in self.layers)}.\" ) raise ValueError ( \"Provide either a layer name or layer index at `get_layer`.\" ) get_metrics_result def get_metrics_result ( self ) Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: Type Description None A dict containing values of the metrics listed in self.metrics . Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def get_metrics_result ( self ) : \" \"\" Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: A `dict` containing values of the metrics listed in `self.metrics`. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weight_paths def get_weight_paths ( self ) Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a tf.Variable instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding tf.Variable instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: Type Description None A dict where keys are variable paths and values are tf.Variable instances. View Source def get_weight_paths ( self ) : \"\"\"Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a `tf.Variable` instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding `tf.Variable` instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: A dict where keys are variable paths and values are `tf.Variable` instances. Example: ```python class SubclassModel(tf.keras.Model): def __init__(self, name=None): super().__init__(name=name) self.d1 = tf.keras.layers.Dense(10) self.d2 = tf.keras.layers.Dense(20) def call(self, inputs): x = self.d1(inputs) return self.d2(x) model = SubclassModel() model(tf.zeros((10, 10))) weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': model.d1.kernel, # 'd1.bias': model.d1.bias, # 'd2.kernel': model.d2.kernel, # 'd2.bias': model.d2.bias, # } # Functional model inputs = tf.keras.Input((10,), batch_size=10) x = tf.keras.layers.Dense(20, name='d1')(inputs) output = tf.keras.layers.Dense(30, name='d2')(x) model = tf.keras.Model(inputs, output) d1 = model.layers[1] d2 = model.layers[2] weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': d1.kernel, # 'd1.bias': d1.bias, # 'd2.kernel': d2.kernel, # 'd2.bias': d2.bias, # } ``` \"\"\" result = {} ( descendants , object_paths_dict , ) = tf . __internal__ . tracking . ObjectGraphView ( self ). breadth_first_traversal () for descendant in descendants : if isinstance ( descendant , tf . Variable ) : trackable_references = object_paths_dict [ descendant ] object_path = \".\" . join ( [ t.name for t in trackable_references ] ) result [ object_path ] = descendant return result get_weights def get_weights ( self ) Retrieves the weights of the model. Returns: Type Description None A flat list of Numpy arrays. View Source def get_weights ( self ) : \"\" \"Retrieves the weights of the model. Returns: A flat list of Numpy arrays. \"\" \" with self.distribute_strategy.scope(): return super().get_weights() load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Type Description Default filepath None String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . None by_name None Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. None skip_mismatch None Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). None options None Optional tf.train.CheckpointOptions object that specifies options for loading weights. None Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ) ): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( \"Load weights is not implemented with TPUStrategy \" \"with `steps_per_run` greater than 1. The \" f \"`steps_per_run` is {spr}\" ) if skip_mismatch and not by_name : raise ValueError ( \"When calling model.load_weights, skip_mismatch can only be \" \"set to True when by_name is True.\" ) filepath , save_format = _detect_save_format ( filepath ) if save_format == \"tf\" : status = self . _checkpoint . read ( filepath , options ) if by_name : raise NotImplementedError ( \"Weights may only be loaded based on topology into Models \" \"when loading TensorFlow-formatted weights \" \"(got by_name=True to load_weights).\" ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( \"`load_weights` requires h5py package when loading weights \" \"from HDF5. Try installing h5py.\" ) if not self . _is_graph_network and not self . built : raise ValueError ( \"Unable to load weights saved in HDF5 format into a \" \"subclassed Model which has not created its variables yet. \" \"Call the Model first, then load the weights.\" ) self . _assert_weights_created () with h5py . File ( filepath , \"r\" ) as f : if \"layer_names\" not in f . attrs and \"model_weights\" in f : f = f [ \"model_weights\" ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the predict function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = \"concat\" ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( outputs , tf . nest . map_structure ( lambda t : tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape , outputs , ), ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , reduce_retracing = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the test function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) if self . _cluster_coordinator : self . test_function = ( lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it ,) ) ) else : self . test_function = test_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it , self . _steps_per_execution . value ()) ) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the train function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = ( lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it ,) ) ) else : self . train_function = train_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it , self . _steps_per_execution . value ()) ) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 'auto' , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Type Description Default x None Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None batch_size None Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None steps None Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = \"auto\" , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks]( https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( \"predict\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"predict\" ) self . _check_call_args ( \"predict\" ) _disallow_inside_tf_function ( \"predict\" ) # TODO(yashkatariya): Cache model on the coordinator for faster # prediction. If running under PSS, then swap it with OneDeviceStrategy # so that execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None verbose = _get_verbosity ( verbose , self . distribute_strategy ) outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy ) ) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = ( data_option ) x = x . with_options ( options ) except ValueError : warnings . warn ( \"Using Model.predict with MultiWorkerMirroredStrategy \" \"or TPUStrategy and AutoShardPolicy.FILE might lead to \" \"out-of-order result. Consider setting it to \" \"AutoShardPolicy.DATA.\" , stacklevel = 2 , ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = ( tmp_batch_outputs # No error, now safe to assign. ) if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs , ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs , ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { \"outputs\" : batch_outputs } ) if batch_outputs is None : raise ValueError ( \"Unexpected result of `predict_function` \" \"(Empty batch_outputs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , potentially_ragged_concat , outputs ) # If originally PSS strategy was used, then replace it back since # predict is running under `OneDeviceStrategy` after the swap and once # its done we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.predict_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.predict`, which supports generators.\" , stacklevel = 2 , ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\" \"Returns predictions for a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`. \"\" \" self . _check_call_args ( \"predict_on_batch\" ) _disallow_inside_tf_function ( \"predict_on_batch\" ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None The result of one inference step, typically the output of calling the Model on data. View Source def predict_step ( self , data ) : \" \"\" The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by `Model.make_predict_function`. This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_predict_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: The result of one inference step, typically the output of calling the `Model` on data. \"\" \" x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) return self ( x , training = False ) reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. View Source def reset_metrics ( self ) : \"\" \"Resets the state of all the metrics in the model. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model . compile ( optimizer = \"Adam\" , loss = \"mse\" , metrics = [ \"mae\" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\" \" for m in self.metrics: m.reset_state() reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , \"reset_states\" ) and getattr ( layer , \"stateful\" , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Parameters: Name Type Description Default filepath None String, PathLike, path to SavedModel or H5 file to save the model. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None include_optimizer None If True, save optimizer's state together. None save_format None Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. None signatures None Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. None options None (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. None save_traces None (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. None View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True , ) : \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide]( https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces , ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Type Description Default dynamic_batch None Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . None Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Type Description Default filepath None String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None save_format None Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. None options None Optional tf.train.CheckpointOptions object that specifies options for saving weights. None Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints]( https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = io_utils . path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = \"h5\" else : save_format = \"tf\" else : user_format = save_format . lower () . strip () if user_format in ( \"tensorflow\" , \"tf\" ): save_format = \"tf\" elif user_format in ( \"hdf5\" , \"h5\" , \"keras\" ): save_format = \"h5\" else : raise ValueError ( f \"Unknown format. Received: `save_format`={save_format}. \" 'Was expecting one of {\"tf\", \"h5\"}.' ) if save_format == \"tf\" and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f \"filepath ({filepath}) looks like an HDF5 file. \" 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == \"h5\" and h5py is None : raise ImportError ( \"`save_weights` requires h5py when saving in hdf5, but h5py is \" \"not available. Try installing h5py package.\" ) if save_format == \"tf\" : check_filepath = filepath + \".index\" else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = io_utils . ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == \"h5\" : with h5py . File ( filepath , \"w\" ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if not tf . executing_eagerly (): # Call `get_session` to initialize any uninitialized variables. backend . get_session () self . _checkpoint . write ( filepath , options = options ) # Record this checkpoint so it's visible from # tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ], ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state () summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None ) Prints a string summary of the network. Parameters: Name Type Description Default line_length None Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). None positions None Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . None print_fn None Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. print expand_nested None Whether to expand the nested models. If not provided, defaults to False . None show_trainable None Whether to show if a layer is trainable. If not provided, defaults to False . None layer_range None a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to layer_range[0] and the end predicate will be the last element it matches to layer_range[1] . By default None which considers all layers of model. None Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None , ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. layer_range: a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to `layer_range[0]` and the end predicate will be the last element it matches to `layer_range[1]`. By default `None` which considers all layers of model. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( \"This model has not yet been built. \" \"Build the model first by calling `build()` or by calling \" \"the model on a batch of data.\" ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable , layer_range = layer_range , ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"test_on_batch\" ) _disallow_inside_tf_function ( \"test_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. View Source def test_step ( self , data ) : \" \"\" The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by `Model.make_test_function`. This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_test_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) # Updates stateful loss metrics. self . compute_loss ( x , y , y_pred , sample_weight ) return self . compute_metrics ( x , y , y_pred , sample_weight ) to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to * json.dumps() . None Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to *`json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to yaml.dump() . None Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( \"Method `model.to_yaml()` has been removed due to security risk of \" \"arbitrary code execution. Please use `model.to_json()` instead.\" ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight None Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"train_on_batch\" ) _disallow_inside_tf_function ( \"train_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see Customizing what happens in fit . This method is called by Model.make_train_function . This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def train_step ( self , data ) : \" \"\" The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see [Customizing what happens in fit]( https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This method is called by `Model.make_train_function`. This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_train_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) # Run forward pass. with tf . GradientTape () as tape : y_pred = self ( x , training = True ) loss = self . compute_loss ( x , y , y_pred , sample_weight ) self . _validate_target_and_loss ( y , loss ) # Run backwards pass. self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return self . compute_metrics ( x , y , y_pred , sample_weight )","title":"Layers"},{"location":"reference/grid_transformer/layers/#module-grid_transformerlayers","text":"The module contains two classes, Conversion and Vec . Conversion class is a model that converts input sequence of tokens to a sequence of tokens desired size and desired number of tokens. It can be initialized with a desired size and maximum token number. The build method is used to set the token_max and mul attributes based on the input_shape, and the call method performs the conversion by reshaping the inputs tensor. Vec class is a model that applies a given model on each element of the input tensor. It is initialized with a model, and the call method applies the model on each element of the input tensor by first transposing the input tensor and then using tf.vectorized_map to apply the model on each element, and then transposing the output tensor. Example usage: # create an instance of Conversion model conversion_model = Conversion ( size = 5 , max_ = 20 ) conversion_model . build (( None , 30 , 100 )) # create an instance of Vec model vec_model = Vec ( tf . keras . layers . Dense ( units = 64 )) # input tensor x = tf . random . normal (( 10 , 30 , 100 )) # apply conversion model on input tensor converted_x = conversion_model ( x ) # apply vec model on input tensor vec_x = vec_model ( x ) View Source \" \"\" The module contains two classes, `Conversion` and `Vec`. `Conversion` class is a model that converts input sequence of tokens to a sequence of tokens desired size and desired number of tokens. It can be initialized with a desired size and maximum token number. The build method is used to set the token_max and mul attributes based on the input_shape, and the call method performs the conversion by reshaping the inputs tensor. `Vec` class is a model that applies a given model on each element of the input tensor. It is initialized with a model, and the call method applies the model on each element of the input tensor by first transposing the input tensor and then using tf.vectorized_map to apply the model on each element, and then transposing the output tensor. Example usage: ```python # create an instance of Conversion model conversion_model = Conversion(size=5, max_=20) conversion_model.build((None, 30, 100)) # create an instance of Vec model vec_model = Vec(tf.keras.layers.Dense(units=64)) # input tensor x = tf.random.normal((10,30,100)) # apply conversion model on input tensor converted_x = conversion_model(x) # apply vec model on input tensor vec_x = vec_model(x) ``` \"\" \" from typing import Tuple , Optional import tensorflow as tf import math from loguru import logger from tensorflow . keras import Model class Conversion ( Model ) : \" \"\" A model for converting input sequence of tokens to a sequence of tokens desired size and desired number of tokens.. \"\" \" def __init__ ( self , size : int = 9 , max_ : Optional [ int ] = None ) : \" \"\" Initialize the class with desired size and maximum token number. :param size: The desired size of the output tokens. :param max_: The maximum number of tokens in the output shape. \"\" \" super (). __init__ () self . token_max = max_ # number off transformer output token that will be used to create model output self . size = size # number of outputs self . mul = 1 def build ( self , input_shape : Tuple [ int , int , int ] ) -> None : \" \"\" Build the model. :param input_shape: The input shape of the model. \"\" \" if self . token_max is None : ratio = input_shape [ 1 ] / self . size if ratio < 1 : self . mul = math . ceil ( 1 / ratio ) self . token_max = self . size if input_shape [ 2 ] % self . mul != 0 : # todo high rise error logger . error ( f \"To create {self.size} from {input_shape[1]} tokens the size of transformer {input_shape[2]} need to divisible by {self.mul}.\" ) else : self . token_max = math . floor ( ratio ) * self . size def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \" \"\" Perform the conversion. :param inputs: Input tensor of shape (batch_size, tokens, features). :return: Tensor of shape (batch_size, size, features * (token_max / size)). \"\" \" def call ( self , inputs ) : shape = tf . shape ( inputs ) if self . mul > 1 : inputs = tf . reshape ( inputs , ( shape [ 0 ] , int ( inputs . shape [ 1 ] * self . mul ), int ( inputs . shape [ 2 ] / self . mul ))) shape = tf . shape ( inputs ) return tf . reshape ( inputs [ : , : self . token_max ] , tf . stack ( [ shape [ 0 ] , self . size , int ( inputs . shape [ - 1 ] * ( self . token_max / self . size )) ] )) class Vec ( tf . keras . Model ) : def __init__ ( self , model : tf . keras . Model ) : \" \"\" Class that applies a given model on each element of the input tensor. Parameters: model (tf.keras.Model): The model to apply on each element of the input tensor. \"\" \" super (). __init__ () self . model = model def call ( self , x : tf . Tensor ) -> tf . Tensor : \" \"\" Applies the model on each element of the input tensor. Parameters: x (tf.Tensor): The input tensor Returns: tf.Tensor: The output tensor with the model applied on each element. \"\" \" x = tf . transpose ( x , perm = ( 1 , 0 , 2 , 3 , 4 )) x = tf . vectorized_map ( self . model , x ) return tf . transpose ( x , perm = ( 1 , 0 , 2 , 3 , 4 ))","title":"Module grid_transformer.layers"},{"location":"reference/grid_transformer/layers/#classes","text":"","title":"Classes"},{"location":"reference/grid_transformer/layers/#conversion","text":"class Conversion ( size : int = 9 , max_ : Optional [ int ] = None ) A model for converting input sequence of tokens to a sequence of tokens desired size and desired number of tokens.. View Source class Conversion ( Model ) : \"\"\" A model for converting input sequence of tokens to a sequence of tokens desired size and desired number of tokens.. \"\"\" def __init__ ( self , size : int = 9 , max_ : Optional [ int ] = None ) : \"\"\" Initialize the class with desired size and maximum token number. :param size: The desired size of the output tokens. :param max_: The maximum number of tokens in the output shape. \"\"\" super (). __init__ () self . token_max = max_ # number off transformer output token that will be used to create model output self . size = size # number of outputs self . mul = 1 def build ( self , input_shape : Tuple [ int, int, int ] ) -> None : \"\"\" Build the model. :param input_shape: The input shape of the model. \"\"\" if self . token_max is None : ratio = input_shape [ 1 ] / self . size if ratio < 1 : self . mul = math . ceil ( 1 / ratio ) self . token_max = self . size if input_shape [ 2 ] % self . mul != 0 : # todo high rise error logger . error ( f \"To create {self.size} from {input_shape[1]} tokens the size of transformer {input_shape[2]} need to divisible by {self.mul}.\" ) else : self . token_max = math . floor ( ratio ) * self . size def call ( self , inputs : tf . Tensor ) -> tf . Tensor : \"\"\" Perform the conversion. :param inputs: Input tensor of shape (batch_size, tokens, features). :return: Tensor of shape (batch_size, size, features * (token_max / size)). \"\"\" def call ( self , inputs ) : shape = tf . shape ( inputs ) if self . mul > 1 : inputs = tf . reshape ( inputs , ( shape [ 0 ] , int ( inputs . shape [ 1 ] * self . mul ), int ( inputs . shape [ 2 ] / self . mul ))) shape = tf . shape ( inputs ) return tf . reshape ( inputs [ :, :self.token_max ] , tf . stack ( [ shape[0 ] , self . size , int ( inputs . shape [ -1 ] * ( self . token_max / self . size )) ] ))","title":"Conversion"},{"location":"reference/grid_transformer/layers/#ancestors-in-mro","text":"keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/layers/#static-methods","text":"","title":"Static methods"},{"location":"reference/grid_transformer/layers/#from_config","text":"def from_config ( config , custom_objects = None ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config , custom_objects = None ): compile_config = config . pop ( \"compile_config\" , None ) build_input_shape = config . pop ( \"build_input_shape\" , None ) # `from_config` assumes `cls` is either `Functional` or a child class of # `Functional`. In the case that `cls` is meant to behave like a child # class of `Functional` but only inherits from the `Model` class, we # have to call `cls(...)` instead of `Functional.from_config`. from keras.engine import functional with serialization . SharedObjectLoadingScope (): functional_model_keys = [ \"name\" , \"layers\" , \"input_layers\" , \"output_layers\" , ] if all ( key in config for key in functional_model_keys ): inputs , outputs , layers = functional . reconstruct_from_config ( config , custom_objects ) model = cls ( inputs = inputs , outputs = outputs , name = config . get ( \"name\" ) ) functional . connect_ancillary_layers ( model , layers ) else : # The config does not contain all the information necessary to # revive a Functional model. This happens when the user creates # subclassed models where `get_config()` is returning # insufficient information to be considered a Functional model. # In this case, we fall back to provide all config into the # constructor of the class. try : model = cls ( ** config ) except TypeError as e : raise TypeError ( \"Unable to revive model from config. When overriding \" \"the `get_config()`, make sure that the returned \" \"config contains all items used as arguments in the \" f \"constructor to { cls } , which is the default behavior. \" \"You can override this default behavior by defining a \" \"`from_config` method to specify how to create an \" f \"instance of { cls . __name__ } from the config. \\n\\n \" f \"Error encountered during deserialization: \\n { e } \" ) if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ): if build_input_shape : model . build ( build_input_shape ) if compile_config is not None : model . _compile_from_config ( compile_config , base_class = Model ) return model","title":"from_config"},{"location":"reference/grid_transformer/layers/#with_name_scope","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/layers/#instance-variables","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. distribute_reduction_method The method employed to reduce per-replica values during training. Unless specified, the value \"auto\" will be assumed, indicating that the reduction strategy should be chosen based on the current running environment. See reduce_per_replica function for more details. distribute_strategy The tf.distribute.Strategy this model was created under. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . layers losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics Returns the model's metrics added using compile() , add_metric() APIs. Note: Metrics passed to compile() are available only after a keras.Model has been trained/evaluated on actual data. metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. state_updates Deprecated, do NOT use! Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers.","title":"Instance variables"},{"location":"reference/grid_transformer/layers/#methods","text":"","title":"Methods"},{"location":"reference/grid_transformer/layers/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/layers/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/layers/#add_update","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/layers/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/layers/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/layers/#build","text":"def build ( self , input_shape : Tuple [ int , int , int ] ) -> None Build the model. Parameters: Name Type Description Default input_shape None The input shape of the model. None View Source def build ( self , input_shape : Tuple [ int , int , int ]) -> None : \"\"\" Build the model. :param input_shape: The input shape of the model. \"\"\" if self . token_max is None : ratio = input_shape [ 1 ] / self . size if ratio < 1 : self . mul = math . ceil ( 1 / ratio ) self . token_max = self . size if input_shape [ 2 ] % self . mul != 0 : # todo high rise error logger . error ( f \"To create {self.size} from {input_shape[1]} tokens the size of transformer {input_shape[2]} need to divisible by {self.mul}.\" ) else : self . token_max = math . floor ( ratio ) * self . size","title":"build"},{"location":"reference/grid_transformer/layers/#call","text":"def call ( self , inputs ) Calls the model on new inputs and returns the outputs as tensors. In this case call() just reapplies all ops in the graph to the new inputs (e.g. build a new computational graph from the provided inputs). Note: This method should not be called directly. It is only meant to be overridden when subclassing tf.keras.Model . To call a model on an input, always use the __call__() method, i.e. model(inputs) , which relies on the underlying call() method. Parameters: Name Type Description Default inputs None Input tensor, or dict/list/tuple of input tensors. None training None Boolean or boolean scalar tensor, indicating whether to run the Network in training mode or inference mode. None mask None A mask or list of masks. A mask can be either a boolean tensor or None (no mask). For more details, check the guide here . None Returns: Type Description None A tensor if there is a single output, or a list of tensors if there are more than one outputs. View Source def call ( self , inputs ) : shape = tf . shape ( inputs ) if self . mul > 1 : inputs = tf . reshape ( inputs , ( shape [ 0 ], int ( inputs . shape [ 1 ] * self . mul ) , int ( inputs . shape [ 2 ] / self . mul ))) shape = tf . shape ( inputs ) return tf . reshape ( inputs [:, : self . token_max ], tf . stack ( [ shape [ 0 ], self . size , int ( inputs . shape [ - 1 ] * ( self . token_max / self . size )) ] ))","title":"call"},{"location":"reference/grid_transformer/layers/#compile","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Parameters: Name Type Description Default optimizer None String (name of optimizer) or optimizer instance. See tf.keras.optimizers . None loss None Loss function. May be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true,<br>y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. None metrics None List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true,<br>y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the shapes of the targets and of the model output. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. The metrics passed here are evaluated without sample weighting; if you would like sample weighting to apply, you can specify your metrics via the weighted_metrics argument instead. None loss_weights None Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. None weighted_metrics None List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. None run_eagerly None Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . False steps_per_execution None Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). 1 jit_compile None If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. None **kwargs None Arguments supported for backwards compatibility only. None View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = \"rmsprop\" , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs , ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . May be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = {' output_a ':' accuracy ' , ' output_b ' :[ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the shapes of the targets and of the model output . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . The metrics passed here are evaluated without sample weighting ; if you would like sample weighting to apply , you can specify your metrics via the ` weighted_metrics ` argument instead . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( \"compile\" ). set ( True ) self . _compile_config = generic_utils . Config ( optimizer = optimizer , loss = loss , metrics = metrics , loss_weights = loss_weights , weighted_metrics = weighted_metrics , run_eagerly = run_eagerly , steps_per_execution = steps_per_execution , jit_compile = jit_compile , ) with self . distribute_strategy . scope () : if \"experimental_steps_per_execution\" in kwargs : logging . warning ( \"The argument `steps_per_execution` is no longer \" \"experimental. Pass `steps_per_execution` instead of \" \"`experimental_steps_per_execution`.\" ) if not steps_per_execution : steps_per_execution = kwargs . pop ( \"experimental_steps_per_execution\" ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for # multi-output models, which have prefixes added for each # corresponding output name). from_serialized = kwargs . pop ( \"from_serialized\" , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) if isinstance ( loss , compile_utils . LossesContainer ) : self . compiled_loss = loss else : self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized , ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( \"You cannot enable `run_eagerly` and `jit_compile` \" \"at the same time.\" ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/grid_transformer/layers/#compute_loss","text":"def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses )","title":"compute_loss"},{"location":"reference/grid_transformer/layers/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/layers/#compute_metrics","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model.call(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns # results for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) return self . get_metrics_result ()","title":"compute_metrics"},{"location":"reference/grid_transformer/layers/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/layers/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/layers/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/layers/#evaluate","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 'auto' , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs,<br> targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). None batch_size None Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None sample_weight None Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . None steps None Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None **kwargs None Unused at this time. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = \"auto\" , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs , ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( \"evaluate\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"evaluate\" ) self . _assert_compile_was_called () self . _check_call_args ( \"evaluate\" ) self . _check_sample_weight_warning ( x , sample_weight ) _disallow_inside_tf_function ( \"evaluate\" ) use_cached_eval_dataset = kwargs . pop ( \"_use_cached_eval_dataset\" , False ) if kwargs : raise TypeError ( f \"Invalid keyword arguments: {list(kwargs.keys())}\" ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , \"_eval_data_handler\" , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch # iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( \"test\" , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/grid_transformer/layers/#evaluate_generator","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.evaluate_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.evaluate`, which supports generators.\" , stacklevel = 2 , ) self . _check_call_args ( \"evaluate_generator\" ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , )","title":"evaluate_generator"},{"location":"reference/grid_transformer/layers/#finalize_state","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/layers/#fit","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include sample_weights as a third component, note that sample weighting applies to the weighted_metrics argument but not the metrics argument in compile() . If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. If both validation_data and validation_split are provided, validation_data will override validation_split . validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . Note that sample weighting does not apply to metrics specified via the metrics argument in compile() . To apply sample weighting to your metrics, you can specify them via the weighted_metrics in compile() instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = \"auto\" , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include `sample_weights` as a third component, note that sample weighting applies to the `weighted_metrics` argument but not the `metrics` argument in `compile()`. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. If both `validation_data` and `validation_split` are provided, `validation_data` will override `validation_split`. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. Note that sample weighting does not apply to metrics specified via the `metrics` argument in `compile()`. To apply sample weighting to your metrics, you can specify them via the `weighted_metrics` in `compile()` instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( \"fit\" ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( \"Model\" , \"fit\" ) self . _assert_compile_was_called () self . _check_call_args ( \"fit\" ) _disallow_inside_tf_function ( \"fit\" ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) if validation_split and validation_data is None : # Create the validation data using the training data. Only supported # for `Tensor` and `NumPy` input. ( x , y , sample_weight , ), validation_data = data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split ) if validation_data : ( val_x , val_y , val_sample_weight , ) = data_adapter . unpack_x_y_sample_weight ( validation_data ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. steps_per_epoch_inferred = ( steps_per_epoch or data_handler . inferred_steps ) ( data_handler . _initial_epoch , data_handler . _initial_step , ) = self . _maybe_load_initial_counters_from_ckpt ( steps_per_epoch_inferred , initial_epoch ) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( \"train\" , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 , ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( \"Unexpected result of `train_function` \" \"(Empty logs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , \"_eval_data_handler\" , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items () } epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break if ( isinstance ( self . optimizer , optimizer_experimental . Optimizer ) and epochs > 0 ): self . optimizer . finalize_variable_values ( self . trainable_variables ) # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , \"_eval_data_handler\" , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/grid_transformer/layers/#fit_generator","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 , ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.fit_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.fit`, which supports generators.\" , stacklevel = 2 , ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch , )","title":"fit_generator"},{"location":"reference/grid_transformer/layers/#get_config","text":"def get_config ( self ) Returns the config of the Model . Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a Model . This allows the Model to be be reinstantiated later (without its trained weights) from this configuration. Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed Model are advised to override this method, and continue to update the dict from super(MyModel, self).get_config() to provide the proper configuration of this Model . The default config is an empty dict. Optionally, raise NotImplementedError to allow Keras to attempt a default serialization. Returns: Type Description None Python dictionary containing the configuration of this Model . View Source def get_config ( self ) : \" \"\" Returns the config of the `Model`. Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a `Model`. This allows the `Model` to be be reinstantiated later (without its trained weights) from this configuration. Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed `Model` are advised to override this method, and continue to update the dict from `super(MyModel, self).get_config()` to provide the proper configuration of this `Model`. The default config is an empty dict. Optionally, raise `NotImplementedError` to allow Keras to attempt a default serialization. Returns: Python dictionary containing the configuration of this `Model`. \"\" \" # Return an empty dict here because otherwise Model # subclass developers may see # their model's `__init__()` fed with unexpected keyword arguments, # if their `__init__()` takes no argument for example, and they # don't override `from_config()`, which would use `cls(**config)` # as a result. config = {} if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ) : if self . _is_compiled and hasattr ( self , \"_compile_config\" ) : config [ \"compile_config\" ] = self . _compile_config . serialize () if self . built : config [ \"build_input_shape\" ] = self . _build_input_shape return config","title":"get_config"},{"location":"reference/grid_transformer/layers/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/layers/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/layers/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/layers/#get_layer","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Type Description Default name None String, name of layer. None index None Integer, index of layer. None Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( \"Provide only a layer name or a layer index. Received: \" f \"index={index}, name={name}.\" ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f \"Was asked to retrieve layer at index {index}\" f \" but model only has {len(self.layers)}\" \" layers.\" ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f \"No such layer: {name}. Existing layers are: \" f \"{list(layer.name for layer in self.layers)}.\" ) raise ValueError ( \"Provide either a layer name or layer index at `get_layer`.\" )","title":"get_layer"},{"location":"reference/grid_transformer/layers/#get_metrics_result","text":"def get_metrics_result ( self ) Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: Type Description None A dict containing values of the metrics listed in self.metrics . Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def get_metrics_result ( self ) : \" \"\" Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: A `dict` containing values of the metrics listed in `self.metrics`. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"get_metrics_result"},{"location":"reference/grid_transformer/layers/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/layers/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/layers/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/layers/#get_weight_paths","text":"def get_weight_paths ( self ) Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a tf.Variable instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding tf.Variable instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: Type Description None A dict where keys are variable paths and values are tf.Variable instances. View Source def get_weight_paths ( self ) : \"\"\"Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a `tf.Variable` instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding `tf.Variable` instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: A dict where keys are variable paths and values are `tf.Variable` instances. Example: ```python class SubclassModel(tf.keras.Model): def __init__(self, name=None): super().__init__(name=name) self.d1 = tf.keras.layers.Dense(10) self.d2 = tf.keras.layers.Dense(20) def call(self, inputs): x = self.d1(inputs) return self.d2(x) model = SubclassModel() model(tf.zeros((10, 10))) weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': model.d1.kernel, # 'd1.bias': model.d1.bias, # 'd2.kernel': model.d2.kernel, # 'd2.bias': model.d2.bias, # } # Functional model inputs = tf.keras.Input((10,), batch_size=10) x = tf.keras.layers.Dense(20, name='d1')(inputs) output = tf.keras.layers.Dense(30, name='d2')(x) model = tf.keras.Model(inputs, output) d1 = model.layers[1] d2 = model.layers[2] weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': d1.kernel, # 'd1.bias': d1.bias, # 'd2.kernel': d2.kernel, # 'd2.bias': d2.bias, # } ``` \"\"\" result = {} ( descendants , object_paths_dict , ) = tf . __internal__ . tracking . ObjectGraphView ( self ). breadth_first_traversal () for descendant in descendants : if isinstance ( descendant , tf . Variable ) : trackable_references = object_paths_dict [ descendant ] object_path = \".\" . join ( [ t.name for t in trackable_references ] ) result [ object_path ] = descendant return result","title":"get_weight_paths"},{"location":"reference/grid_transformer/layers/#get_weights","text":"def get_weights ( self ) Retrieves the weights of the model. Returns: Type Description None A flat list of Numpy arrays. View Source def get_weights ( self ) : \"\" \"Retrieves the weights of the model. Returns: A flat list of Numpy arrays. \"\" \" with self.distribute_strategy.scope(): return super().get_weights()","title":"get_weights"},{"location":"reference/grid_transformer/layers/#load_weights","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Type Description Default filepath None String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . None by_name None Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. None skip_mismatch None Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). None options None Optional tf.train.CheckpointOptions object that specifies options for loading weights. None Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ) ): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( \"Load weights is not implemented with TPUStrategy \" \"with `steps_per_run` greater than 1. The \" f \"`steps_per_run` is {spr}\" ) if skip_mismatch and not by_name : raise ValueError ( \"When calling model.load_weights, skip_mismatch can only be \" \"set to True when by_name is True.\" ) filepath , save_format = _detect_save_format ( filepath ) if save_format == \"tf\" : status = self . _checkpoint . read ( filepath , options ) if by_name : raise NotImplementedError ( \"Weights may only be loaded based on topology into Models \" \"when loading TensorFlow-formatted weights \" \"(got by_name=True to load_weights).\" ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( \"`load_weights` requires h5py package when loading weights \" \"from HDF5. Try installing h5py.\" ) if not self . _is_graph_network and not self . built : raise ValueError ( \"Unable to load weights saved in HDF5 format into a \" \"subclassed Model which has not created its variables yet. \" \"Call the Model first, then load the weights.\" ) self . _assert_weights_created () with h5py . File ( filepath , \"r\" ) as f : if \"layer_names\" not in f . attrs and \"model_weights\" in f : f = f [ \"model_weights\" ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/grid_transformer/layers/#make_predict_function","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the predict function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = \"concat\" ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( outputs , tf . nest . map_structure ( lambda t : tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape , outputs , ), ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , reduce_retracing = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/grid_transformer/layers/#make_test_function","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the test function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) if self . _cluster_coordinator : self . test_function = ( lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it ,) ) ) else : self . test_function = test_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it , self . _steps_per_execution . value ()) ) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/grid_transformer/layers/#make_train_function","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the train function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = ( lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it ,) ) ) else : self . train_function = train_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it , self . _steps_per_execution . value ()) ) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/grid_transformer/layers/#predict","text":"def predict ( self , x , batch_size = None , verbose = 'auto' , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Type Description Default x None Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None batch_size None Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None steps None Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = \"auto\" , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks]( https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( \"predict\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"predict\" ) self . _check_call_args ( \"predict\" ) _disallow_inside_tf_function ( \"predict\" ) # TODO(yashkatariya): Cache model on the coordinator for faster # prediction. If running under PSS, then swap it with OneDeviceStrategy # so that execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None verbose = _get_verbosity ( verbose , self . distribute_strategy ) outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy ) ) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = ( data_option ) x = x . with_options ( options ) except ValueError : warnings . warn ( \"Using Model.predict with MultiWorkerMirroredStrategy \" \"or TPUStrategy and AutoShardPolicy.FILE might lead to \" \"out-of-order result. Consider setting it to \" \"AutoShardPolicy.DATA.\" , stacklevel = 2 , ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = ( tmp_batch_outputs # No error, now safe to assign. ) if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs , ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs , ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { \"outputs\" : batch_outputs } ) if batch_outputs is None : raise ValueError ( \"Unexpected result of `predict_function` \" \"(Empty batch_outputs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , potentially_ragged_concat , outputs ) # If originally PSS strategy was used, then replace it back since # predict is running under `OneDeviceStrategy` after the swap and once # its done we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/grid_transformer/layers/#predict_generator","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.predict_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.predict`, which supports generators.\" , stacklevel = 2 , ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , )","title":"predict_generator"},{"location":"reference/grid_transformer/layers/#predict_on_batch","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\" \"Returns predictions for a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`. \"\" \" self . _check_call_args ( \"predict_on_batch\" ) _disallow_inside_tf_function ( \"predict_on_batch\" ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/grid_transformer/layers/#predict_step","text":"def predict_step ( self , data ) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None The result of one inference step, typically the output of calling the Model on data. View Source def predict_step ( self , data ) : \" \"\" The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by `Model.make_predict_function`. This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_predict_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: The result of one inference step, typically the output of calling the `Model` on data. \"\" \" x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) return self ( x , training = False )","title":"predict_step"},{"location":"reference/grid_transformer/layers/#reset_metrics","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. View Source def reset_metrics ( self ) : \"\" \"Resets the state of all the metrics in the model. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model . compile ( optimizer = \"Adam\" , loss = \"mse\" , metrics = [ \"mae\" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\" \" for m in self.metrics: m.reset_state()","title":"reset_metrics"},{"location":"reference/grid_transformer/layers/#reset_states","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , \"reset_states\" ) and getattr ( layer , \"stateful\" , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/grid_transformer/layers/#save","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Parameters: Name Type Description Default filepath None String, PathLike, path to SavedModel or H5 file to save the model. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None include_optimizer None If True, save optimizer's state together. None save_format None Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. None signatures None Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. None options None (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. None save_traces None (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. None View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True , ) : \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide]( https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces , )","title":"save"},{"location":"reference/grid_transformer/layers/#save_spec","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Type Description Default dynamic_batch None Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . None Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/grid_transformer/layers/#save_weights","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Type Description Default filepath None String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None save_format None Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. None options None Optional tf.train.CheckpointOptions object that specifies options for saving weights. None Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints]( https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = io_utils . path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = \"h5\" else : save_format = \"tf\" else : user_format = save_format . lower () . strip () if user_format in ( \"tensorflow\" , \"tf\" ): save_format = \"tf\" elif user_format in ( \"hdf5\" , \"h5\" , \"keras\" ): save_format = \"h5\" else : raise ValueError ( f \"Unknown format. Received: `save_format`={save_format}. \" 'Was expecting one of {\"tf\", \"h5\"}.' ) if save_format == \"tf\" and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f \"filepath ({filepath}) looks like an HDF5 file. \" 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == \"h5\" and h5py is None : raise ImportError ( \"`save_weights` requires h5py when saving in hdf5, but h5py is \" \"not available. Try installing h5py package.\" ) if save_format == \"tf\" : check_filepath = filepath + \".index\" else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = io_utils . ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == \"h5\" : with h5py . File ( filepath , \"w\" ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if not tf . executing_eagerly (): # Call `get_session` to initialize any uninitialized variables. backend . get_session () self . _checkpoint . write ( filepath , options = options ) # Record this checkpoint so it's visible from # tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ], )","title":"save_weights"},{"location":"reference/grid_transformer/layers/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/layers/#summary","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None ) Prints a string summary of the network. Parameters: Name Type Description Default line_length None Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). None positions None Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . None print_fn None Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. print expand_nested None Whether to expand the nested models. If not provided, defaults to False . None show_trainable None Whether to show if a layer is trainable. If not provided, defaults to False . None layer_range None a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to layer_range[0] and the end predicate will be the last element it matches to layer_range[1] . By default None which considers all layers of model. None Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None , ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. layer_range: a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to `layer_range[0]` and the end predicate will be the last element it matches to `layer_range[1]`. By default `None` which considers all layers of model. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( \"This model has not yet been built. \" \"Build the model first by calling `build()` or by calling \" \"the model on a batch of data.\" ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable , layer_range = layer_range , )","title":"summary"},{"location":"reference/grid_transformer/layers/#test_on_batch","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"test_on_batch\" ) _disallow_inside_tf_function ( \"test_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/grid_transformer/layers/#test_step","text":"def test_step ( self , data ) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. View Source def test_step ( self , data ) : \" \"\" The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by `Model.make_test_function`. This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_test_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) # Updates stateful loss metrics. self . compute_loss ( x , y , y_pred , sample_weight ) return self . compute_metrics ( x , y , y_pred , sample_weight )","title":"test_step"},{"location":"reference/grid_transformer/layers/#to_json","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to * json.dumps() . None Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to *`json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/grid_transformer/layers/#to_yaml","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to yaml.dump() . None Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( \"Method `model.to_yaml()` has been removed due to security risk of \" \"arbitrary code execution. Please use `model.to_json()` instead.\" )","title":"to_yaml"},{"location":"reference/grid_transformer/layers/#train_on_batch","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight None Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"train_on_batch\" ) _disallow_inside_tf_function ( \"train_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/grid_transformer/layers/#train_step","text":"def train_step ( self , data ) The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see Customizing what happens in fit . This method is called by Model.make_train_function . This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def train_step ( self , data ) : \" \"\" The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see [Customizing what happens in fit]( https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This method is called by `Model.make_train_function`. This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_train_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) # Run forward pass. with tf . GradientTape () as tape : y_pred = self ( x , training = True ) loss = self . compute_loss ( x , y , y_pred , sample_weight ) self . _validate_target_and_loss ( y , loss ) # Run backwards pass. self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return self . compute_metrics ( x , y , y_pred , sample_weight )","title":"train_step"},{"location":"reference/grid_transformer/layers/#vec","text":"class Vec ( model : keras . engine . training . Model ) Model groups layers into an object with training and inference features.","title":"Vec"},{"location":"reference/grid_transformer/layers/#attributes","text":"Name Type Description Default inputs None The input(s) of the model: a keras.Input object or a combination of keras.Input objects in a dict, list or tuple. None outputs None The output(s) of the model: a tensor that originated from keras.Input objects or a combination of such tensors in a dict, list or tuple. See Functional API example below. None name None String, the name of the model. None View Source class Vec ( tf . keras . Model ): def __init__ ( self , model: tf . keras . Model ): \"\"\" Class that applies a given model on each element of the input tensor. Parameters: model (tf.keras.Model): The model to apply on each element of the input tensor. \"\"\" super (). __init__ () self . model = model def call ( self , x : tf . Tensor ) -> tf . Tensor: \"\"\" Applies the model on each element of the input tensor. Parameters: x (tf.Tensor): The input tensor Returns: tf.Tensor: The output tensor with the model applied on each element. \"\"\" x = tf . transpose ( x , perm =( 1 , 0 , 2 , 3 , 4 )) x = tf . vectorized_map ( self . model , x ) return tf . transpose ( x , perm =( 1 , 0 , 2 , 3 , 4 ))","title":"Attributes"},{"location":"reference/grid_transformer/layers/#ancestors-in-mro_1","text":"keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/layers/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/grid_transformer/layers/#from_config_1","text":"def from_config ( config , custom_objects = None ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config , custom_objects = None ): compile_config = config . pop ( \"compile_config\" , None ) build_input_shape = config . pop ( \"build_input_shape\" , None ) # `from_config` assumes `cls` is either `Functional` or a child class of # `Functional`. In the case that `cls` is meant to behave like a child # class of `Functional` but only inherits from the `Model` class, we # have to call `cls(...)` instead of `Functional.from_config`. from keras.engine import functional with serialization . SharedObjectLoadingScope (): functional_model_keys = [ \"name\" , \"layers\" , \"input_layers\" , \"output_layers\" , ] if all ( key in config for key in functional_model_keys ): inputs , outputs , layers = functional . reconstruct_from_config ( config , custom_objects ) model = cls ( inputs = inputs , outputs = outputs , name = config . get ( \"name\" ) ) functional . connect_ancillary_layers ( model , layers ) else : # The config does not contain all the information necessary to # revive a Functional model. This happens when the user creates # subclassed models where `get_config()` is returning # insufficient information to be considered a Functional model. # In this case, we fall back to provide all config into the # constructor of the class. try : model = cls ( ** config ) except TypeError as e : raise TypeError ( \"Unable to revive model from config. When overriding \" \"the `get_config()`, make sure that the returned \" \"config contains all items used as arguments in the \" f \"constructor to { cls } , which is the default behavior. \" \"You can override this default behavior by defining a \" \"`from_config` method to specify how to create an \" f \"instance of { cls . __name__ } from the config. \\n\\n \" f \"Error encountered during deserialization: \\n { e } \" ) if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ): if build_input_shape : model . build ( build_input_shape ) if compile_config is not None : model . _compile_from_config ( compile_config , base_class = Model ) return model","title":"from_config"},{"location":"reference/grid_transformer/layers/#with_name_scope_1","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/layers/#instance-variables_1","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. distribute_reduction_method The method employed to reduce per-replica values during training. Unless specified, the value \"auto\" will be assumed, indicating that the reduction strategy should be chosen based on the current running environment. See reduce_per_replica function for more details. distribute_strategy The tf.distribute.Strategy this model was created under. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . layers losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics Returns the model's metrics added using compile() , add_metric() APIs. Note: Metrics passed to compile() are available only after a keras.Model has been trained/evaluated on actual data. metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. state_updates Deprecated, do NOT use! Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers.","title":"Instance variables"},{"location":"reference/grid_transformer/layers/#methods_1","text":"","title":"Methods"},{"location":"reference/grid_transformer/layers/#add_loss_1","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/layers/#add_metric_1","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/layers/#add_update_1","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/layers/#add_variable_1","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/layers/#add_weight_1","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/layers/#build_1","text":"def build ( self , input_shape ) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape instance, or list/dict of shapes, where shapes are tuples, integers, or TensorShape instances. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, TensorShape , or dict). 2. If the model requires call arguments that are agnostic to the input shapes (positional or keyword arg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. View Source @generic_utils.default def build ( self , input_shape ) : \" \"\" Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call `model.build()` in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, `TensorShape` instance, or list/dict of shapes, where shapes are tuples, integers, or `TensorShape` instances. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, `TensorShape`, or dict). 2. If the model requires call arguments that are agnostic to the input shapes (positional or keyword arg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. \"\" \" if self . _is_graph_network : super (). build ( input_shape ) return if input_shape is None : raise ValueError ( \"Input shape must be defined when calling `build()` on \" \"a `Model` subclass.\" ) valid_types = ( tuple , list , tf . TensorShape , dict ) if not isinstance ( input_shape , valid_types ) : raise ValueError ( \"Specified input shape is not one of the valid types. \" \"Please specify a batch input shape of type tuple or \" \"list of input shapes. User provided \" \"input type: {}.\" . format ( type ( input_shape )) ) if input_shape and not self . inputs : # We create placeholders for the `None`s in the shape and build the # model in a Graph. Since tf.Variable is compatible with both eager # execution and graph building, the variables created after building # the model in a Graph are still valid when executing eagerly. if tf . executing_eagerly () : graph = tf . __internal__ . FuncGraph ( \"build_graph\" ) else : graph = backend . get_graph () with graph . as_default () : if isinstance ( input_shape , list ) and all ( d is None or isinstance ( d , int ) for d in input_shape ) : input_shape = tuple ( input_shape ) if isinstance ( input_shape , list ) : x = [ base_layer_utils . generate_placeholders_from_shape ( shape ) for shape in input_shape ] elif isinstance ( input_shape , dict ) : x = { k : base_layer_utils . generate_placeholders_from_shape ( shape ) for k , shape in input_shape . items () } else : x = base_layer_utils . generate_placeholders_from_shape ( input_shape ) kwargs = {} call_signature = self . _call_spec . full_argspec call_args = call_signature . args # Exclude `self`, `inputs`, and any argument with a default # value. if len ( call_args ) > 2 : if call_signature . defaults : call_args = call_args [ 2 : - len ( call_signature . defaults ) ] else : call_args = call_args [ 2 : ] for arg in call_args : if arg == \"training\" : # Case where `training` is a positional arg with no # default. kwargs [ \"training\" ] = False else : # Has invalid call signature with unknown positional # arguments. raise ValueError ( \"Currently, you cannot build your model if it \" \"has positional or keyword arguments that are \" \"not inputs to the model, but are required for \" \"its `call()` method. Instead, in order to \" \"instantiate and build your model, `call()` \" \"your model on real tensor data with all \" \"expected call arguments. The argument \" \"for `call()` can be a single list/tuple that \" \"contains multiple inputs.\" ) elif len ( call_args ) < 2 : # Signature without `inputs`. raise ValueError ( \"You can only call `build()` on a model if its \" \"`call()` method accepts an `inputs` argument.\" ) try : self . call ( x , ** kwargs ) except ( tf . errors . InvalidArgumentError , TypeError ) as e : raise ValueError ( \"You cannot build your model by calling `build` \" \"if your layers do not support float type inputs. \" \"Instead, in order to instantiate and build your \" \"model, call your model on real tensor data (of \" \"the correct dtype). \\n\\n The actual error from \" f \"`call` is: {e}.\" ) super (). build ( input_shape )","title":"build"},{"location":"reference/grid_transformer/layers/#call_1","text":"def call ( self , x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Applies the model on each element of the input tensor. Parameters: Name Type Description Default x tf.Tensor The input tensor None Returns: Type Description tf.Tensor The output tensor with the model applied on each element. View Source def call ( self , x : tf . Tensor ) -> tf . Tensor : \"\"\" Applies the model on each element of the input tensor. Parameters: x (tf.Tensor): The input tensor Returns: tf.Tensor: The output tensor with the model applied on each element. \"\"\" x = tf . transpose ( x , perm = ( 1 , 0 , 2 , 3 , 4 )) x = tf . vectorized_map ( self . model , x ) return tf . transpose ( x , perm = ( 1 , 0 , 2 , 3 , 4 ))","title":"call"},{"location":"reference/grid_transformer/layers/#compile_1","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Parameters: Name Type Description Default optimizer None String (name of optimizer) or optimizer instance. See tf.keras.optimizers . None loss None Loss function. May be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true,<br>y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. None metrics None List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true,<br>y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the shapes of the targets and of the model output. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. The metrics passed here are evaluated without sample weighting; if you would like sample weighting to apply, you can specify your metrics via the weighted_metrics argument instead. None loss_weights None Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. None weighted_metrics None List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. None run_eagerly None Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . False steps_per_execution None Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). 1 jit_compile None If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. None **kwargs None Arguments supported for backwards compatibility only. None View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = \"rmsprop\" , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs , ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . May be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = {' output_a ':' accuracy ' , ' output_b ' :[ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the shapes of the targets and of the model output . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . The metrics passed here are evaluated without sample weighting ; if you would like sample weighting to apply , you can specify your metrics via the ` weighted_metrics ` argument instead . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( \"compile\" ). set ( True ) self . _compile_config = generic_utils . Config ( optimizer = optimizer , loss = loss , metrics = metrics , loss_weights = loss_weights , weighted_metrics = weighted_metrics , run_eagerly = run_eagerly , steps_per_execution = steps_per_execution , jit_compile = jit_compile , ) with self . distribute_strategy . scope () : if \"experimental_steps_per_execution\" in kwargs : logging . warning ( \"The argument `steps_per_execution` is no longer \" \"experimental. Pass `steps_per_execution` instead of \" \"`experimental_steps_per_execution`.\" ) if not steps_per_execution : steps_per_execution = kwargs . pop ( \"experimental_steps_per_execution\" ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for # multi-output models, which have prefixes added for each # corresponding output name). from_serialized = kwargs . pop ( \"from_serialized\" , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) if isinstance ( loss , compile_utils . LossesContainer ) : self . compiled_loss = loss else : self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized , ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( \"You cannot enable `run_eagerly` and `jit_compile` \" \"at the same time.\" ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/grid_transformer/layers/#compute_loss_1","text":"def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses )","title":"compute_loss"},{"location":"reference/grid_transformer/layers/#compute_mask_1","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/layers/#compute_metrics_1","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model.call(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns # results for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) return self . get_metrics_result ()","title":"compute_metrics"},{"location":"reference/grid_transformer/layers/#compute_output_shape_1","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/layers/#compute_output_signature_1","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/layers/#count_params_1","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/layers/#evaluate_1","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 'auto' , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs,<br> targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). None batch_size None Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None sample_weight None Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . None steps None Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None **kwargs None Unused at this time. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = \"auto\" , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs , ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( \"evaluate\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"evaluate\" ) self . _assert_compile_was_called () self . _check_call_args ( \"evaluate\" ) self . _check_sample_weight_warning ( x , sample_weight ) _disallow_inside_tf_function ( \"evaluate\" ) use_cached_eval_dataset = kwargs . pop ( \"_use_cached_eval_dataset\" , False ) if kwargs : raise TypeError ( f \"Invalid keyword arguments: {list(kwargs.keys())}\" ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , \"_eval_data_handler\" , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch # iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( \"test\" , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/grid_transformer/layers/#evaluate_generator_1","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.evaluate_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.evaluate`, which supports generators.\" , stacklevel = 2 , ) self . _check_call_args ( \"evaluate_generator\" ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , )","title":"evaluate_generator"},{"location":"reference/grid_transformer/layers/#finalize_state_1","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/layers/#fit_1","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include sample_weights as a third component, note that sample weighting applies to the weighted_metrics argument but not the metrics argument in compile() . If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. If both validation_data and validation_split are provided, validation_data will override validation_split . validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . Note that sample weighting does not apply to metrics specified via the metrics argument in compile() . To apply sample weighting to your metrics, you can specify them via the weighted_metrics in compile() instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = \"auto\" , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include `sample_weights` as a third component, note that sample weighting applies to the `weighted_metrics` argument but not the `metrics` argument in `compile()`. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. If both `validation_data` and `validation_split` are provided, `validation_data` will override `validation_split`. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. Note that sample weighting does not apply to metrics specified via the `metrics` argument in `compile()`. To apply sample weighting to your metrics, you can specify them via the `weighted_metrics` in `compile()` instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( \"fit\" ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( \"Model\" , \"fit\" ) self . _assert_compile_was_called () self . _check_call_args ( \"fit\" ) _disallow_inside_tf_function ( \"fit\" ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) if validation_split and validation_data is None : # Create the validation data using the training data. Only supported # for `Tensor` and `NumPy` input. ( x , y , sample_weight , ), validation_data = data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split ) if validation_data : ( val_x , val_y , val_sample_weight , ) = data_adapter . unpack_x_y_sample_weight ( validation_data ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. steps_per_epoch_inferred = ( steps_per_epoch or data_handler . inferred_steps ) ( data_handler . _initial_epoch , data_handler . _initial_step , ) = self . _maybe_load_initial_counters_from_ckpt ( steps_per_epoch_inferred , initial_epoch ) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( \"train\" , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 , ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( \"Unexpected result of `train_function` \" \"(Empty logs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , \"_eval_data_handler\" , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items () } epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break if ( isinstance ( self . optimizer , optimizer_experimental . Optimizer ) and epochs > 0 ): self . optimizer . finalize_variable_values ( self . trainable_variables ) # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , \"_eval_data_handler\" , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/grid_transformer/layers/#fit_generator_1","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 , ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.fit_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.fit`, which supports generators.\" , stacklevel = 2 , ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch , )","title":"fit_generator"},{"location":"reference/grid_transformer/layers/#get_config_1","text":"def get_config ( self ) Returns the config of the Model . Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a Model . This allows the Model to be be reinstantiated later (without its trained weights) from this configuration. Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed Model are advised to override this method, and continue to update the dict from super(MyModel, self).get_config() to provide the proper configuration of this Model . The default config is an empty dict. Optionally, raise NotImplementedError to allow Keras to attempt a default serialization. Returns: Type Description None Python dictionary containing the configuration of this Model . View Source def get_config ( self ) : \" \"\" Returns the config of the `Model`. Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a `Model`. This allows the `Model` to be be reinstantiated later (without its trained weights) from this configuration. Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed `Model` are advised to override this method, and continue to update the dict from `super(MyModel, self).get_config()` to provide the proper configuration of this `Model`. The default config is an empty dict. Optionally, raise `NotImplementedError` to allow Keras to attempt a default serialization. Returns: Python dictionary containing the configuration of this `Model`. \"\" \" # Return an empty dict here because otherwise Model # subclass developers may see # their model's `__init__()` fed with unexpected keyword arguments, # if their `__init__()` takes no argument for example, and they # don't override `from_config()`, which would use `cls(**config)` # as a result. config = {} if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ) : if self . _is_compiled and hasattr ( self , \"_compile_config\" ) : config [ \"compile_config\" ] = self . _compile_config . serialize () if self . built : config [ \"build_input_shape\" ] = self . _build_input_shape return config","title":"get_config"},{"location":"reference/grid_transformer/layers/#get_input_at_1","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/layers/#get_input_mask_at_1","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/layers/#get_input_shape_at_1","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/layers/#get_layer_1","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Type Description Default name None String, name of layer. None index None Integer, index of layer. None Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( \"Provide only a layer name or a layer index. Received: \" f \"index={index}, name={name}.\" ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f \"Was asked to retrieve layer at index {index}\" f \" but model only has {len(self.layers)}\" \" layers.\" ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f \"No such layer: {name}. Existing layers are: \" f \"{list(layer.name for layer in self.layers)}.\" ) raise ValueError ( \"Provide either a layer name or layer index at `get_layer`.\" )","title":"get_layer"},{"location":"reference/grid_transformer/layers/#get_metrics_result_1","text":"def get_metrics_result ( self ) Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: Type Description None A dict containing values of the metrics listed in self.metrics . Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def get_metrics_result ( self ) : \" \"\" Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: A `dict` containing values of the metrics listed in `self.metrics`. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"get_metrics_result"},{"location":"reference/grid_transformer/layers/#get_output_at_1","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/layers/#get_output_mask_at_1","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/layers/#get_output_shape_at_1","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/layers/#get_weight_paths_1","text":"def get_weight_paths ( self ) Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a tf.Variable instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding tf.Variable instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: Type Description None A dict where keys are variable paths and values are tf.Variable instances. View Source def get_weight_paths ( self ) : \"\"\"Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a `tf.Variable` instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding `tf.Variable` instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: A dict where keys are variable paths and values are `tf.Variable` instances. Example: ```python class SubclassModel(tf.keras.Model): def __init__(self, name=None): super().__init__(name=name) self.d1 = tf.keras.layers.Dense(10) self.d2 = tf.keras.layers.Dense(20) def call(self, inputs): x = self.d1(inputs) return self.d2(x) model = SubclassModel() model(tf.zeros((10, 10))) weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': model.d1.kernel, # 'd1.bias': model.d1.bias, # 'd2.kernel': model.d2.kernel, # 'd2.bias': model.d2.bias, # } # Functional model inputs = tf.keras.Input((10,), batch_size=10) x = tf.keras.layers.Dense(20, name='d1')(inputs) output = tf.keras.layers.Dense(30, name='d2')(x) model = tf.keras.Model(inputs, output) d1 = model.layers[1] d2 = model.layers[2] weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': d1.kernel, # 'd1.bias': d1.bias, # 'd2.kernel': d2.kernel, # 'd2.bias': d2.bias, # } ``` \"\"\" result = {} ( descendants , object_paths_dict , ) = tf . __internal__ . tracking . ObjectGraphView ( self ). breadth_first_traversal () for descendant in descendants : if isinstance ( descendant , tf . Variable ) : trackable_references = object_paths_dict [ descendant ] object_path = \".\" . join ( [ t.name for t in trackable_references ] ) result [ object_path ] = descendant return result","title":"get_weight_paths"},{"location":"reference/grid_transformer/layers/#get_weights_1","text":"def get_weights ( self ) Retrieves the weights of the model. Returns: Type Description None A flat list of Numpy arrays. View Source def get_weights ( self ) : \"\" \"Retrieves the weights of the model. Returns: A flat list of Numpy arrays. \"\" \" with self.distribute_strategy.scope(): return super().get_weights()","title":"get_weights"},{"location":"reference/grid_transformer/layers/#load_weights_1","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Type Description Default filepath None String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . None by_name None Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. None skip_mismatch None Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). None options None Optional tf.train.CheckpointOptions object that specifies options for loading weights. None Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ) ): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( \"Load weights is not implemented with TPUStrategy \" \"with `steps_per_run` greater than 1. The \" f \"`steps_per_run` is {spr}\" ) if skip_mismatch and not by_name : raise ValueError ( \"When calling model.load_weights, skip_mismatch can only be \" \"set to True when by_name is True.\" ) filepath , save_format = _detect_save_format ( filepath ) if save_format == \"tf\" : status = self . _checkpoint . read ( filepath , options ) if by_name : raise NotImplementedError ( \"Weights may only be loaded based on topology into Models \" \"when loading TensorFlow-formatted weights \" \"(got by_name=True to load_weights).\" ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( \"`load_weights` requires h5py package when loading weights \" \"from HDF5. Try installing h5py.\" ) if not self . _is_graph_network and not self . built : raise ValueError ( \"Unable to load weights saved in HDF5 format into a \" \"subclassed Model which has not created its variables yet. \" \"Call the Model first, then load the weights.\" ) self . _assert_weights_created () with h5py . File ( filepath , \"r\" ) as f : if \"layer_names\" not in f . attrs and \"model_weights\" in f : f = f [ \"model_weights\" ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/grid_transformer/layers/#make_predict_function_1","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the predict function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = \"concat\" ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( outputs , tf . nest . map_structure ( lambda t : tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape , outputs , ), ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , reduce_retracing = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/grid_transformer/layers/#make_test_function_1","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the test function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) if self . _cluster_coordinator : self . test_function = ( lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it ,) ) ) else : self . test_function = test_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it , self . _steps_per_execution . value ()) ) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/grid_transformer/layers/#make_train_function_1","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the train function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = ( lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it ,) ) ) else : self . train_function = train_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it , self . _steps_per_execution . value ()) ) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/grid_transformer/layers/#predict_1","text":"def predict ( self , x , batch_size = None , verbose = 'auto' , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Type Description Default x None Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None batch_size None Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None steps None Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = \"auto\" , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks]( https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( \"predict\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"predict\" ) self . _check_call_args ( \"predict\" ) _disallow_inside_tf_function ( \"predict\" ) # TODO(yashkatariya): Cache model on the coordinator for faster # prediction. If running under PSS, then swap it with OneDeviceStrategy # so that execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None verbose = _get_verbosity ( verbose , self . distribute_strategy ) outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy ) ) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = ( data_option ) x = x . with_options ( options ) except ValueError : warnings . warn ( \"Using Model.predict with MultiWorkerMirroredStrategy \" \"or TPUStrategy and AutoShardPolicy.FILE might lead to \" \"out-of-order result. Consider setting it to \" \"AutoShardPolicy.DATA.\" , stacklevel = 2 , ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = ( tmp_batch_outputs # No error, now safe to assign. ) if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs , ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs , ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { \"outputs\" : batch_outputs } ) if batch_outputs is None : raise ValueError ( \"Unexpected result of `predict_function` \" \"(Empty batch_outputs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , potentially_ragged_concat , outputs ) # If originally PSS strategy was used, then replace it back since # predict is running under `OneDeviceStrategy` after the swap and once # its done we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/grid_transformer/layers/#predict_generator_1","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.predict_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.predict`, which supports generators.\" , stacklevel = 2 , ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , )","title":"predict_generator"},{"location":"reference/grid_transformer/layers/#predict_on_batch_1","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\" \"Returns predictions for a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`. \"\" \" self . _check_call_args ( \"predict_on_batch\" ) _disallow_inside_tf_function ( \"predict_on_batch\" ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/grid_transformer/layers/#predict_step_1","text":"def predict_step ( self , data ) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None The result of one inference step, typically the output of calling the Model on data. View Source def predict_step ( self , data ) : \" \"\" The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by `Model.make_predict_function`. This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_predict_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: The result of one inference step, typically the output of calling the `Model` on data. \"\" \" x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) return self ( x , training = False )","title":"predict_step"},{"location":"reference/grid_transformer/layers/#reset_metrics_1","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. View Source def reset_metrics ( self ) : \"\" \"Resets the state of all the metrics in the model. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model . compile ( optimizer = \"Adam\" , loss = \"mse\" , metrics = [ \"mae\" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\" \" for m in self.metrics: m.reset_state()","title":"reset_metrics"},{"location":"reference/grid_transformer/layers/#reset_states_1","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , \"reset_states\" ) and getattr ( layer , \"stateful\" , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/grid_transformer/layers/#save_1","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Parameters: Name Type Description Default filepath None String, PathLike, path to SavedModel or H5 file to save the model. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None include_optimizer None If True, save optimizer's state together. None save_format None Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. None signatures None Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. None options None (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. None save_traces None (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. None View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True , ) : \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide]( https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces , )","title":"save"},{"location":"reference/grid_transformer/layers/#save_spec_1","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Type Description Default dynamic_batch None Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . None Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/grid_transformer/layers/#save_weights_1","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Type Description Default filepath None String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None save_format None Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. None options None Optional tf.train.CheckpointOptions object that specifies options for saving weights. None Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints]( https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = io_utils . path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = \"h5\" else : save_format = \"tf\" else : user_format = save_format . lower () . strip () if user_format in ( \"tensorflow\" , \"tf\" ): save_format = \"tf\" elif user_format in ( \"hdf5\" , \"h5\" , \"keras\" ): save_format = \"h5\" else : raise ValueError ( f \"Unknown format. Received: `save_format`={save_format}. \" 'Was expecting one of {\"tf\", \"h5\"}.' ) if save_format == \"tf\" and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f \"filepath ({filepath}) looks like an HDF5 file. \" 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == \"h5\" and h5py is None : raise ImportError ( \"`save_weights` requires h5py when saving in hdf5, but h5py is \" \"not available. Try installing h5py package.\" ) if save_format == \"tf\" : check_filepath = filepath + \".index\" else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = io_utils . ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == \"h5\" : with h5py . File ( filepath , \"w\" ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if not tf . executing_eagerly (): # Call `get_session` to initialize any uninitialized variables. backend . get_session () self . _checkpoint . write ( filepath , options = options ) # Record this checkpoint so it's visible from # tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ], )","title":"save_weights"},{"location":"reference/grid_transformer/layers/#set_weights_1","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/layers/#summary_1","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None ) Prints a string summary of the network. Parameters: Name Type Description Default line_length None Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). None positions None Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . None print_fn None Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. print expand_nested None Whether to expand the nested models. If not provided, defaults to False . None show_trainable None Whether to show if a layer is trainable. If not provided, defaults to False . None layer_range None a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to layer_range[0] and the end predicate will be the last element it matches to layer_range[1] . By default None which considers all layers of model. None Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None , ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. layer_range: a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to `layer_range[0]` and the end predicate will be the last element it matches to `layer_range[1]`. By default `None` which considers all layers of model. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( \"This model has not yet been built. \" \"Build the model first by calling `build()` or by calling \" \"the model on a batch of data.\" ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable , layer_range = layer_range , )","title":"summary"},{"location":"reference/grid_transformer/layers/#test_on_batch_1","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"test_on_batch\" ) _disallow_inside_tf_function ( \"test_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/grid_transformer/layers/#test_step_1","text":"def test_step ( self , data ) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. View Source def test_step ( self , data ) : \" \"\" The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by `Model.make_test_function`. This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_test_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) # Updates stateful loss metrics. self . compute_loss ( x , y , y_pred , sample_weight ) return self . compute_metrics ( x , y , y_pred , sample_weight )","title":"test_step"},{"location":"reference/grid_transformer/layers/#to_json_1","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to * json.dumps() . None Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to *`json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/grid_transformer/layers/#to_yaml_1","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to yaml.dump() . None Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( \"Method `model.to_yaml()` has been removed due to security risk of \" \"arbitrary code execution. Please use `model.to_json()` instead.\" )","title":"to_yaml"},{"location":"reference/grid_transformer/layers/#train_on_batch_1","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight None Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"train_on_batch\" ) _disallow_inside_tf_function ( \"train_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/grid_transformer/layers/#train_step_1","text":"def train_step ( self , data ) The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see Customizing what happens in fit . This method is called by Model.make_train_function . This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def train_step ( self , data ) : \" \"\" The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see [Customizing what happens in fit]( https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This method is called by `Model.make_train_function`. This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_train_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) # Run forward pass. with tf . GradientTape () as tape : y_pred = self ( x , training = True ) loss = self . compute_loss ( x , y , y_pred , sample_weight ) self . _validate_target_and_loss ( y , loss ) # Run backwards pass. self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return self . compute_metrics ( x , y , y_pred , sample_weight )","title":"train_step"},{"location":"reference/grid_transformer/mask/","text":"Module grid_transformer.mask This module contains utility functions for creating and manipulating masks for grid-like inputs. Functions: init_weights: Initialize weights for a tensor with a shape of shape and data type of dtype . take_left: Select the leftmost column of the tensor x. take_by_index: Select a column of the tensor x by index mix: Mix the leftmost and the middle column of the tensor x empty_last: Returns a tensor of zeros with the same shape as the last column of x random_mask: Generates a random mask with the same shape as the input tensor take_by_index: Take a slice of an array by index mix: Mix 2 slices of an array by index Example usage: # Initialize weights for a tensor of shape ( 3 , 3 , 3 ) weights = init_weights ( shape = ( 3 , 3 , 3 )) # Select the leftmost column of a tensor of shape ( batch_size , height , width , channels ) left_column = take_left ( x ) # Select a column of the tensor x by index column = take_by_index ( x , i = 4 ) # Mix the leftmost and the middle column of the tensor x mixed = mix ( x ) # Generates a random mask with the same shape as the input tensor mask = random_mask ( inputs , last_index = 5 ) # Take a slice of an array by index sliced = take_by_index ( x , i = 4 ) # Mix 2 slices of an array by index mixed = mix ( x ) View Source \"\"\" This module contains utility functions for creating and manipulating masks for grid-like inputs. Functions: - init_weights: Initialize weights for a tensor with a shape of `shape` and data type of `dtype`. - take_left: Select the leftmost column of the tensor x. - take_by_index: Select a column of the tensor x by index - mix: Mix the leftmost and the middle column of the tensor x - empty_last: Returns a tensor of zeros with the same shape as the last column of x - random_mask: Generates a random mask with the same shape as the input tensor - take_by_index: Take a slice of an array by index - mix: Mix 2 slices of an array by index Example usage: # Initialize weights for a tensor of shape (3,3,3) weights = init_weights(shape=(3,3,3)) # Select the leftmost column of a tensor of shape (batch_size, height, width, channels) left_column = take_left(x) # Select a column of the tensor x by index column = take_by_index(x, i=4) # Mix the leftmost and the middle column of the tensor x mixed = mix(x) # Generates a random mask with the same shape as the input tensor mask = random_mask(inputs, last_index=5) # Take a slice of an array by index sliced = take_by_index(x, i=4) # Mix 2 slices of an array by index mixed = mix(x) \"\"\" from typing import Tuple , Callable , Optional , Type , Union import tensorflow as tf from tensorflow . keras import Model from models_utils import ops as K def init_weights ( shape : Tuple [ int ], dtype : tf . DType = tf . float32 ) -> tf . Tensor : \"\"\" Initialize weights for a tensor with a shape of `shape` and data type of `dtype` :param shape: Tuple of integers representing the shape of the tensor. :param dtype: Data type of the tensor. Default is tf.float32 :return: Tensor with the initialized weights \"\"\" return tf . cast ( K . init . image ( shape = shape , pre = True ), dtype = dtype ) def take_left ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Select the leftmost column of the tensor x :param x: Tensor of shape (batch_size, height, width, channels) :return: Tensor of shape (batch_size, height, 1, channels) \"\"\" return x [ ... , 7 : 8 ] def take_by_index ( x : tf . Tensor , i : int = 8 ) -> tf . Tensor : \"\"\" Select a column of the tensor x by index :param x: Tensor of shape (batch_size, height, width, channels) :param i: Index of the column to select, Default is 8 :return: Tensor of shape (batch_size, height, 1, channels) \"\"\" return x [ ... , i : i + 1 ] def mix ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Mix the leftmost and the middle column of the tensor x :param x: Tensor of shape (batch_size, height, width, channels) :return: Tensor of shape (batch_size, height, 1, channels) \"\"\" return ( x [ ... , 7 : 8 ] + x [ ... , 5 : 6 ]) / 2 def empty_last ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Returns a tensor of zeros with the same shape as the last column of x :param x: Tensor of shape (batch_size, height, width, channels) :return: Tensor of shape (batch_size, height, 1, channels) filled with zeros \"\"\" return tf . zeros_like ( x [ ... , 7 : 8 ]) def random_mask ( inputs : tf . Tensor , last_index : int ) -> tf . Tensor : \"\"\" Generates a random mask with the same shape as the input tensor. Each element in the mask is a boolean value indicating whether or not the corresponding element in the input tensor should be kept. :param inputs: Tensor to generate a mask for. :param last_index: The last index to choose from when generating the mask. :return: Boolean tensor of the same shape as the input tensor, indicating which elements should be kept. \"\"\" shape = tf . shape ( inputs ) indexes = tf . random . uniform ( shape = shape [ 0 : 1 ], maxval = last_index , dtype = tf . int32 ) mask = tf . cast ( tf . one_hot ( indexes , last_index ), dtype = 'bool' ) return mask def take_by_index ( x : tf . Tensor , i : int = 8 ) -> tf . Tensor : \"\"\" Take a slice of an array by index. Parameters: x (tf.Tensor): The input array. i (int, optional): The index of the slice. Default is 8. Returns: tf.Tensor: The sliced array. \"\"\" return x [ ... , i : i + 1 ] def mix ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Mix 2 slices of an array by index. Parameters: x (tf.Tensor): The input array. Returns: tf.Tensor: The mixed array. \"\"\" return ( x [ ... , 7 : 8 ] + x [ ... , 5 : 6 ]) / 2 def empty_last ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Return a tensor of zeros with the shape of the last slice of input tensor. Parameters: x (tf.Tensor): The input tensor. Returns: tf.Tensor: The tensor of zeros. \"\"\" return tf . zeros_like ( x [ ... , 7 : 8 ]) def random_mask ( inputs : tf . Tensor , last_index : int ) -> tf . Tensor : \"\"\" Generate a random mask of the same shape as inputs. Parameters: inputs (tf.Tensor): The input tensor. last_index (int): The last index to use for generating the mask. Returns: tf.Tensor: A boolean tensor of the same shape as inputs, with random values of True or False. \"\"\" shape = tf . shape ( inputs ) indexes = tf . random . uniform ( shape = shape [ 0 : 1 ], maxval = last_index , dtype = tf . int32 ) mask = tf . cast ( tf . one_hot ( indexes , last_index ), dtype = 'bool' ) return mask def constant_mask ( inputs : tf . Tensor , value : int = 8 , last_index : Optional [ int ] = None ) -> tf . Tensor : \"\"\" Generate a constant mask of the same shape as inputs. The mask will have the same value in all positions. Parameters: inputs (tf.Tensor): The input tensor. value (int, optional): The value to fill the mask with. Default is 8. last_index (int, optional): The last index to use for generating the mask. If not provided, it will be set to value + 1. Returns: tf.Tensor: A boolean tensor of the same shape as inputs, with constant value of True or False. \"\"\" shape = tf . shape ( inputs ) indexes = tf . fill ( dims = shape [ 0 : 1 ], value = value ) mask = tf . cast ( tf . one_hot ( indexes , last_index if last_index else value + 1 ), dtype = 'bool' ) return mask class ImageMask ( tf . keras . Model ): def __init__ ( self , last : Callable [[ tf . Tensor ], tf . Tensor ], last_index : Optional [ int ] = None ): \"\"\" Class that applies a mask to an image. Parameters: last (Callable[[tf.Tensor], tf.Tensor]): A callable that takes an image tensor and returns the last slice. last_index (int, optional): The last index of the image. If not provided, it will be set to the last index of the image. \"\"\" super () . __init__ () self . get_last = last self . last_index = last_index def call ( self , inputs : Tuple [ tf . Tensor , tf . Tensor ]) -> tf . Tensor : \"\"\" Applies the mask to the image. Parameters: inputs (Tuple[tf.Tensor, tf.Tensor]): A tuple containing the image tensor and the mask tensor. Returns: tf.Tensor: The masked image. \"\"\" inputs , mask = inputs mask = mask [:, None , None ] inputs = inputs [ ... , : self . last_index ] return tf . cast (( 1 - mask ) * inputs + mask * tf . tile ( self . get_last ( inputs )[ None ], ( 1 , 1 , 1 , inputs . shape [ - 1 ])), dtype = 'float32' ) Functions constant_mask def constant_mask ( inputs : tensorflow . python . framework . ops . Tensor , value : int = 8 , last_index : Optional [ int ] = None ) -> tensorflow . python . framework . ops . Tensor Generate a constant mask of the same shape as inputs. The mask will have the same value in all positions. Parameters: Name Type Description Default inputs tf.Tensor The input tensor. None value int The value to fill the mask with. Default is 8. None last_index int The last index to use for generating the mask. If not provided, it will be set to value + 1. None Returns: Type Description tf.Tensor A boolean tensor of the same shape as inputs, with constant value of True or False. View Source def constant_mask ( inputs : tf . Tensor , value : int = 8 , last_index : Optional [ int ] = None ) -> tf . Tensor : \"\"\" Generate a constant mask of the same shape as inputs. The mask will have the same value in all positions. Parameters: inputs (tf.Tensor): The input tensor. value (int, optional): The value to fill the mask with. Default is 8. last_index (int, optional): The last index to use for generating the mask. If not provided, it will be set to value + 1. Returns: tf.Tensor: A boolean tensor of the same shape as inputs, with constant value of True or False. \"\"\" shape = tf . shape ( inputs ) indexes = tf . fill ( dims = shape [ 0:1 ] , value = value ) mask = tf . cast ( tf . one_hot ( indexes , last_index if last_index else value + 1 ), dtype = 'bool' ) return mask empty_last def empty_last ( x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Return a tensor of zeros with the shape of the last slice of input tensor. Parameters: Name Type Description Default x tf.Tensor The input tensor. None Returns: Type Description tf.Tensor The tensor of zeros. View Source def empty_last ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Return a tensor of zeros with the shape of the last slice of input tensor. Parameters: x (tf.Tensor): The input tensor. Returns: tf.Tensor: The tensor of zeros. \"\"\" return tf . zeros_like ( x [..., 7 : 8 ]) init_weights def init_weights ( shape : Tuple [ int ], dtype : tensorflow . python . framework . dtypes . DType = tf . float32 ) -> tensorflow . python . framework . ops . Tensor Initialize weights for a tensor with a shape of shape and data type of dtype Parameters: Name Type Description Default shape None Tuple of integers representing the shape of the tensor. None dtype None Data type of the tensor. Default is tf.float32 None Returns: Type Description None Tensor with the initialized weights View Source def init_weights ( shape : Tuple [ int ] , dtype : tf . DType = tf . float32 ) -> tf . Tensor : \" \"\" Initialize weights for a tensor with a shape of `shape` and data type of `dtype` :param shape: Tuple of integers representing the shape of the tensor. :param dtype: Data type of the tensor. Default is tf.float32 :return: Tensor with the initialized weights \"\" \" return tf . cast ( K . init . image ( shape = shape , pre = True ), dtype = dtype ) mix def mix ( x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Mix 2 slices of an array by index. Parameters: Name Type Description Default x tf.Tensor The input array. None Returns: Type Description tf.Tensor The mixed array. View Source def mix ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Mix 2 slices of an array by index. Parameters: x (tf.Tensor): The input array. Returns: tf.Tensor: The mixed array. \"\"\" return ( x [..., 7 : 8 ] + x [..., 5 : 6 ]) / 2 random_mask def random_mask ( inputs : tensorflow . python . framework . ops . Tensor , last_index : int ) -> tensorflow . python . framework . ops . Tensor Generate a random mask of the same shape as inputs. Parameters: Name Type Description Default inputs tf.Tensor The input tensor. None last_index int The last index to use for generating the mask. None Returns: Type Description tf.Tensor A boolean tensor of the same shape as inputs, with random values of True or False. View Source def random_mask ( inputs : tf . Tensor , last_index : int ) -> tf . Tensor : \"\"\" Generate a random mask of the same shape as inputs. Parameters: inputs (tf.Tensor): The input tensor. last_index (int): The last index to use for generating the mask. Returns: tf.Tensor: A boolean tensor of the same shape as inputs, with random values of True or False. \"\"\" shape = tf . shape ( inputs ) indexes = tf . random . uniform ( shape = shape [ 0 : 1 ], maxval = last_index , dtype = tf . int32 ) mask = tf . cast ( tf . one_hot ( indexes , last_index ), dtype = 'bool' ) return mask take_by_index def take_by_index ( x : tensorflow . python . framework . ops . Tensor , i : int = 8 ) -> tensorflow . python . framework . ops . Tensor Take a slice of an array by index. Parameters: Name Type Description Default x tf.Tensor The input array. None i int The index of the slice. Default is 8. None Returns: Type Description tf.Tensor The sliced array. View Source def take_by_index ( x : tf . Tensor , i : int = 8 ) -> tf . Tensor : \"\"\" Take a slice of an array by index. Parameters: x (tf.Tensor): The input array. i (int, optional): The index of the slice. Default is 8. Returns: tf.Tensor: The sliced array. \"\"\" return x [..., i : i + 1 ] take_left def take_left ( x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Select the leftmost column of the tensor x Parameters: Name Type Description Default x None Tensor of shape (batch_size, height, width, channels) None Returns: Type Description None Tensor of shape (batch_size, height, 1, channels) View Source def take_left ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Select the leftmost column of the tensor x :param x: Tensor of shape (batch_size, height, width, channels) :return: Tensor of shape (batch_size, height, 1, channels) \"\"\" return x [..., 7 : 8 ] Classes ImageMask class ImageMask ( last : Callable [[ tensorflow . python . framework . ops . Tensor ], tensorflow . python . framework . ops . Tensor ], last_index : Optional [ int ] = None ) Model groups layers into an object with training and inference features. Attributes Name Type Description Default inputs None The input(s) of the model: a keras.Input object or a combination of keras.Input objects in a dict, list or tuple. None outputs None The output(s) of the model: a tensor that originated from keras.Input objects or a combination of such tensors in a dict, list or tuple. See Functional API example below. None name None String, the name of the model. None View Source class ImageMask ( tf . keras . Model ) : def __init__ ( self , last : Callable [ [tf.Tensor ] , tf . Tensor ] , last_index : Optional [ int ] = None ) : \"\"\" Class that applies a mask to an image. Parameters: last (Callable[[tf.Tensor], tf.Tensor]): A callable that takes an image tensor and returns the last slice. last_index (int, optional): The last index of the image. If not provided, it will be set to the last index of the image. \"\"\" super (). __init__ () self . get_last = last self . last_index = last_index def call ( self , inputs : Tuple [ tf.Tensor, tf.Tensor ] ) -> tf . Tensor : \"\"\" Applies the mask to the image. Parameters: inputs (Tuple[tf.Tensor, tf.Tensor]): A tuple containing the image tensor and the mask tensor. Returns: tf.Tensor: The masked image. \"\"\" inputs , mask = inputs mask = mask [ :, None, None ] inputs = inputs [ ..., :self.last_index ] return tf . cast (( 1 - mask ) * inputs + mask * tf . tile ( self . get_last ( inputs ) [ None ] , ( 1 , 1 , 1 , inputs . shape [ -1 ] )), dtype = 'float32' ) Ancestors (in MRO) keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector Static methods from_config def from_config ( config , custom_objects = None ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config , custom_objects = None ): compile_config = config . pop ( \"compile_config\" , None ) build_input_shape = config . pop ( \"build_input_shape\" , None ) # `from_config` assumes `cls` is either `Functional` or a child class of # `Functional`. In the case that `cls` is meant to behave like a child # class of `Functional` but only inherits from the `Model` class, we # have to call `cls(...)` instead of `Functional.from_config`. from keras.engine import functional with serialization . SharedObjectLoadingScope (): functional_model_keys = [ \"name\" , \"layers\" , \"input_layers\" , \"output_layers\" , ] if all ( key in config for key in functional_model_keys ): inputs , outputs , layers = functional . reconstruct_from_config ( config , custom_objects ) model = cls ( inputs = inputs , outputs = outputs , name = config . get ( \"name\" ) ) functional . connect_ancillary_layers ( model , layers ) else : # The config does not contain all the information necessary to # revive a Functional model. This happens when the user creates # subclassed models where `get_config()` is returning # insufficient information to be considered a Functional model. # In this case, we fall back to provide all config into the # constructor of the class. try : model = cls ( ** config ) except TypeError as e : raise TypeError ( \"Unable to revive model from config. When overriding \" \"the `get_config()`, make sure that the returned \" \"config contains all items used as arguments in the \" f \"constructor to { cls } , which is the default behavior. \" \"You can override this default behavior by defining a \" \"`from_config` method to specify how to create an \" f \"instance of { cls . __name__ } from the config. \\n\\n \" f \"Error encountered during deserialization: \\n { e } \" ) if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ): if build_input_shape : model . build ( build_input_shape ) if compile_config is not None : model . _compile_from_config ( compile_config , base_class = Model ) return model with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. distribute_reduction_method The method employed to reduce per-replica values during training. Unless specified, the value \"auto\" will be assumed, indicating that the reduction strategy should be chosen based on the current running environment. See reduce_per_replica function for more details. distribute_strategy The tf.distribute.Strategy this model was created under. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . layers losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics Returns the model's metrics added using compile() , add_metric() APIs. Note: Metrics passed to compile() are available only after a keras.Model has been trained/evaluated on actual data. metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. state_updates Deprecated, do NOT use! Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape ) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape instance, or list/dict of shapes, where shapes are tuples, integers, or TensorShape instances. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, TensorShape , or dict). 2. If the model requires call arguments that are agnostic to the input shapes (positional or keyword arg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. View Source @generic_utils.default def build ( self , input_shape ) : \" \"\" Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call `model.build()` in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, `TensorShape` instance, or list/dict of shapes, where shapes are tuples, integers, or `TensorShape` instances. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, `TensorShape`, or dict). 2. If the model requires call arguments that are agnostic to the input shapes (positional or keyword arg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. \"\" \" if self . _is_graph_network : super (). build ( input_shape ) return if input_shape is None : raise ValueError ( \"Input shape must be defined when calling `build()` on \" \"a `Model` subclass.\" ) valid_types = ( tuple , list , tf . TensorShape , dict ) if not isinstance ( input_shape , valid_types ) : raise ValueError ( \"Specified input shape is not one of the valid types. \" \"Please specify a batch input shape of type tuple or \" \"list of input shapes. User provided \" \"input type: {}.\" . format ( type ( input_shape )) ) if input_shape and not self . inputs : # We create placeholders for the `None`s in the shape and build the # model in a Graph. Since tf.Variable is compatible with both eager # execution and graph building, the variables created after building # the model in a Graph are still valid when executing eagerly. if tf . executing_eagerly () : graph = tf . __internal__ . FuncGraph ( \"build_graph\" ) else : graph = backend . get_graph () with graph . as_default () : if isinstance ( input_shape , list ) and all ( d is None or isinstance ( d , int ) for d in input_shape ) : input_shape = tuple ( input_shape ) if isinstance ( input_shape , list ) : x = [ base_layer_utils . generate_placeholders_from_shape ( shape ) for shape in input_shape ] elif isinstance ( input_shape , dict ) : x = { k : base_layer_utils . generate_placeholders_from_shape ( shape ) for k , shape in input_shape . items () } else : x = base_layer_utils . generate_placeholders_from_shape ( input_shape ) kwargs = {} call_signature = self . _call_spec . full_argspec call_args = call_signature . args # Exclude `self`, `inputs`, and any argument with a default # value. if len ( call_args ) > 2 : if call_signature . defaults : call_args = call_args [ 2 : - len ( call_signature . defaults ) ] else : call_args = call_args [ 2 : ] for arg in call_args : if arg == \"training\" : # Case where `training` is a positional arg with no # default. kwargs [ \"training\" ] = False else : # Has invalid call signature with unknown positional # arguments. raise ValueError ( \"Currently, you cannot build your model if it \" \"has positional or keyword arguments that are \" \"not inputs to the model, but are required for \" \"its `call()` method. Instead, in order to \" \"instantiate and build your model, `call()` \" \"your model on real tensor data with all \" \"expected call arguments. The argument \" \"for `call()` can be a single list/tuple that \" \"contains multiple inputs.\" ) elif len ( call_args ) < 2 : # Signature without `inputs`. raise ValueError ( \"You can only call `build()` on a model if its \" \"`call()` method accepts an `inputs` argument.\" ) try : self . call ( x , ** kwargs ) except ( tf . errors . InvalidArgumentError , TypeError ) as e : raise ValueError ( \"You cannot build your model by calling `build` \" \"if your layers do not support float type inputs. \" \"Instead, in order to instantiate and build your \" \"model, call your model on real tensor data (of \" \"the correct dtype). \\n\\n The actual error from \" f \"`call` is: {e}.\" ) super (). build ( input_shape ) call def call ( self , inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ] ) -> tensorflow . python . framework . ops . Tensor Applies the mask to the image. Parameters: Name Type Description Default inputs Tuple[tf.Tensor, tf.Tensor] A tuple containing the image tensor and the mask tensor. None Returns: Type Description tf.Tensor The masked image. View Source def call ( self , inputs : Tuple [ tf.Tensor, tf.Tensor ] ) -> tf . Tensor : \"\"\" Applies the mask to the image. Parameters: inputs (Tuple[tf.Tensor, tf.Tensor]): A tuple containing the image tensor and the mask tensor. Returns: tf.Tensor: The masked image. \"\"\" inputs , mask = inputs mask = mask [ :, None, None ] inputs = inputs [ ..., :self.last_index ] return tf . cast (( 1 - mask ) * inputs + mask * tf . tile ( self . get_last ( inputs ) [ None ] , ( 1 , 1 , 1 , inputs . shape [ -1 ] )), dtype = 'float32' ) compile def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Parameters: Name Type Description Default optimizer None String (name of optimizer) or optimizer instance. See tf.keras.optimizers . None loss None Loss function. May be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true,<br>y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. None metrics None List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true,<br>y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the shapes of the targets and of the model output. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. The metrics passed here are evaluated without sample weighting; if you would like sample weighting to apply, you can specify your metrics via the weighted_metrics argument instead. None loss_weights None Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. None weighted_metrics None List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. None run_eagerly None Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . False steps_per_execution None Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). 1 jit_compile None If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. None **kwargs None Arguments supported for backwards compatibility only. None View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = \"rmsprop\" , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs , ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . May be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = {' output_a ':' accuracy ' , ' output_b ' :[ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the shapes of the targets and of the model output . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . The metrics passed here are evaluated without sample weighting ; if you would like sample weighting to apply , you can specify your metrics via the ` weighted_metrics ` argument instead . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( \"compile\" ). set ( True ) self . _compile_config = generic_utils . Config ( optimizer = optimizer , loss = loss , metrics = metrics , loss_weights = loss_weights , weighted_metrics = weighted_metrics , run_eagerly = run_eagerly , steps_per_execution = steps_per_execution , jit_compile = jit_compile , ) with self . distribute_strategy . scope () : if \"experimental_steps_per_execution\" in kwargs : logging . warning ( \"The argument `steps_per_execution` is no longer \" \"experimental. Pass `steps_per_execution` instead of \" \"`experimental_steps_per_execution`.\" ) if not steps_per_execution : steps_per_execution = kwargs . pop ( \"experimental_steps_per_execution\" ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for # multi-output models, which have prefixes added for each # corresponding output name). from_serialized = kwargs . pop ( \"from_serialized\" , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) if isinstance ( loss , compile_utils . LossesContainer ) : self . compiled_loss = loss else : self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized , ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( \"You cannot enable `run_eagerly` and `jit_compile` \" \"at the same time.\" ) else : self . _jit_compile = jit_compile compute_loss def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses ) compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_metrics def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model.call(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns # results for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) return self . get_metrics_result () compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) evaluate def evaluate ( self , x = None , y = None , batch_size = None , verbose = 'auto' , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs,<br> targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). None batch_size None Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None sample_weight None Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . None steps None Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None **kwargs None Unused at this time. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = \"auto\" , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs , ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( \"evaluate\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"evaluate\" ) self . _assert_compile_was_called () self . _check_call_args ( \"evaluate\" ) self . _check_sample_weight_warning ( x , sample_weight ) _disallow_inside_tf_function ( \"evaluate\" ) use_cached_eval_dataset = kwargs . pop ( \"_use_cached_eval_dataset\" , False ) if kwargs : raise TypeError ( f \"Invalid keyword arguments: {list(kwargs.keys())}\" ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , \"_eval_data_handler\" , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch # iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( \"test\" , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) evaluate_generator def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.evaluate_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.evaluate`, which supports generators.\" , stacklevel = 2 , ) self . _check_call_args ( \"evaluate_generator\" ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass fit def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include sample_weights as a third component, note that sample weighting applies to the weighted_metrics argument but not the metrics argument in compile() . If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. If both validation_data and validation_split are provided, validation_data will override validation_split . validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . Note that sample weighting does not apply to metrics specified via the metrics argument in compile() . To apply sample weighting to your metrics, you can specify them via the weighted_metrics in compile() instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = \"auto\" , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include `sample_weights` as a third component, note that sample weighting applies to the `weighted_metrics` argument but not the `metrics` argument in `compile()`. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. If both `validation_data` and `validation_split` are provided, `validation_data` will override `validation_split`. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. Note that sample weighting does not apply to metrics specified via the `metrics` argument in `compile()`. To apply sample weighting to your metrics, you can specify them via the `weighted_metrics` in `compile()` instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( \"fit\" ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( \"Model\" , \"fit\" ) self . _assert_compile_was_called () self . _check_call_args ( \"fit\" ) _disallow_inside_tf_function ( \"fit\" ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) if validation_split and validation_data is None : # Create the validation data using the training data. Only supported # for `Tensor` and `NumPy` input. ( x , y , sample_weight , ), validation_data = data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split ) if validation_data : ( val_x , val_y , val_sample_weight , ) = data_adapter . unpack_x_y_sample_weight ( validation_data ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. steps_per_epoch_inferred = ( steps_per_epoch or data_handler . inferred_steps ) ( data_handler . _initial_epoch , data_handler . _initial_step , ) = self . _maybe_load_initial_counters_from_ckpt ( steps_per_epoch_inferred , initial_epoch ) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( \"train\" , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 , ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( \"Unexpected result of `train_function` \" \"(Empty logs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , \"_eval_data_handler\" , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items () } epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break if ( isinstance ( self . optimizer , optimizer_experimental . Optimizer ) and epochs > 0 ): self . optimizer . finalize_variable_values ( self . trainable_variables ) # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , \"_eval_data_handler\" , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history fit_generator def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 , ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.fit_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.fit`, which supports generators.\" , stacklevel = 2 , ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch , ) get_config def get_config ( self ) Returns the config of the Model . Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a Model . This allows the Model to be be reinstantiated later (without its trained weights) from this configuration. Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed Model are advised to override this method, and continue to update the dict from super(MyModel, self).get_config() to provide the proper configuration of this Model . The default config is an empty dict. Optionally, raise NotImplementedError to allow Keras to attempt a default serialization. Returns: Type Description None Python dictionary containing the configuration of this Model . View Source def get_config ( self ) : \" \"\" Returns the config of the `Model`. Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a `Model`. This allows the `Model` to be be reinstantiated later (without its trained weights) from this configuration. Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed `Model` are advised to override this method, and continue to update the dict from `super(MyModel, self).get_config()` to provide the proper configuration of this `Model`. The default config is an empty dict. Optionally, raise `NotImplementedError` to allow Keras to attempt a default serialization. Returns: Python dictionary containing the configuration of this `Model`. \"\" \" # Return an empty dict here because otherwise Model # subclass developers may see # their model's `__init__()` fed with unexpected keyword arguments, # if their `__init__()` takes no argument for example, and they # don't override `from_config()`, which would use `cls(**config)` # as a result. config = {} if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ) : if self . _is_compiled and hasattr ( self , \"_compile_config\" ) : config [ \"compile_config\" ] = self . _compile_config . serialize () if self . built : config [ \"build_input_shape\" ] = self . _build_input_shape return config get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_layer def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Type Description Default name None String, name of layer. None index None Integer, index of layer. None Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( \"Provide only a layer name or a layer index. Received: \" f \"index={index}, name={name}.\" ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f \"Was asked to retrieve layer at index {index}\" f \" but model only has {len(self.layers)}\" \" layers.\" ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f \"No such layer: {name}. Existing layers are: \" f \"{list(layer.name for layer in self.layers)}.\" ) raise ValueError ( \"Provide either a layer name or layer index at `get_layer`.\" ) get_metrics_result def get_metrics_result ( self ) Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: Type Description None A dict containing values of the metrics listed in self.metrics . Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def get_metrics_result ( self ) : \" \"\" Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: A `dict` containing values of the metrics listed in `self.metrics`. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weight_paths def get_weight_paths ( self ) Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a tf.Variable instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding tf.Variable instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: Type Description None A dict where keys are variable paths and values are tf.Variable instances. View Source def get_weight_paths ( self ) : \"\"\"Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a `tf.Variable` instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding `tf.Variable` instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: A dict where keys are variable paths and values are `tf.Variable` instances. Example: ```python class SubclassModel(tf.keras.Model): def __init__(self, name=None): super().__init__(name=name) self.d1 = tf.keras.layers.Dense(10) self.d2 = tf.keras.layers.Dense(20) def call(self, inputs): x = self.d1(inputs) return self.d2(x) model = SubclassModel() model(tf.zeros((10, 10))) weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': model.d1.kernel, # 'd1.bias': model.d1.bias, # 'd2.kernel': model.d2.kernel, # 'd2.bias': model.d2.bias, # } # Functional model inputs = tf.keras.Input((10,), batch_size=10) x = tf.keras.layers.Dense(20, name='d1')(inputs) output = tf.keras.layers.Dense(30, name='d2')(x) model = tf.keras.Model(inputs, output) d1 = model.layers[1] d2 = model.layers[2] weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': d1.kernel, # 'd1.bias': d1.bias, # 'd2.kernel': d2.kernel, # 'd2.bias': d2.bias, # } ``` \"\"\" result = {} ( descendants , object_paths_dict , ) = tf . __internal__ . tracking . ObjectGraphView ( self ). breadth_first_traversal () for descendant in descendants : if isinstance ( descendant , tf . Variable ) : trackable_references = object_paths_dict [ descendant ] object_path = \".\" . join ( [ t.name for t in trackable_references ] ) result [ object_path ] = descendant return result get_weights def get_weights ( self ) Retrieves the weights of the model. Returns: Type Description None A flat list of Numpy arrays. View Source def get_weights ( self ) : \"\" \"Retrieves the weights of the model. Returns: A flat list of Numpy arrays. \"\" \" with self.distribute_strategy.scope(): return super().get_weights() load_weights def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Type Description Default filepath None String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . None by_name None Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. None skip_mismatch None Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). None options None Optional tf.train.CheckpointOptions object that specifies options for loading weights. None Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ) ): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( \"Load weights is not implemented with TPUStrategy \" \"with `steps_per_run` greater than 1. The \" f \"`steps_per_run` is {spr}\" ) if skip_mismatch and not by_name : raise ValueError ( \"When calling model.load_weights, skip_mismatch can only be \" \"set to True when by_name is True.\" ) filepath , save_format = _detect_save_format ( filepath ) if save_format == \"tf\" : status = self . _checkpoint . read ( filepath , options ) if by_name : raise NotImplementedError ( \"Weights may only be loaded based on topology into Models \" \"when loading TensorFlow-formatted weights \" \"(got by_name=True to load_weights).\" ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( \"`load_weights` requires h5py package when loading weights \" \"from HDF5. Try installing h5py.\" ) if not self . _is_graph_network and not self . built : raise ValueError ( \"Unable to load weights saved in HDF5 format into a \" \"subclassed Model which has not created its variables yet. \" \"Call the Model first, then load the weights.\" ) self . _assert_weights_created () with h5py . File ( filepath , \"r\" ) as f : if \"layer_names\" not in f . attrs and \"model_weights\" in f : f = f [ \"model_weights\" ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status make_predict_function def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the predict function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = \"concat\" ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( outputs , tf . nest . map_structure ( lambda t : tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape , outputs , ), ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , reduce_retracing = True ) self . predict_function = predict_function return self . predict_function make_test_function def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the test function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) if self . _cluster_coordinator : self . test_function = ( lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it ,) ) ) else : self . test_function = test_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it , self . _steps_per_execution . value ()) ) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = test_function return self . test_function make_train_function def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the train function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = ( lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it ,) ) ) else : self . train_function = train_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it , self . _steps_per_execution . value ()) ) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function predict def predict ( self , x , batch_size = None , verbose = 'auto' , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Type Description Default x None Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None batch_size None Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None steps None Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = \"auto\" , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks]( https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( \"predict\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"predict\" ) self . _check_call_args ( \"predict\" ) _disallow_inside_tf_function ( \"predict\" ) # TODO(yashkatariya): Cache model on the coordinator for faster # prediction. If running under PSS, then swap it with OneDeviceStrategy # so that execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None verbose = _get_verbosity ( verbose , self . distribute_strategy ) outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy ) ) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = ( data_option ) x = x . with_options ( options ) except ValueError : warnings . warn ( \"Using Model.predict with MultiWorkerMirroredStrategy \" \"or TPUStrategy and AutoShardPolicy.FILE might lead to \" \"out-of-order result. Consider setting it to \" \"AutoShardPolicy.DATA.\" , stacklevel = 2 , ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = ( tmp_batch_outputs # No error, now safe to assign. ) if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs , ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs , ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { \"outputs\" : batch_outputs } ) if batch_outputs is None : raise ValueError ( \"Unexpected result of `predict_function` \" \"(Empty batch_outputs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , potentially_ragged_concat , outputs ) # If originally PSS strategy was used, then replace it back since # predict is running under `OneDeviceStrategy` after the swap and once # its done we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs ) predict_generator def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.predict_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.predict`, which supports generators.\" , stacklevel = 2 , ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , ) predict_on_batch def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\" \"Returns predictions for a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`. \"\" \" self . _check_call_args ( \"predict_on_batch\" ) _disallow_inside_tf_function ( \"predict_on_batch\" ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs ) predict_step def predict_step ( self , data ) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None The result of one inference step, typically the output of calling the Model on data. View Source def predict_step ( self , data ) : \" \"\" The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by `Model.make_predict_function`. This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_predict_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: The result of one inference step, typically the output of calling the `Model` on data. \"\" \" x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) return self ( x , training = False ) reset_metrics def reset_metrics ( self ) Resets the state of all the metrics in the model. View Source def reset_metrics ( self ) : \"\" \"Resets the state of all the metrics in the model. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model . compile ( optimizer = \"Adam\" , loss = \"mse\" , metrics = [ \"mae\" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\" \" for m in self.metrics: m.reset_state() reset_states def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , \"reset_states\" ) and getattr ( layer , \"stateful\" , False ) : layer . reset_states () save def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Parameters: Name Type Description Default filepath None String, PathLike, path to SavedModel or H5 file to save the model. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None include_optimizer None If True, save optimizer's state together. None save_format None Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. None signatures None Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. None options None (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. None save_traces None (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. None View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True , ) : \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide]( https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces , ) save_spec def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Type Description Default dynamic_batch None Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . None Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False ) save_weights def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Type Description Default filepath None String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None save_format None Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. None options None Optional tf.train.CheckpointOptions object that specifies options for saving weights. None Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints]( https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = io_utils . path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = \"h5\" else : save_format = \"tf\" else : user_format = save_format . lower () . strip () if user_format in ( \"tensorflow\" , \"tf\" ): save_format = \"tf\" elif user_format in ( \"hdf5\" , \"h5\" , \"keras\" ): save_format = \"h5\" else : raise ValueError ( f \"Unknown format. Received: `save_format`={save_format}. \" 'Was expecting one of {\"tf\", \"h5\"}.' ) if save_format == \"tf\" and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f \"filepath ({filepath}) looks like an HDF5 file. \" 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == \"h5\" and h5py is None : raise ImportError ( \"`save_weights` requires h5py when saving in hdf5, but h5py is \" \"not available. Try installing h5py package.\" ) if save_format == \"tf\" : check_filepath = filepath + \".index\" else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = io_utils . ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == \"h5\" : with h5py . File ( filepath , \"w\" ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if not tf . executing_eagerly (): # Call `get_session` to initialize any uninitialized variables. backend . get_session () self . _checkpoint . write ( filepath , options = options ) # Record this checkpoint so it's visible from # tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ], ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state () summary def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None ) Prints a string summary of the network. Parameters: Name Type Description Default line_length None Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). None positions None Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . None print_fn None Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. print expand_nested None Whether to expand the nested models. If not provided, defaults to False . None show_trainable None Whether to show if a layer is trainable. If not provided, defaults to False . None layer_range None a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to layer_range[0] and the end predicate will be the last element it matches to layer_range[1] . By default None which considers all layers of model. None Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None , ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. layer_range: a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to `layer_range[0]` and the end predicate will be the last element it matches to `layer_range[1]`. By default `None` which considers all layers of model. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( \"This model has not yet been built. \" \"Build the model first by calling `build()` or by calling \" \"the model on a batch of data.\" ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable , layer_range = layer_range , ) test_on_batch def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"test_on_batch\" ) _disallow_inside_tf_function ( \"test_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) test_step def test_step ( self , data ) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. View Source def test_step ( self , data ) : \" \"\" The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by `Model.make_test_function`. This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_test_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) # Updates stateful loss metrics. self . compute_loss ( x , y , y_pred , sample_weight ) return self . compute_metrics ( x , y , y_pred , sample_weight ) to_json def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to * json.dumps() . None Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to *`json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs ) to_yaml def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to yaml.dump() . None Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( \"Method `model.to_yaml()` has been removed due to security risk of \" \"arbitrary code execution. Please use `model.to_json()` instead.\" ) train_on_batch def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight None Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"train_on_batch\" ) _disallow_inside_tf_function ( \"train_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names ) train_step def train_step ( self , data ) The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see Customizing what happens in fit . This method is called by Model.make_train_function . This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def train_step ( self , data ) : \" \"\" The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see [Customizing what happens in fit]( https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This method is called by `Model.make_train_function`. This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_train_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) # Run forward pass. with tf . GradientTape () as tape : y_pred = self ( x , training = True ) loss = self . compute_loss ( x , y , y_pred , sample_weight ) self . _validate_target_and_loss ( y , loss ) # Run backwards pass. self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return self . compute_metrics ( x , y , y_pred , sample_weight )","title":"Mask"},{"location":"reference/grid_transformer/mask/#module-grid_transformermask","text":"This module contains utility functions for creating and manipulating masks for grid-like inputs. Functions: init_weights: Initialize weights for a tensor with a shape of shape and data type of dtype . take_left: Select the leftmost column of the tensor x. take_by_index: Select a column of the tensor x by index mix: Mix the leftmost and the middle column of the tensor x empty_last: Returns a tensor of zeros with the same shape as the last column of x random_mask: Generates a random mask with the same shape as the input tensor take_by_index: Take a slice of an array by index mix: Mix 2 slices of an array by index Example usage: # Initialize weights for a tensor of shape ( 3 , 3 , 3 ) weights = init_weights ( shape = ( 3 , 3 , 3 )) # Select the leftmost column of a tensor of shape ( batch_size , height , width , channels ) left_column = take_left ( x ) # Select a column of the tensor x by index column = take_by_index ( x , i = 4 ) # Mix the leftmost and the middle column of the tensor x mixed = mix ( x ) # Generates a random mask with the same shape as the input tensor mask = random_mask ( inputs , last_index = 5 ) # Take a slice of an array by index sliced = take_by_index ( x , i = 4 ) # Mix 2 slices of an array by index mixed = mix ( x ) View Source \"\"\" This module contains utility functions for creating and manipulating masks for grid-like inputs. Functions: - init_weights: Initialize weights for a tensor with a shape of `shape` and data type of `dtype`. - take_left: Select the leftmost column of the tensor x. - take_by_index: Select a column of the tensor x by index - mix: Mix the leftmost and the middle column of the tensor x - empty_last: Returns a tensor of zeros with the same shape as the last column of x - random_mask: Generates a random mask with the same shape as the input tensor - take_by_index: Take a slice of an array by index - mix: Mix 2 slices of an array by index Example usage: # Initialize weights for a tensor of shape (3,3,3) weights = init_weights(shape=(3,3,3)) # Select the leftmost column of a tensor of shape (batch_size, height, width, channels) left_column = take_left(x) # Select a column of the tensor x by index column = take_by_index(x, i=4) # Mix the leftmost and the middle column of the tensor x mixed = mix(x) # Generates a random mask with the same shape as the input tensor mask = random_mask(inputs, last_index=5) # Take a slice of an array by index sliced = take_by_index(x, i=4) # Mix 2 slices of an array by index mixed = mix(x) \"\"\" from typing import Tuple , Callable , Optional , Type , Union import tensorflow as tf from tensorflow . keras import Model from models_utils import ops as K def init_weights ( shape : Tuple [ int ], dtype : tf . DType = tf . float32 ) -> tf . Tensor : \"\"\" Initialize weights for a tensor with a shape of `shape` and data type of `dtype` :param shape: Tuple of integers representing the shape of the tensor. :param dtype: Data type of the tensor. Default is tf.float32 :return: Tensor with the initialized weights \"\"\" return tf . cast ( K . init . image ( shape = shape , pre = True ), dtype = dtype ) def take_left ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Select the leftmost column of the tensor x :param x: Tensor of shape (batch_size, height, width, channels) :return: Tensor of shape (batch_size, height, 1, channels) \"\"\" return x [ ... , 7 : 8 ] def take_by_index ( x : tf . Tensor , i : int = 8 ) -> tf . Tensor : \"\"\" Select a column of the tensor x by index :param x: Tensor of shape (batch_size, height, width, channels) :param i: Index of the column to select, Default is 8 :return: Tensor of shape (batch_size, height, 1, channels) \"\"\" return x [ ... , i : i + 1 ] def mix ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Mix the leftmost and the middle column of the tensor x :param x: Tensor of shape (batch_size, height, width, channels) :return: Tensor of shape (batch_size, height, 1, channels) \"\"\" return ( x [ ... , 7 : 8 ] + x [ ... , 5 : 6 ]) / 2 def empty_last ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Returns a tensor of zeros with the same shape as the last column of x :param x: Tensor of shape (batch_size, height, width, channels) :return: Tensor of shape (batch_size, height, 1, channels) filled with zeros \"\"\" return tf . zeros_like ( x [ ... , 7 : 8 ]) def random_mask ( inputs : tf . Tensor , last_index : int ) -> tf . Tensor : \"\"\" Generates a random mask with the same shape as the input tensor. Each element in the mask is a boolean value indicating whether or not the corresponding element in the input tensor should be kept. :param inputs: Tensor to generate a mask for. :param last_index: The last index to choose from when generating the mask. :return: Boolean tensor of the same shape as the input tensor, indicating which elements should be kept. \"\"\" shape = tf . shape ( inputs ) indexes = tf . random . uniform ( shape = shape [ 0 : 1 ], maxval = last_index , dtype = tf . int32 ) mask = tf . cast ( tf . one_hot ( indexes , last_index ), dtype = 'bool' ) return mask def take_by_index ( x : tf . Tensor , i : int = 8 ) -> tf . Tensor : \"\"\" Take a slice of an array by index. Parameters: x (tf.Tensor): The input array. i (int, optional): The index of the slice. Default is 8. Returns: tf.Tensor: The sliced array. \"\"\" return x [ ... , i : i + 1 ] def mix ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Mix 2 slices of an array by index. Parameters: x (tf.Tensor): The input array. Returns: tf.Tensor: The mixed array. \"\"\" return ( x [ ... , 7 : 8 ] + x [ ... , 5 : 6 ]) / 2 def empty_last ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Return a tensor of zeros with the shape of the last slice of input tensor. Parameters: x (tf.Tensor): The input tensor. Returns: tf.Tensor: The tensor of zeros. \"\"\" return tf . zeros_like ( x [ ... , 7 : 8 ]) def random_mask ( inputs : tf . Tensor , last_index : int ) -> tf . Tensor : \"\"\" Generate a random mask of the same shape as inputs. Parameters: inputs (tf.Tensor): The input tensor. last_index (int): The last index to use for generating the mask. Returns: tf.Tensor: A boolean tensor of the same shape as inputs, with random values of True or False. \"\"\" shape = tf . shape ( inputs ) indexes = tf . random . uniform ( shape = shape [ 0 : 1 ], maxval = last_index , dtype = tf . int32 ) mask = tf . cast ( tf . one_hot ( indexes , last_index ), dtype = 'bool' ) return mask def constant_mask ( inputs : tf . Tensor , value : int = 8 , last_index : Optional [ int ] = None ) -> tf . Tensor : \"\"\" Generate a constant mask of the same shape as inputs. The mask will have the same value in all positions. Parameters: inputs (tf.Tensor): The input tensor. value (int, optional): The value to fill the mask with. Default is 8. last_index (int, optional): The last index to use for generating the mask. If not provided, it will be set to value + 1. Returns: tf.Tensor: A boolean tensor of the same shape as inputs, with constant value of True or False. \"\"\" shape = tf . shape ( inputs ) indexes = tf . fill ( dims = shape [ 0 : 1 ], value = value ) mask = tf . cast ( tf . one_hot ( indexes , last_index if last_index else value + 1 ), dtype = 'bool' ) return mask class ImageMask ( tf . keras . Model ): def __init__ ( self , last : Callable [[ tf . Tensor ], tf . Tensor ], last_index : Optional [ int ] = None ): \"\"\" Class that applies a mask to an image. Parameters: last (Callable[[tf.Tensor], tf.Tensor]): A callable that takes an image tensor and returns the last slice. last_index (int, optional): The last index of the image. If not provided, it will be set to the last index of the image. \"\"\" super () . __init__ () self . get_last = last self . last_index = last_index def call ( self , inputs : Tuple [ tf . Tensor , tf . Tensor ]) -> tf . Tensor : \"\"\" Applies the mask to the image. Parameters: inputs (Tuple[tf.Tensor, tf.Tensor]): A tuple containing the image tensor and the mask tensor. Returns: tf.Tensor: The masked image. \"\"\" inputs , mask = inputs mask = mask [:, None , None ] inputs = inputs [ ... , : self . last_index ] return tf . cast (( 1 - mask ) * inputs + mask * tf . tile ( self . get_last ( inputs )[ None ], ( 1 , 1 , 1 , inputs . shape [ - 1 ])), dtype = 'float32' )","title":"Module grid_transformer.mask"},{"location":"reference/grid_transformer/mask/#functions","text":"","title":"Functions"},{"location":"reference/grid_transformer/mask/#constant_mask","text":"def constant_mask ( inputs : tensorflow . python . framework . ops . Tensor , value : int = 8 , last_index : Optional [ int ] = None ) -> tensorflow . python . framework . ops . Tensor Generate a constant mask of the same shape as inputs. The mask will have the same value in all positions. Parameters: Name Type Description Default inputs tf.Tensor The input tensor. None value int The value to fill the mask with. Default is 8. None last_index int The last index to use for generating the mask. If not provided, it will be set to value + 1. None Returns: Type Description tf.Tensor A boolean tensor of the same shape as inputs, with constant value of True or False. View Source def constant_mask ( inputs : tf . Tensor , value : int = 8 , last_index : Optional [ int ] = None ) -> tf . Tensor : \"\"\" Generate a constant mask of the same shape as inputs. The mask will have the same value in all positions. Parameters: inputs (tf.Tensor): The input tensor. value (int, optional): The value to fill the mask with. Default is 8. last_index (int, optional): The last index to use for generating the mask. If not provided, it will be set to value + 1. Returns: tf.Tensor: A boolean tensor of the same shape as inputs, with constant value of True or False. \"\"\" shape = tf . shape ( inputs ) indexes = tf . fill ( dims = shape [ 0:1 ] , value = value ) mask = tf . cast ( tf . one_hot ( indexes , last_index if last_index else value + 1 ), dtype = 'bool' ) return mask","title":"constant_mask"},{"location":"reference/grid_transformer/mask/#empty_last","text":"def empty_last ( x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Return a tensor of zeros with the shape of the last slice of input tensor. Parameters: Name Type Description Default x tf.Tensor The input tensor. None Returns: Type Description tf.Tensor The tensor of zeros. View Source def empty_last ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Return a tensor of zeros with the shape of the last slice of input tensor. Parameters: x (tf.Tensor): The input tensor. Returns: tf.Tensor: The tensor of zeros. \"\"\" return tf . zeros_like ( x [..., 7 : 8 ])","title":"empty_last"},{"location":"reference/grid_transformer/mask/#init_weights","text":"def init_weights ( shape : Tuple [ int ], dtype : tensorflow . python . framework . dtypes . DType = tf . float32 ) -> tensorflow . python . framework . ops . Tensor Initialize weights for a tensor with a shape of shape and data type of dtype Parameters: Name Type Description Default shape None Tuple of integers representing the shape of the tensor. None dtype None Data type of the tensor. Default is tf.float32 None Returns: Type Description None Tensor with the initialized weights View Source def init_weights ( shape : Tuple [ int ] , dtype : tf . DType = tf . float32 ) -> tf . Tensor : \" \"\" Initialize weights for a tensor with a shape of `shape` and data type of `dtype` :param shape: Tuple of integers representing the shape of the tensor. :param dtype: Data type of the tensor. Default is tf.float32 :return: Tensor with the initialized weights \"\" \" return tf . cast ( K . init . image ( shape = shape , pre = True ), dtype = dtype )","title":"init_weights"},{"location":"reference/grid_transformer/mask/#mix","text":"def mix ( x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Mix 2 slices of an array by index. Parameters: Name Type Description Default x tf.Tensor The input array. None Returns: Type Description tf.Tensor The mixed array. View Source def mix ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Mix 2 slices of an array by index. Parameters: x (tf.Tensor): The input array. Returns: tf.Tensor: The mixed array. \"\"\" return ( x [..., 7 : 8 ] + x [..., 5 : 6 ]) / 2","title":"mix"},{"location":"reference/grid_transformer/mask/#random_mask","text":"def random_mask ( inputs : tensorflow . python . framework . ops . Tensor , last_index : int ) -> tensorflow . python . framework . ops . Tensor Generate a random mask of the same shape as inputs. Parameters: Name Type Description Default inputs tf.Tensor The input tensor. None last_index int The last index to use for generating the mask. None Returns: Type Description tf.Tensor A boolean tensor of the same shape as inputs, with random values of True or False. View Source def random_mask ( inputs : tf . Tensor , last_index : int ) -> tf . Tensor : \"\"\" Generate a random mask of the same shape as inputs. Parameters: inputs (tf.Tensor): The input tensor. last_index (int): The last index to use for generating the mask. Returns: tf.Tensor: A boolean tensor of the same shape as inputs, with random values of True or False. \"\"\" shape = tf . shape ( inputs ) indexes = tf . random . uniform ( shape = shape [ 0 : 1 ], maxval = last_index , dtype = tf . int32 ) mask = tf . cast ( tf . one_hot ( indexes , last_index ), dtype = 'bool' ) return mask","title":"random_mask"},{"location":"reference/grid_transformer/mask/#take_by_index","text":"def take_by_index ( x : tensorflow . python . framework . ops . Tensor , i : int = 8 ) -> tensorflow . python . framework . ops . Tensor Take a slice of an array by index. Parameters: Name Type Description Default x tf.Tensor The input array. None i int The index of the slice. Default is 8. None Returns: Type Description tf.Tensor The sliced array. View Source def take_by_index ( x : tf . Tensor , i : int = 8 ) -> tf . Tensor : \"\"\" Take a slice of an array by index. Parameters: x (tf.Tensor): The input array. i (int, optional): The index of the slice. Default is 8. Returns: tf.Tensor: The sliced array. \"\"\" return x [..., i : i + 1 ]","title":"take_by_index"},{"location":"reference/grid_transformer/mask/#take_left","text":"def take_left ( x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Select the leftmost column of the tensor x Parameters: Name Type Description Default x None Tensor of shape (batch_size, height, width, channels) None Returns: Type Description None Tensor of shape (batch_size, height, 1, channels) View Source def take_left ( x : tf . Tensor ) -> tf . Tensor : \"\"\" Select the leftmost column of the tensor x :param x: Tensor of shape (batch_size, height, width, channels) :return: Tensor of shape (batch_size, height, 1, channels) \"\"\" return x [..., 7 : 8 ]","title":"take_left"},{"location":"reference/grid_transformer/mask/#classes","text":"","title":"Classes"},{"location":"reference/grid_transformer/mask/#imagemask","text":"class ImageMask ( last : Callable [[ tensorflow . python . framework . ops . Tensor ], tensorflow . python . framework . ops . Tensor ], last_index : Optional [ int ] = None ) Model groups layers into an object with training and inference features.","title":"ImageMask"},{"location":"reference/grid_transformer/mask/#attributes","text":"Name Type Description Default inputs None The input(s) of the model: a keras.Input object or a combination of keras.Input objects in a dict, list or tuple. None outputs None The output(s) of the model: a tensor that originated from keras.Input objects or a combination of such tensors in a dict, list or tuple. See Functional API example below. None name None String, the name of the model. None View Source class ImageMask ( tf . keras . Model ) : def __init__ ( self , last : Callable [ [tf.Tensor ] , tf . Tensor ] , last_index : Optional [ int ] = None ) : \"\"\" Class that applies a mask to an image. Parameters: last (Callable[[tf.Tensor], tf.Tensor]): A callable that takes an image tensor and returns the last slice. last_index (int, optional): The last index of the image. If not provided, it will be set to the last index of the image. \"\"\" super (). __init__ () self . get_last = last self . last_index = last_index def call ( self , inputs : Tuple [ tf.Tensor, tf.Tensor ] ) -> tf . Tensor : \"\"\" Applies the mask to the image. Parameters: inputs (Tuple[tf.Tensor, tf.Tensor]): A tuple containing the image tensor and the mask tensor. Returns: tf.Tensor: The masked image. \"\"\" inputs , mask = inputs mask = mask [ :, None, None ] inputs = inputs [ ..., :self.last_index ] return tf . cast (( 1 - mask ) * inputs + mask * tf . tile ( self . get_last ( inputs ) [ None ] , ( 1 , 1 , 1 , inputs . shape [ -1 ] )), dtype = 'float32' )","title":"Attributes"},{"location":"reference/grid_transformer/mask/#ancestors-in-mro","text":"keras.engine.training.Model keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/mask/#static-methods","text":"","title":"Static methods"},{"location":"reference/grid_transformer/mask/#from_config","text":"def from_config ( config , custom_objects = None ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config , custom_objects = None ): compile_config = config . pop ( \"compile_config\" , None ) build_input_shape = config . pop ( \"build_input_shape\" , None ) # `from_config` assumes `cls` is either `Functional` or a child class of # `Functional`. In the case that `cls` is meant to behave like a child # class of `Functional` but only inherits from the `Model` class, we # have to call `cls(...)` instead of `Functional.from_config`. from keras.engine import functional with serialization . SharedObjectLoadingScope (): functional_model_keys = [ \"name\" , \"layers\" , \"input_layers\" , \"output_layers\" , ] if all ( key in config for key in functional_model_keys ): inputs , outputs , layers = functional . reconstruct_from_config ( config , custom_objects ) model = cls ( inputs = inputs , outputs = outputs , name = config . get ( \"name\" ) ) functional . connect_ancillary_layers ( model , layers ) else : # The config does not contain all the information necessary to # revive a Functional model. This happens when the user creates # subclassed models where `get_config()` is returning # insufficient information to be considered a Functional model. # In this case, we fall back to provide all config into the # constructor of the class. try : model = cls ( ** config ) except TypeError as e : raise TypeError ( \"Unable to revive model from config. When overriding \" \"the `get_config()`, make sure that the returned \" \"config contains all items used as arguments in the \" f \"constructor to { cls } , which is the default behavior. \" \"You can override this default behavior by defining a \" \"`from_config` method to specify how to create an \" f \"instance of { cls . __name__ } from the config. \\n\\n \" f \"Error encountered during deserialization: \\n { e } \" ) if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ): if build_input_shape : model . build ( build_input_shape ) if compile_config is not None : model . _compile_from_config ( compile_config , base_class = Model ) return model","title":"from_config"},{"location":"reference/grid_transformer/mask/#with_name_scope","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/mask/#instance-variables","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. distribute_reduction_method The method employed to reduce per-replica values during training. Unless specified, the value \"auto\" will be assumed, indicating that the reduction strategy should be chosen based on the current running environment. See reduce_per_replica function for more details. distribute_strategy The tf.distribute.Strategy this model was created under. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . layers losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics Returns the model's metrics added using compile() , add_metric() APIs. Note: Metrics passed to compile() are available only after a keras.Model has been trained/evaluated on actual data. metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. state_updates Deprecated, do NOT use! Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers.","title":"Instance variables"},{"location":"reference/grid_transformer/mask/#methods","text":"","title":"Methods"},{"location":"reference/grid_transformer/mask/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/mask/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/mask/#add_update","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/mask/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/mask/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/mask/#build","text":"def build ( self , input_shape ) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape instance, or list/dict of shapes, where shapes are tuples, integers, or TensorShape instances. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, TensorShape , or dict). 2. If the model requires call arguments that are agnostic to the input shapes (positional or keyword arg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. View Source @generic_utils.default def build ( self , input_shape ) : \" \"\" Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call `model.build()` in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, `TensorShape` instance, or list/dict of shapes, where shapes are tuples, integers, or `TensorShape` instances. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, `TensorShape`, or dict). 2. If the model requires call arguments that are agnostic to the input shapes (positional or keyword arg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. \"\" \" if self . _is_graph_network : super (). build ( input_shape ) return if input_shape is None : raise ValueError ( \"Input shape must be defined when calling `build()` on \" \"a `Model` subclass.\" ) valid_types = ( tuple , list , tf . TensorShape , dict ) if not isinstance ( input_shape , valid_types ) : raise ValueError ( \"Specified input shape is not one of the valid types. \" \"Please specify a batch input shape of type tuple or \" \"list of input shapes. User provided \" \"input type: {}.\" . format ( type ( input_shape )) ) if input_shape and not self . inputs : # We create placeholders for the `None`s in the shape and build the # model in a Graph. Since tf.Variable is compatible with both eager # execution and graph building, the variables created after building # the model in a Graph are still valid when executing eagerly. if tf . executing_eagerly () : graph = tf . __internal__ . FuncGraph ( \"build_graph\" ) else : graph = backend . get_graph () with graph . as_default () : if isinstance ( input_shape , list ) and all ( d is None or isinstance ( d , int ) for d in input_shape ) : input_shape = tuple ( input_shape ) if isinstance ( input_shape , list ) : x = [ base_layer_utils . generate_placeholders_from_shape ( shape ) for shape in input_shape ] elif isinstance ( input_shape , dict ) : x = { k : base_layer_utils . generate_placeholders_from_shape ( shape ) for k , shape in input_shape . items () } else : x = base_layer_utils . generate_placeholders_from_shape ( input_shape ) kwargs = {} call_signature = self . _call_spec . full_argspec call_args = call_signature . args # Exclude `self`, `inputs`, and any argument with a default # value. if len ( call_args ) > 2 : if call_signature . defaults : call_args = call_args [ 2 : - len ( call_signature . defaults ) ] else : call_args = call_args [ 2 : ] for arg in call_args : if arg == \"training\" : # Case where `training` is a positional arg with no # default. kwargs [ \"training\" ] = False else : # Has invalid call signature with unknown positional # arguments. raise ValueError ( \"Currently, you cannot build your model if it \" \"has positional or keyword arguments that are \" \"not inputs to the model, but are required for \" \"its `call()` method. Instead, in order to \" \"instantiate and build your model, `call()` \" \"your model on real tensor data with all \" \"expected call arguments. The argument \" \"for `call()` can be a single list/tuple that \" \"contains multiple inputs.\" ) elif len ( call_args ) < 2 : # Signature without `inputs`. raise ValueError ( \"You can only call `build()` on a model if its \" \"`call()` method accepts an `inputs` argument.\" ) try : self . call ( x , ** kwargs ) except ( tf . errors . InvalidArgumentError , TypeError ) as e : raise ValueError ( \"You cannot build your model by calling `build` \" \"if your layers do not support float type inputs. \" \"Instead, in order to instantiate and build your \" \"model, call your model on real tensor data (of \" \"the correct dtype). \\n\\n The actual error from \" f \"`call` is: {e}.\" ) super (). build ( input_shape )","title":"build"},{"location":"reference/grid_transformer/mask/#call","text":"def call ( self , inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ] ) -> tensorflow . python . framework . ops . Tensor Applies the mask to the image. Parameters: Name Type Description Default inputs Tuple[tf.Tensor, tf.Tensor] A tuple containing the image tensor and the mask tensor. None Returns: Type Description tf.Tensor The masked image. View Source def call ( self , inputs : Tuple [ tf.Tensor, tf.Tensor ] ) -> tf . Tensor : \"\"\" Applies the mask to the image. Parameters: inputs (Tuple[tf.Tensor, tf.Tensor]): A tuple containing the image tensor and the mask tensor. Returns: tf.Tensor: The masked image. \"\"\" inputs , mask = inputs mask = mask [ :, None, None ] inputs = inputs [ ..., :self.last_index ] return tf . cast (( 1 - mask ) * inputs + mask * tf . tile ( self . get_last ( inputs ) [ None ] , ( 1 , 1 , 1 , inputs . shape [ -1 ] )), dtype = 'float32' )","title":"call"},{"location":"reference/grid_transformer/mask/#compile","text":"def compile ( self , optimizer = 'rmsprop' , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs ) Configures the model for training. Parameters: Name Type Description Default optimizer None String (name of optimizer) or optimizer instance. See tf.keras.optimizers . None loss None Loss function. May be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses . A loss function is any callable with the signature loss = fn(y_true,<br>y_pred) , where y_true are the ground truth values, and y_pred are the model's predictions. y_true should have shape (batch_size, d0, .. dN) (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape (batch_size, d0, .. dN-1) ). y_pred should have shape (batch_size, d0, .. dN) . The loss function should return a float tensor. If a custom Loss instance is used and reduction is set to None , return value has shape (batch_size, d0, .. dN-1) i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified. None metrics None List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true,<br>y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']} . You can also pass a list to specify a metric or a list of metrics for each output, such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the shapes of the targets and of the model output. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. The metrics passed here are evaluated without sample weighting; if you would like sample weighting to apply, you can specify your metrics via the weighted_metrics argument instead. None loss_weights None Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. None weighted_metrics None List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. None run_eagerly None Bool. Defaults to False . If True , this Model 's logic will not be wrapped in a tf.function . Recommended to leave this as None unless your Model cannot be run inside a tf.function . run_eagerly=True is not supported when using tf.distribute.experimental.ParameterServerStrategy . False steps_per_execution None Int. Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if steps_per_execution is set to N , Callback.on_batch_begin and Callback.on_batch_end methods will only be called every N batches (i.e. before/after each tf.function execution). 1 jit_compile None If True , compile the model training step with XLA. XLA is an optimizing compiler for machine learning. jit_compile is not enabled for by default. This option cannot be enabled with run_eagerly=True . Note that jit_compile=True may not necessarily work for all models. For more information on supported operations please refer to the XLA documentation . Also refer to known XLA issues for more details. None **kwargs None Arguments supported for backwards compatibility only. None View Source @ traceback_utils . filter_traceback def compile ( self , optimizer = \"rmsprop\" , loss = None , metrics = None , loss_weights = None , weighted_metrics = None , run_eagerly = None , steps_per_execution = None , jit_compile = None , ** kwargs , ) : \"\"\"Configures the model for training. Example : ``` python model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = 1e-3 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . FalseNegatives ()]) ``` Args : optimizer : String ( name of optimizer ) or optimizer instance . See ` tf . keras . optimizers ` . loss : Loss function . May be a string ( name of loss function ), or a ` tf . keras . losses . Loss ` instance . See ` tf . keras . losses ` . A loss function is any callable with the signature ` loss = fn ( y_true , y_pred ) ` , where ` y_true ` are the ground truth values , and ` y_pred ` are the model ' s predictions . ` y_true ` should have shape ` ( batch_size , d0 , .. dN ) ` ( except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape ` ( batch_size , d0 , .. dN -1 ) ` ). ` y_pred ` should have shape ` ( batch_size , d0 , .. dN ) ` . The loss function should return a float tensor . If a custom ` Loss ` instance is used and reduction is set to ` None ` , return value has shape ` ( batch_size , d0 , .. dN -1 ) ` i . e . per - sample or per - timestep loss values ; otherwise , it is a scalar . If the model has multiple outputs , you can use a different loss on each output by passing a dictionary or a list of losses . The loss value that will be minimized by the model will then be the sum of all individual losses , unless ` loss_weights ` is specified . metrics : List of metrics to be evaluated by the model during training and testing . Each of this can be a string ( name of a built - in function ), function or a ` tf . keras . metrics . Metric ` instance . See ` tf . keras . metrics ` . Typically you will use ` metrics = [ ' accuracy ' ] ` . A function is any callable with the signature ` result = fn ( y_true , y_pred ) ` . To specify different metrics for different outputs of a multi - output model , you could also pass a dictionary , such as ` metrics = {' output_a ':' accuracy ' , ' output_b ' :[ ' accuracy ' , ' mse ' ]} ` . You can also pass a list to specify a metric or a list of metrics for each output , such as ` metrics = [[ ' accuracy ' ], [ ' accuracy ' , ' mse ' ]] ` or ` metrics = [ ' accuracy ' , [ ' accuracy ' , ' mse ' ]] ` . When you pass the strings ' accuracy ' or ' acc ' , we convert this to one of ` tf . keras . metrics . BinaryAccuracy ` , ` tf . keras . metrics . CategoricalAccuracy ` , ` tf . keras . metrics . SparseCategoricalAccuracy ` based on the shapes of the targets and of the model output . We do a similar conversion for the strings ' crossentropy ' and ' ce ' as well . The metrics passed here are evaluated without sample weighting ; if you would like sample weighting to apply , you can specify your metrics via the ` weighted_metrics ` argument instead . loss_weights : Optional list or dictionary specifying scalar coefficients ( Python floats ) to weight the loss contributions of different model outputs . The loss value that will be minimized by the model will then be the * weighted sum * of all individual losses , weighted by the ` loss_weights ` coefficients . If a list , it is expected to have a 1 : 1 mapping to the model ' s outputs . If a dict , it is expected to map output names ( strings ) to scalar coefficients . weighted_metrics : List of metrics to be evaluated and weighted by ` sample_weight ` or ` class_weight ` during training and testing . run_eagerly : Bool . Defaults to ` False ` . If ` True ` , this ` Model `' s logic will not be wrapped in a ` tf . function ` . Recommended to leave this as ` None ` unless your ` Model ` cannot be run inside a ` tf . function ` . ` run_eagerly = True ` is not supported when using ` tf . distribute . experimental . ParameterServerStrategy ` . steps_per_execution : Int . Defaults to 1. The number of batches to run during each ` tf . function ` call . Running multiple batches inside a single ` tf . function ` call can greatly improve performance on TPUs or small models with a large Python overhead . At most , one full epoch will be run each execution . If a number larger than the size of the epoch is passed , the execution will be truncated to the size of the epoch . Note that if ` steps_per_execution ` is set to ` N ` , ` Callback . on_batch_begin ` and ` Callback . on_batch_end ` methods will only be called every ` N ` batches ( i . e . before / after each ` tf . function ` execution ). jit_compile : If ` True ` , compile the model training step with XLA . [ XLA ]( https : //www.tensorflow.org/xla) is an optimizing compiler for machine learning . ` jit_compile ` is not enabled for by default . This option cannot be enabled with ` run_eagerly = True ` . Note that ` jit_compile = True ` may not necessarily work for all models . For more information on supported operations please refer to the [ XLA documentation ]( https : //www.tensorflow.org/xla). Also refer to [ known XLA issues ]( https : //www.tensorflow.org/xla/known_issues) for more details . ** kwargs : Arguments supported for backwards compatibility only . \"\"\" base_layer . keras_api_gauge . get_cell ( \"compile\" ). set ( True ) self . _compile_config = generic_utils . Config ( optimizer = optimizer , loss = loss , metrics = metrics , loss_weights = loss_weights , weighted_metrics = weighted_metrics , run_eagerly = run_eagerly , steps_per_execution = steps_per_execution , jit_compile = jit_compile , ) with self . distribute_strategy . scope () : if \"experimental_steps_per_execution\" in kwargs : logging . warning ( \"The argument `steps_per_execution` is no longer \" \"experimental. Pass `steps_per_execution` instead of \" \"`experimental_steps_per_execution`.\" ) if not steps_per_execution : steps_per_execution = kwargs . pop ( \"experimental_steps_per_execution\" ) # When compiling from an already-serialized model, we do not want to # reapply some processing steps (e.g. metric renaming for # multi-output models, which have prefixes added for each # corresponding output name). from_serialized = kwargs . pop ( \"from_serialized\" , False ) self . _validate_compile ( optimizer , metrics , ** kwargs ) self . _run_eagerly = run_eagerly self . optimizer = self . _get_optimizer ( optimizer ) if isinstance ( loss , compile_utils . LossesContainer ) : self . compiled_loss = loss else : self . compiled_loss = compile_utils . LossesContainer ( loss , loss_weights , output_names = self . output_names ) self . compiled_metrics = compile_utils . MetricsContainer ( metrics , weighted_metrics , output_names = self . output_names , from_serialized = from_serialized , ) self . _configure_steps_per_execution ( steps_per_execution or 1 ) # Initializes attrs that are reset each time `compile` is called. self . _reset_compile_cache () self . _is_compiled = True self . loss = loss or {} if ( self . _run_eagerly or self . dynamic ) and jit_compile : raise ValueError ( \"You cannot enable `run_eagerly` and `jit_compile` \" \"at the same time.\" ) else : self . _jit_compile = jit_compile","title":"compile"},{"location":"reference/grid_transformer/mask/#compute_loss","text":"def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None The total loss as a tf.Tensor , or None if no loss results (which is the case when called by Model.test_step ). View Source def compute_loss ( self , x = None , y = None , y_pred = None , sample_weight = None ) : \" \"\" Compute the total loss, validate it, and return it. Subclasses can optionally override this method to provide custom loss computation logic. Example: ```python class MyModel(tf.keras.Model): def __init__(self, *args, **kwargs): super(MyModel, self).__init__(*args, **kwargs) self.loss_tracker = tf.keras.metrics.Mean(name='loss') def compute_loss(self, x, y, y_pred, sample_weight): loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y)) loss += tf.add_n(self.losses) self.loss_tracker.update_state(loss) return loss def reset_metrics(self): self.loss_tracker.reset_states() @property def metrics(self): return [self.loss_tracker] tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,)) dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1) inputs = tf.keras.layers.Input(shape=(10,), name='my_input') outputs = tf.keras.layers.Dense(10)(inputs) model = MyModel(inputs, outputs) model.add_loss(tf.reduce_sum(outputs)) optimizer = tf.keras.optimizers.SGD() model.compile(optimizer, loss='mse', steps_per_execution=10) model.fit(dataset, epochs=2, steps_per_epoch=10) print('My custom loss: ', model.loss_tracker.result().numpy()) ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model(x)`) sample_weight: Sample weights for weighting the loss function. Returns: The total loss as a `tf.Tensor`, or `None` if no loss results (which is the case when called by `Model.test_step`). \"\" \" del x # The default implementation does not use `x`. return self . compiled_loss ( y , y_pred , sample_weight , regularization_losses = self . losses )","title":"compute_loss"},{"location":"reference/grid_transformer/mask/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/mask/#compute_metrics","text":"def compute_metrics ( self , x , y , y_pred , sample_weight ) Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Parameters: Name Type Description Default x None Input data. None y None Target data. None y_pred None Predictions returned by the model (output of model.call(x) ) None sample_weight None Sample weights for weighting the loss function. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end() . Typically, the values of the metrics listed in self.metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def compute_metrics ( self , x , y , y_pred , sample_weight ) : \" \"\" Update metric states and collect all metrics to be returned. Subclasses can optionally override this method to provide custom metric updating and collection logic. Example: ```python class MyModel(tf.keras.Sequential): def compute_metrics(self, x, y, y_pred, sample_weight): # This super call updates `self.compiled_metrics` and returns # results for all metrics listed in `self.metrics`. metric_results = super(MyModel, self).compute_metrics( x, y, y_pred, sample_weight) # Note that `self.custom_metric` is not listed in `self.metrics`. self.custom_metric.update_state(x, y, y_pred, sample_weight) metric_results['custom_metric_name'] = self.custom_metric.result() return metric_results ``` Args: x: Input data. y: Target data. y_pred: Predictions returned by the model (output of `model.call(x)`) sample_weight: Sample weights for weighting the loss function. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the values of the metrics listed in `self.metrics` are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" del x # The default implementation does not use `x`. self . compiled_metrics . update_state ( y , y_pred , sample_weight ) return self . get_metrics_result ()","title":"compute_metrics"},{"location":"reference/grid_transformer/mask/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/mask/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/mask/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/mask/#evaluate","text":"def evaluate ( self , x = None , y = None , batch_size = None , verbose = 'auto' , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the batch_size arg.) Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs,<br> targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). None batch_size None Integer or None . Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None sample_weight None Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples,<br> sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . None steps None Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None **kwargs None Unused at this time. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.evaluate is wrapped in a tf.function . View Source @traceback_utils.filter_traceback def evaluate ( self , x = None , y = None , batch_size = None , verbose = \"auto\" , sample_weight = None , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , return_dict = False , ** kwargs , ) : \" \"\" Returns the loss value & metrics values for the model in test mode. Computation is done in batches (see the `batch_size` arg.) Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. **kwargs: Unused at this time. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.evaluate` is wrapped in a `tf.function`. \"\" \" base_layer . keras_api_gauge . get_cell ( \"evaluate\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"evaluate\" ) self . _assert_compile_was_called () self . _check_call_args ( \"evaluate\" ) self . _check_sample_weight_warning ( x , sample_weight ) _disallow_inside_tf_function ( \"evaluate\" ) use_cached_eval_dataset = kwargs . pop ( \"_use_cached_eval_dataset\" , False ) if kwargs : raise TypeError ( f \"Invalid keyword arguments: {list(kwargs.keys())}\" ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) with self . distribute_strategy . scope () : # Use cached evaluation data only when it's called in `Model.fit` if ( use_cached_eval_dataset and getattr ( self , \"_eval_data_handler\" , None ) is not None ) : data_handler = self . _eval_data_handler else : # Creates a `tf.data.Dataset` and handles batch and epoch # iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) logs = {} self . test_function = self . make_test_function () self . _test_counter . assign ( 0 ) callbacks . on_test_begin () for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. self . reset_metrics () with data_handler . catch_stop_iteration () : for step in data_handler . steps () : with tf . profiler . experimental . Trace ( \"test\" , step_num = step , _r = 1 ) : callbacks . on_test_batch_begin ( step ) tmp_logs = self . test_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_test_batch_end ( end_step , logs ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) callbacks . on_test_end ( logs = logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"evaluate"},{"location":"reference/grid_transformer/mask/#evaluate_generator","text":"def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Evaluates the model on a data generator. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def evaluate_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Evaluates the model on a data generator. DEPRECATED: `Model.evaluate` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.evaluate_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.evaluate`, which supports generators.\" , stacklevel = 2 , ) self . _check_call_args ( \"evaluate_generator\" ) return self . evaluate ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , )","title":"evaluate_generator"},{"location":"reference/grid_transformer/mask/#finalize_state","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/mask/#fit","text":"def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 'auto' , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . - A tf.keras.utils.experimental.DatasetCreator , which wraps a callable that takes a single argument of type tf.distribute.InputContext , and returns a tf.data.Dataset . DatasetCreator should be used when users prefer to specify the per-replica batching and sharding logic for the Dataset . See tf.keras.utils.experimental.DatasetCreator doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include sample_weights as a third component, note that sample weighting applies to the weighted_metrics argument but not the metrics argument in compile() . If using tf.distribute.experimental.ParameterServerStrategy , only DatasetCreator type is supported for x . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . Note tf.keras.callbacks.ProgbarLogger and tf.keras.callbacks.History callbacks are created automatically and need not be passed into model.fit . tf.keras.callbacks.ProgbarLogger is created or not based on verbose argument to model.fit . Callbacks with batch-level calls are currently unsupported with tf.distribute.experimental.ParameterServerStrategy , and users are advised to implement epoch-level calls instead with an appropriate steps_per_epoch value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. If both validation_data and validation_split are provided, validation_data will override validation_split . validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy . validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using validation_split or validation_data is not affected by regularization layers like noise and dropout. validation_data will override validation_split . validation_data could be: - A tuple (x_val, y_val) of Numpy arrays or tensors. - A tuple (x_val, y_val, val_sample_weights) of NumPy arrays. - A tf.data.Dataset . - A Python generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . validation_data is not yet supported with tf.distribute.experimental.ParameterServerStrategy . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . Note that sample weighting does not apply to metrics specified via the metrics argument in compile() . To apply sample weighting to your metrics, you can specify them via the weighted_metrics in compile() instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. If steps_per_epoch=-1 the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using tf.distribute.experimental.ParameterServerStrategy : * steps_per_epoch=None is not supported. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections.abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If model.fit is wrapped in tf.function . ValueError : In case of mismatch between the provided input data and what the model expects or when the input data is empty . View Source @ traceback_utils . filter_traceback def fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = \"auto\" , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ): \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If these include `sample_weights` as a third component, note that sample weighting applies to the `weighted_metrics` argument but not the `metrics` argument in `compile()`. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger` and `tf.keras.callbacks.History` callbacks are created automatically and need not be passed into `model.fit`. `tf.keras.callbacks.ProgbarLogger` is created or not based on `verbose` argument to `model.fit`. Callbacks with batch-level calls are currently unsupported with `tf.distribute.experimental.ParameterServerStrategy`, and users are advised to implement epoch-level calls instead with an appropriate `steps_per_epoch` value. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. If both `validation_data` and `validation_split` are provided, `validation_data` will override `validation_split`. `validation_split` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. `validation_data` could be: - A tuple `(x_val, y_val)` of Numpy arrays or tensors. - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays. - A `tf.data.Dataset`. - A Python generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. `validation_data` is not yet supported with `tf.distribute.experimental.ParameterServerStrategy`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. Note that sample weighting does not apply to metrics specified via the `metrics` argument in `compile()`. To apply sample weighting to your metrics, you can specify them via the `weighted_metrics` in `compile()` instead. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset. This argument is not supported with array inputs. When using `tf.distribute.experimental.ParameterServerStrategy`: * `steps_per_epoch=None` is not supported. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections.abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: 1. If the model was never compiled or, 2. If `model.fit` is wrapped in `tf.function`. ValueError: In case of mismatch between the provided input data and what the model expects or when the input data is empty. \"\"\" base_layer . keras_api_gauge . get_cell ( \"fit\" ) . set ( True ) # Legacy graph support is contained in `training_v1.Model`. version_utils . disallow_legacy_graph ( \"Model\" , \"fit\" ) self . _assert_compile_was_called () self . _check_call_args ( \"fit\" ) _disallow_inside_tf_function ( \"fit\" ) verbose = _get_verbosity ( verbose , self . distribute_strategy ) if validation_split and validation_data is None : # Create the validation data using the training data. Only supported # for `Tensor` and `NumPy` input. ( x , y , sample_weight , ), validation_data = data_adapter . train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split ) if validation_data : ( val_x , val_y , val_sample_weight , ) = data_adapter . unpack_x_y_sample_weight ( validation_data ) if self . distribute_strategy . _should_use_with_coordinator : self . _cluster_coordinator = ( tf . distribute . experimental . coordinator . ClusterCoordinator ( self . distribute_strategy ) ) with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ): # Creates a `tf.data.Dataset` and handles batch and epoch iteration. data_handler = data_adapter . get_data_handler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ): callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) self . stop_training = False self . train_function = self . make_train_function () self . _train_counter . assign ( 0 ) callbacks . on_train_begin () training_logs = None # Handle fault-tolerance for multi-worker. # TODO(omalleyt): Fix the ordering issues that mean this has to # happen after `callbacks.on_train_begin`. steps_per_epoch_inferred = ( steps_per_epoch or data_handler . inferred_steps ) ( data_handler . _initial_epoch , data_handler . _initial_step , ) = self . _maybe_load_initial_counters_from_ckpt ( steps_per_epoch_inferred , initial_epoch ) logs = None for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) with data_handler . catch_stop_iteration (): for step in data_handler . steps (): with tf . profiler . experimental . Trace ( \"train\" , epoch_num = epoch , step_num = step , batch_size = batch_size , _r = 1 , ): callbacks . on_train_batch_begin ( step ) tmp_logs = self . train_function ( iterator ) if data_handler . should_sync : context . async_wait () # No error, now safe to assign to logs. logs = tmp_logs end_step = step + data_handler . step_increment callbacks . on_train_batch_end ( end_step , logs ) if self . stop_training : break logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if logs is None : raise ValueError ( \"Unexpected result of `train_function` \" \"(Empty logs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) # Override with model metrics instead of last step logs logs = self . _validate_and_get_metrics_result ( logs ) epoch_logs = copy . copy ( logs ) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): # Create data_handler for evaluation and cache it. if getattr ( self , \"_eval_data_handler\" , None ) is None : self . _eval_data_handler = data_adapter . get_data_handler ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps_per_epoch = validation_steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , return_dict = True , _use_cached_eval_dataset = True , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items () } epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) training_logs = epoch_logs if self . stop_training : break if ( isinstance ( self . optimizer , optimizer_experimental . Optimizer ) and epochs > 0 ): self . optimizer . finalize_variable_values ( self . trainable_variables ) # If eval data_handler exists, delete it after all epochs are done. if getattr ( self , \"_eval_data_handler\" , None ) is not None : del self . _eval_data_handler callbacks . on_train_end ( logs = training_logs ) return self . history","title":"fit"},{"location":"reference/grid_transformer/mask/#fit_generator","text":"def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 ) Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def fit_generator ( self , generator , steps_per_epoch = None , epochs = 1 , verbose = 1 , callbacks = None , validation_data = None , validation_steps = None , validation_freq = 1 , class_weight = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , shuffle = True , initial_epoch = 0 , ) : \"\"\"Fits the model on data yielded batch-by-batch by a Python generator. DEPRECATED: `Model.fit` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.fit_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.fit`, which supports generators.\" , stacklevel = 2 , ) return self . fit ( generator , steps_per_epoch = steps_per_epoch , epochs = epochs , verbose = verbose , callbacks = callbacks , validation_data = validation_data , validation_steps = validation_steps , validation_freq = validation_freq , class_weight = class_weight , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , shuffle = shuffle , initial_epoch = initial_epoch , )","title":"fit_generator"},{"location":"reference/grid_transformer/mask/#get_config","text":"def get_config ( self ) Returns the config of the Model . Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a Model . This allows the Model to be be reinstantiated later (without its trained weights) from this configuration. Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed Model are advised to override this method, and continue to update the dict from super(MyModel, self).get_config() to provide the proper configuration of this Model . The default config is an empty dict. Optionally, raise NotImplementedError to allow Keras to attempt a default serialization. Returns: Type Description None Python dictionary containing the configuration of this Model . View Source def get_config ( self ) : \" \"\" Returns the config of the `Model`. Config is a Python dictionary (serializable) containing the configuration of an object, which in this case is a `Model`. This allows the `Model` to be be reinstantiated later (without its trained weights) from this configuration. Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Developers of subclassed `Model` are advised to override this method, and continue to update the dict from `super(MyModel, self).get_config()` to provide the proper configuration of this `Model`. The default config is an empty dict. Optionally, raise `NotImplementedError` to allow Keras to attempt a default serialization. Returns: Python dictionary containing the configuration of this `Model`. \"\" \" # Return an empty dict here because otherwise Model # subclass developers may see # their model's `__init__()` fed with unexpected keyword arguments, # if their `__init__()` takes no argument for example, and they # don't override `from_config()`, which would use `cls(**config)` # as a result. config = {} if getattr ( saving_lib . _SAVING_V3_ENABLED , \"value\" , False ) : if self . _is_compiled and hasattr ( self , \"_compile_config\" ) : config [ \"compile_config\" ] = self . _compile_config . serialize () if self . built : config [ \"build_input_shape\" ] = self . _build_input_shape return config","title":"get_config"},{"location":"reference/grid_transformer/mask/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/mask/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/mask/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/mask/#get_layer","text":"def get_layer ( self , name = None , index = None ) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Parameters: Name Type Description Default name None String, name of layer. None index None Integer, index of layer. None Returns: Type Description None A layer instance. View Source def get_layer ( self , name = None , index = None ) : \" \"\" Retrieves a layer based on either its name (unique) or index. If `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Args: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. \"\" \" # TODO(fchollet): We could build a dictionary based on layer names # since they are constant, but we have not done that yet. if index is not None and name is not None : raise ValueError ( \"Provide only a layer name or a layer index. Received: \" f \"index={index}, name={name}.\" ) if index is not None : if len ( self . layers ) <= index : raise ValueError ( f \"Was asked to retrieve layer at index {index}\" f \" but model only has {len(self.layers)}\" \" layers.\" ) else : return self . layers [ index ] if name is not None : for layer in self . layers : if layer . name == name : return layer raise ValueError ( f \"No such layer: {name}. Existing layers are: \" f \"{list(layer.name for layer in self.layers)}.\" ) raise ValueError ( \"Provide either a layer name or layer index at `get_layer`.\" )","title":"get_layer"},{"location":"reference/grid_transformer/mask/#get_metrics_result","text":"def get_metrics_result ( self ) Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: Type Description None A dict containing values of the metrics listed in self.metrics . Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def get_metrics_result ( self ) : \" \"\" Returns the model's metrics values as a dict. If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method. Returns: A `dict` containing values of the metrics listed in `self.metrics`. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" # Collect metrics to return return_metrics = {} for metric in self . metrics : result = metric . result () if isinstance ( result , dict ) : return_metrics . update ( result ) else : return_metrics [ metric . name ] = result return return_metrics","title":"get_metrics_result"},{"location":"reference/grid_transformer/mask/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/mask/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/mask/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/mask/#get_weight_paths","text":"def get_weight_paths ( self ) Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a tf.Variable instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding tf.Variable instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: Type Description None A dict where keys are variable paths and values are tf.Variable instances. View Source def get_weight_paths ( self ) : \"\"\"Retrieve all the variables and their paths for the model. The variable path (string) is a stable key to indentify a `tf.Variable` instance owned by the model. It can be used to specify variable-specific configurations (e.g. DTensor, quantization) from a global view. This method returns a dict with weight object paths as keys and the corresponding `tf.Variable` instances as values. Note that if the model is a subclassed model and the weights haven't been initialized, an empty dict will be returned. Returns: A dict where keys are variable paths and values are `tf.Variable` instances. Example: ```python class SubclassModel(tf.keras.Model): def __init__(self, name=None): super().__init__(name=name) self.d1 = tf.keras.layers.Dense(10) self.d2 = tf.keras.layers.Dense(20) def call(self, inputs): x = self.d1(inputs) return self.d2(x) model = SubclassModel() model(tf.zeros((10, 10))) weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': model.d1.kernel, # 'd1.bias': model.d1.bias, # 'd2.kernel': model.d2.kernel, # 'd2.bias': model.d2.bias, # } # Functional model inputs = tf.keras.Input((10,), batch_size=10) x = tf.keras.layers.Dense(20, name='d1')(inputs) output = tf.keras.layers.Dense(30, name='d2')(x) model = tf.keras.Model(inputs, output) d1 = model.layers[1] d2 = model.layers[2] weight_paths = model.get_weight_paths() # weight_paths: # { # 'd1.kernel': d1.kernel, # 'd1.bias': d1.bias, # 'd2.kernel': d2.kernel, # 'd2.bias': d2.bias, # } ``` \"\"\" result = {} ( descendants , object_paths_dict , ) = tf . __internal__ . tracking . ObjectGraphView ( self ). breadth_first_traversal () for descendant in descendants : if isinstance ( descendant , tf . Variable ) : trackable_references = object_paths_dict [ descendant ] object_path = \".\" . join ( [ t.name for t in trackable_references ] ) result [ object_path ] = descendant return result","title":"get_weight_paths"},{"location":"reference/grid_transformer/mask/#get_weights","text":"def get_weights ( self ) Retrieves the weights of the model. Returns: Type Description None A flat list of Numpy arrays. View Source def get_weights ( self ) : \"\" \"Retrieves the weights of the model. Returns: A flat list of Numpy arrays. \"\" \" with self.distribute_strategy.scope(): return super().get_weights()","title":"get_weights"},{"location":"reference/grid_transformer/mask/#load_weights","text":"def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Parameters: Name Type Description Default filepath None String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). This can also be a path to a SavedModel saved from model.save . None by_name None Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. None skip_mismatch None Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). None options None Optional tf.train.CheckpointOptions object that specifies options for loading weights. None Returns: Type Description None When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: Type Description ImportError If h5py is not available and the weight file is in HDF5 format. ValueError If skip_mismatch is set to True when by_name is False . View Source @ traceback_utils . filter_traceback def load_weights ( self , filepath , by_name = False , skip_mismatch = False , options = None ): \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If `by_name` is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If `by_name` is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading (`by_name=False`) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the `Model`'s constructor. Args: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to `save_weights`). This can also be a path to a SavedModel saved from `model.save`. by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when `by_name=True`). options: Optional `tf.train.CheckpointOptions` object that specifies options for loading weights. Returns: When loading a weight file in TensorFlow format, returns the same status object as `tf.train.Checkpoint.restore`. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from `Model`, immediately if it is already built). When loading weights in HDF5 format, returns `None`. Raises: ImportError: If `h5py` is not available and the weight file is in HDF5 format. ValueError: If `skip_mismatch` is set to `True` when `by_name` is `False`. \"\"\" if backend . is_tpu_strategy ( self . _distribution_strategy ): if self . _distribution_strategy . extended . steps_per_run > 1 and ( not saving_utils . is_hdf5_filepath ( filepath ) ): spr = self . _distribution_strategy . extended . steps_per_run raise ValueError ( \"Load weights is not implemented with TPUStrategy \" \"with `steps_per_run` greater than 1. The \" f \"`steps_per_run` is {spr}\" ) if skip_mismatch and not by_name : raise ValueError ( \"When calling model.load_weights, skip_mismatch can only be \" \"set to True when by_name is True.\" ) filepath , save_format = _detect_save_format ( filepath ) if save_format == \"tf\" : status = self . _checkpoint . read ( filepath , options ) if by_name : raise NotImplementedError ( \"Weights may only be loaded based on topology into Models \" \"when loading TensorFlow-formatted weights \" \"(got by_name=True to load_weights).\" ) if not tf . executing_eagerly (): session = backend . get_session () # Restore existing variables (if any) immediately, and set up a # streaming restore for any variables created in the future. tf . __internal__ . tracking . streaming_restore ( status = status , session = session ) status . assert_nontrivial_match () else : status = None if h5py is None : raise ImportError ( \"`load_weights` requires h5py package when loading weights \" \"from HDF5. Try installing h5py.\" ) if not self . _is_graph_network and not self . built : raise ValueError ( \"Unable to load weights saved in HDF5 format into a \" \"subclassed Model which has not created its variables yet. \" \"Call the Model first, then load the weights.\" ) self . _assert_weights_created () with h5py . File ( filepath , \"r\" ) as f : if \"layer_names\" not in f . attrs and \"model_weights\" in f : f = f [ \"model_weights\" ] if by_name : hdf5_format . load_weights_from_hdf5_group_by_name ( f , self , skip_mismatch ) else : hdf5_format . load_weights_from_hdf5_group ( f , self ) # Perform any layer defined finalization of the layer state. for layer in self . layers : layer . finalize_state () return status","title":"load_weights"},{"location":"reference/grid_transformer/mask/#make_predict_function","text":"def make_predict_function ( self , force = False ) Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the predict function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . View Source def make_predict_function ( self , force = False ) : \" \"\" Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by `Model.predict` and `Model.predict_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.predict_step`. This function is cached the first time `Model.predict` or `Model.predict_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the predict function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return the outputs of the `Model`. \"\" \" if self . predict_function is not None and not force : return self . predict_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . predict_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _predict_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = \"concat\" ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with a single step. \"\" \" return step_function ( self , iterator ) else : def predict_function ( iterator ) : \" \"\" Runs an evaluation execution with multiple steps. \"\" \" outputs = step_function ( self , iterator ) for _ in tf . range ( self . _steps_per_execution - 1 ) : tf . autograph . experimental . set _loop_options ( shape_invariants = [ ( outputs , tf . nest . map_structure ( lambda t : tf_utils . get_tensor_spec ( t , dynamic_batch = True ). shape , outputs , ), ) ] ) step_outputs = step_function ( self , iterator ) outputs = tf . nest . map_structure ( lambda t1 , t2 : concat ( [ t1 , t2 ] ), outputs , step_outputs ) return outputs if not self . run_eagerly : predict_function = tf . function ( predict_function , reduce_retracing = True ) self . predict_function = predict_function return self . predict_function","title":"make_predict_function"},{"location":"reference/grid_transformer/mask/#make_test_function","text":"def make_test_function ( self , force = False ) Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the test function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . View Source def make_test_function ( self , force = False ) : \" \"\" Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by `Model.evaluate` and `Model.test_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual evaluation logic to `Model.test_step`. This function is cached the first time `Model.evaluate` or `Model.test_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the test function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_test_batch_end`. \"\" \" if self . test_function is not None and not force : return self . test_function def step_function ( model , iterator ) : \" \"\" Runs a single evaluation step. \"\" \" def run_step ( data ) : outputs = model . test_step ( data ) # Ensure counter is updated only if `test_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _test_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def test_function ( iterator ) : \" \"\" Runs a test execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) if self . _cluster_coordinator : self . test_function = ( lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it ,) ) ) else : self . test_function = test_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def test_function ( iterator , steps_per_execution ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = lambda it : self . _cluster_coordinator . schedule ( test_function , args = ( it , self . _steps_per_execution . value ()) ) else : def test_function ( iterator ) : \" \"\" Runs a test execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : test_function = tf . function ( test_function , reduce_retracing = True ) self . test_function = test_function return self . test_function","title":"make_test_function"},{"location":"reference/grid_transformer/mask/#make_train_function","text":"def make_train_function ( self , force = False ) Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. You can skip the cache and generate again the function with force=True . Parameters: Name Type Description Default force None Whether to regenerate the train function and skip the cached function if available. None Returns: Type Description None Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . View Source def make_train_function ( self , force = False ) : \" \"\" Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by `Model.fit` and `Model.train_on_batch`. Typically, this method directly controls `tf.function` and `tf.distribute.Strategy` settings, and delegates the actual training logic to `Model.train_step`. This function is cached the first time `Model.fit` or `Model.train_on_batch` is called. The cache is cleared whenever `Model.compile` is called. You can skip the cache and generate again the function with `force=True`. Args: force: Whether to regenerate the train function and skip the cached function if available. Returns: Function. The function created by this method should accept a `tf.data.Iterator`, and return a `dict` containing values that will be passed to `tf.keras.Callbacks.on_train_batch_end`, such as `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" if self . train_function is not None and not force : return self . train_function def step_function ( model , iterator ) : \" \"\" Runs a single training step. \"\" \" def run_step ( data ) : outputs = model . train_step ( data ) # Ensure counter is updated only if `train_step` succeeds. with tf . control_dependencies ( _minimum_control_deps ( outputs )) : model . _train_counter . assign_add ( 1 ) return outputs if self . _jit_compile : run_step = tf . function ( run_step , jit_compile = True , reduce_retracing = True ) data = next ( iterator ) outputs = model . distribute_strategy . run ( run_step , args = ( data ,)) outputs = reduce_per_replica ( outputs , self . distribute_strategy , reduction = self . distribute_reduction_method , ) return outputs # Special case if steps_per_execution is one. if ( self . _steps_per_execution is None or self . _steps_per_execution . numpy (). item () == 1 ) : def train_function ( iterator ) : \" \"\" Runs a training execution with a single step. \"\" \" return step_function ( self , iterator ) if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function if self . _cluster_coordinator : self . train_function = ( lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it ,) ) ) else : self . train_function = train_function # If we're using a coordinator, use the value of # self._steps_per_execution at the time the function is # called/scheduled, and not when it is actually executed. elif self . _cluster_coordinator : def train_function ( iterator , steps_per_execution ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = lambda it : self . _cluster_coordinator . schedule ( train_function , args = ( it , self . _steps_per_execution . value ()) ) else : def train_function ( iterator ) : \" \"\" Runs a training execution with multiple steps. \"\" \" for _ in tf . range ( self . _steps_per_execution ) : outputs = step_function ( self , iterator ) return outputs if not self . run_eagerly : train_function = tf . function ( train_function , reduce_retracing = True ) self . train_tf_function = train_function self . train_function = train_function return self . train_function","title":"make_train_function"},{"location":"reference/grid_transformer/mask/#predict","text":"def predict ( self , x , batch_size = None , verbose = 'auto' , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use __call__() for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behave differently during inference. You may pair the individual model call with a tf.function for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use tensor.numpy() to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See this FAQ entry for more details about the difference between Model methods predict() and __call__() . Parameters: Name Type Description Default x None Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking<br>behavior for iterator-like inputs section of Model.fit . None batch_size None Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose None \"auto\" , 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" defaults to 1 for most cases, and to 2 when used with ParameterServerStrategy . Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (e.g. in a production environment). None steps None Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict() will run until the input dataset is exhausted. None callbacks None List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . None max_queue_size None Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. None workers None Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. None use_multiprocessing None Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict is wrapped in a tf.function . ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. View Source @traceback_utils.filter_traceback def predict ( self , x , batch_size = None , verbose = \"auto\" , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , ) : \" \"\" Generates output predictions for the input samples. Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time. For small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behave differently during inference. You may pair the individual model call with a `tf.function` for additional performance inside your inner loop. If you need access to numpy array values instead of tensors after your model call, you can use `tensor.numpy()` to get the numpy array value of an eager tensor. Also, note the fact that test loss is not affected by regularization layers like noise and dropout. Note: See [this FAQ entry]( https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call) for more details about the difference between `Model` methods `predict()` and `__call__()`. Args: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: `\" auto \"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\" auto \"` defaults to 1 for most cases, and to 2 when used with `ParameterServerStrategy`. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict()` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks]( https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict` is wrapped in a `tf.function`. ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\" \" base_layer . keras_api_gauge . get_cell ( \"predict\" ). set ( True ) version_utils . disallow_legacy_graph ( \"Model\" , \"predict\" ) self . _check_call_args ( \"predict\" ) _disallow_inside_tf_function ( \"predict\" ) # TODO(yashkatariya): Cache model on the coordinator for faster # prediction. If running under PSS, then swap it with OneDeviceStrategy # so that execution will run on the coordinator. original_pss_strategy = None if self . distribute_strategy . _should_use_with_coordinator : original_pss_strategy = self . distribute_strategy self . _distribution_strategy = None # Cluster coordinator is set by `.fit()` and `.evaluate()` which is not # needed in `.predict()` because all the predictions happen on the # coordinator/locally. if self . _cluster_coordinator : self . _cluster_coordinator = None verbose = _get_verbosity ( verbose , self . distribute_strategy ) outputs = None with self . distribute_strategy . scope () : # Creates a `tf.data.Dataset` and handles batch and epoch iteration. dataset_types = ( tf . compat . v1 . data . Dataset , tf . data . Dataset ) if ( self . _in_multi_worker_mode () or _is_tpu_multi_host ( self . distribute_strategy ) ) and isinstance ( x , dataset_types ) : try : options = tf . data . Options () data_option = tf . data . experimental . AutoShardPolicy . DATA options . experimental_distribute . auto_shard_policy = ( data_option ) x = x . with_options ( options ) except ValueError : warnings . warn ( \"Using Model.predict with MultiWorkerMirroredStrategy \" \"or TPUStrategy and AutoShardPolicy.FILE might lead to \" \"out-of-order result. Consider setting it to \" \"AutoShardPolicy.DATA.\" , stacklevel = 2 , ) data_handler = data_adapter . get_data_handler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , model = self , steps_per_execution = self . _steps_per_execution , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , callbacks_module . CallbackList ) : callbacks = callbacks_module . CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) self . predict_function = self . make_predict_function () self . _predict_counter . assign ( 0 ) callbacks . on_predict_begin () batch_outputs = None for _ , iterator in data_handler . enumerate_epochs () : # Single epoch. with data_handler . catch_stop_iteration () : for step in data_handler . steps () : callbacks . on_predict_batch_begin ( step ) tmp_batch_outputs = self . predict_function ( iterator ) if data_handler . should_sync : context . async_wait () batch_outputs = ( tmp_batch_outputs # No error, now safe to assign. ) if outputs is None : outputs = tf . nest . map_structure ( lambda batch_output : [ batch_output ] , batch_outputs , ) else : tf . __internal__ . nest . map_structure_up_to ( batch_outputs , lambda output , batch_output : output . append ( batch_output ), outputs , batch_outputs , ) end_step = step + data_handler . step_increment callbacks . on_predict_batch_end ( end_step , { \"outputs\" : batch_outputs } ) if batch_outputs is None : raise ValueError ( \"Unexpected result of `predict_function` \" \"(Empty batch_outputs). Please use \" \"`Model.compile(..., run_eagerly=True)`, or \" \"`tf.config.run_functions_eagerly(True)` for more \" \"information of where went wrong, or file a \" \"issue/bug to `tf.keras`.\" ) callbacks . on_predict_end () all_outputs = tf . __internal__ . nest . map_structure_up_to ( batch_outputs , potentially_ragged_concat , outputs ) # If originally PSS strategy was used, then replace it back since # predict is running under `OneDeviceStrategy` after the swap and once # its done we need to replace it back to PSS again. if original_pss_strategy is not None : self . _distribution_strategy = original_pss_strategy return tf_utils . sync_to_numpy_or_python_type ( all_outputs )","title":"predict"},{"location":"reference/grid_transformer/mask/#predict_generator","text":"def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 ) Generates predictions for the input samples from a data generator. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. View Source @doc_controls . do_not_generate_docs def predict_generator ( self , generator , steps = None , callbacks = None , max_queue_size = 10 , workers = 1 , use_multiprocessing = False , verbose = 0 , ) : \"\"\"Generates predictions for the input samples from a data generator. DEPRECATED: `Model.predict` now supports generators, so there is no longer any need to use this endpoint. \"\"\" warnings . warn ( \"`Model.predict_generator` is deprecated and \" \"will be removed in a future version. \" \"Please use `Model.predict`, which supports generators.\" , stacklevel = 2 , ) return self . predict ( generator , steps = steps , max_queue_size = max_queue_size , workers = workers , use_multiprocessing = use_multiprocessing , verbose = verbose , callbacks = callbacks , )","title":"predict_generator"},{"location":"reference/grid_transformer/mask/#predict_on_batch","text":"def predict_on_batch ( self , x ) Returns predictions for a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). None Returns: Type Description None Numpy array(s) of predictions. Raises: Type Description RuntimeError If model.predict_on_batch is wrapped in a tf.function . View Source def predict_on_batch ( self , x ) : \"\" \"Returns predictions for a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`. \"\" \" self . _check_call_args ( \"predict_on_batch\" ) _disallow_inside_tf_function ( \"predict_on_batch\" ) with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x ) self . predict_function = self . make_predict_function () outputs = self . predict_function ( iterator ) return tf_utils . sync_to_numpy_or_python_type ( outputs )","title":"predict_on_batch"},{"location":"reference/grid_transformer/mask/#predict_step","text":"def predict_step ( self , data ) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None The result of one inference step, typically the output of calling the Model on data. View Source def predict_step ( self , data ) : \" \"\" The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by `Model.make_predict_function`. This method should contain the mathematical logic for one step of inference. This typically includes the forward pass. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_predict_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: The result of one inference step, typically the output of calling the `Model` on data. \"\" \" x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) return self ( x , training = False )","title":"predict_step"},{"location":"reference/grid_transformer/mask/#reset_metrics","text":"def reset_metrics ( self ) Resets the state of all the metrics in the model. View Source def reset_metrics ( self ) : \"\" \"Resets the state of all the metrics in the model. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model . compile ( optimizer = \"Adam\" , loss = \"mse\" , metrics = [ \"mae\" ] ) >>> x = np . random . random (( 2 , 3 )) >>> y = np . random . randint ( 0 , 2 , ( 2 , 2 )) >>> _ = model . fit ( x , y , verbose = 0 ) >>> assert all ( float ( m . result ()) for m in model . metrics ) >>> model . reset_metrics () >>> assert all ( float ( m . result ()) == 0 for m in model . metrics ) \"\" \" for m in self.metrics: m.reset_state()","title":"reset_metrics"},{"location":"reference/grid_transformer/mask/#reset_states","text":"def reset_states ( self ) View Source def reset_states ( self ) : for layer in self . layers : if hasattr ( layer , \"reset_states\" ) and getattr ( layer , \"stateful\" , False ) : layer . reset_states ()","title":"reset_states"},{"location":"reference/grid_transformer/mask/#save","text":"def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True ) Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see tf.keras.models.save_model or the Serialization and Saving guide for details. Parameters: Name Type Description Default filepath None String, PathLike, path to SavedModel or H5 file to save the model. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None include_optimizer None If True, save optimizer's state together. None save_format None Either 'tf' or 'h5' , indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. None signatures None Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. None options None (only applies to SavedModel format) tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. None save_traces None (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to True . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a get_config() method. None View Source @traceback_utils.filter_traceback def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None , save_traces = True , ) : \" \"\" Saves the model to Tensorflow SavedModel or a single HDF5 file. Please see `tf.keras.models.save_model` or the [Serialization and Saving guide]( https://keras.io/guides/serialization_and_saving/) for details. Args: filepath: String, PathLike, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either `'tf'` or `'h5'`, indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Example: ```python from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') ``` \"\" \" save . save_model ( self , filepath , overwrite , include_optimizer , save_format , signatures , options , save_traces , )","title":"save"},{"location":"reference/grid_transformer/mask/#save_spec","text":"def save_spec ( self , dynamic_batch = True ) Returns the tf.TensorSpec of call inputs as a tuple (args, kwargs) . This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: model = tf . keras . Model ( ... ) @tf . function def serve ( * args , ** kwargs ): outputs = model ( * args , ** kwargs ) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs , kwarg_specs = model . save_spec () model . save ( path , signatures = { 'serving_default' : serve . get_concrete_function ( * arg_specs , ** kwarg_specs ) }) Parameters: Name Type Description Default dynamic_batch None Whether to set the batch sizes of all the returned tf.TensorSpec to None . (Note that when defining functional or Sequential models with tf.keras.Input([...], batch_size=X) , the batch size will always be preserved). Defaults to True . None Returns: Type Description None If the model inputs are defined, returns a tuple (args, kwargs) . All elements in args and kwargs are tf.TensorSpec . If the model inputs are not defined, returns None . The model inputs are automatically set when calling the model, model.fit , model.evaluate or model.predict . View Source def save_spec ( self , dynamic_batch = True ) : \" \"\" Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`. This value is automatically defined after calling the model for the first time. Afterwards, you can use it when exporting the model for serving: ```python model = tf.keras.Model(...) @tf.function def serve(*args, **kwargs): outputs = model(*args, **kwargs) # Apply postprocessing steps, or add additional outputs. ... return outputs # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this # example, is an empty dict since functional models do not use keyword # arguments. arg_specs, kwarg_specs = model.save_spec() model.save(path, signatures={ 'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs) }) ``` Args: dynamic_batch: Whether to set the batch sizes of all the returned `tf.TensorSpec` to `None`. (Note that when defining functional or Sequential models with `tf.keras.Input([...], batch_size=X)`, the batch size will always be preserved). Defaults to `True`. Returns: If the model inputs are defined, returns a tuple `(args, kwargs)`. All elements in `args` and `kwargs` are `tf.TensorSpec`. If the model inputs are not defined, returns `None`. The model inputs are automatically set when calling the model, `model.fit`, `model.evaluate` or `model.predict`. \"\" \" return self . _get_save_spec ( dynamic_batch , inputs_only = False )","title":"save_spec"},{"location":"reference/grid_transformer/mask/#save_weights","text":"def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Parameters: Name Type Description Default filepath None String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. None overwrite None Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. None save_format None Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. None options None Optional tf.train.CheckpointOptions object that specifies options for saving weights. None Raises: Type Description ImportError If h5py is not available when attempting to save in HDF5 format. View Source @ traceback_utils . filter_traceback def save_weights ( self , filepath , overwrite = True , save_format = None , options = None ): \"\"\"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints]( https://www.tensorflow.org/guide/checkpoint) for details on the TensorFlow format. Args: filepath: String or PathLike, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or '.keras' will default to HDF5 if `save_format` is `None`. Otherwise `None` defaults to 'tf'. options: Optional `tf.train.CheckpointOptions` object that specifies options for saving weights. Raises: ImportError: If `h5py` is not available when attempting to save in HDF5 format. \"\"\" self . _assert_weights_created () filepath = io_utils . path_to_string ( filepath ) filepath_is_h5 = saving_utils . is_hdf5_filepath ( filepath ) if save_format is None : if filepath_is_h5 : save_format = \"h5\" else : save_format = \"tf\" else : user_format = save_format . lower () . strip () if user_format in ( \"tensorflow\" , \"tf\" ): save_format = \"tf\" elif user_format in ( \"hdf5\" , \"h5\" , \"keras\" ): save_format = \"h5\" else : raise ValueError ( f \"Unknown format. Received: `save_format`={save_format}. \" 'Was expecting one of {\"tf\", \"h5\"}.' ) if save_format == \"tf\" and filepath_is_h5 : raise ValueError ( 'save_weights got save_format=\"tf\"/\"tensorflow\", but the ' f \"filepath ({filepath}) looks like an HDF5 file. \" 'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.' ) if save_format == \"h5\" and h5py is None : raise ImportError ( \"`save_weights` requires h5py when saving in hdf5, but h5py is \" \"not available. Try installing h5py package.\" ) if save_format == \"tf\" : check_filepath = filepath + \".index\" else : check_filepath = filepath # If file exists and should not be overwritten: if not overwrite and os . path . isfile ( check_filepath ): proceed = io_utils . ask_to_proceed_with_overwrite ( check_filepath ) if not proceed : return if save_format == \"h5\" : with h5py . File ( filepath , \"w\" ) as f : hdf5_format . save_weights_to_hdf5_group ( f , self ) else : if not tf . executing_eagerly (): # Call `get_session` to initialize any uninitialized variables. backend . get_session () self . _checkpoint . write ( filepath , options = options ) # Record this checkpoint so it's visible from # tf.train.latest_checkpoint. tf . __internal__ . train . update_checkpoint_state ( save_dir = os . path . dirname ( filepath ), model_checkpoint_path = filepath , save_relative_paths = True , all_model_checkpoint_paths = [ filepath ], )","title":"save_weights"},{"location":"reference/grid_transformer/mask/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/mask/#summary","text":"def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None ) Prints a string summary of the network. Parameters: Name Type Description Default line_length None Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). None positions None Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . None print_fn None Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. print expand_nested None Whether to expand the nested models. If not provided, defaults to False . None show_trainable None Whether to show if a layer is trainable. If not provided, defaults to False . None layer_range None a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to layer_range[0] and the end predicate will be the last element it matches to layer_range[1] . By default None which considers all layers of model. None Raises: Type Description ValueError if summary() is called before the model is built. View Source def summary ( self , line_length = None , positions = None , print_fn = None , expand_nested = False , show_trainable = False , layer_range = None , ) : \" \"\" Prints a string summary of the network. Args: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to `[.33, .55, .67, 1.]`. print_fn: Print function to use. Defaults to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. expand_nested: Whether to expand the nested models. If not provided, defaults to `False`. show_trainable: Whether to show if a layer is trainable. If not provided, defaults to `False`. layer_range: a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to `layer_range[0]` and the end predicate will be the last element it matches to `layer_range[1]`. By default `None` which considers all layers of model. Raises: ValueError: if `summary()` is called before the model is built. \"\" \" if not self . built : raise ValueError ( \"This model has not yet been built. \" \"Build the model first by calling `build()` or by calling \" \"the model on a batch of data.\" ) layer_utils . print_summary ( self , line_length = line_length , positions = positions , print_fn = print_fn , expand_nested = expand_nested , show_trainable = show_trainable , layer_range = layer_range , )","title":"summary"},{"location":"reference/grid_transformer/mask/#test_on_batch","text":"def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False ) Test the model on a single batch of samples. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.test_on_batch is wrapped in a tf.function . View Source def test_on_batch ( self , x , y = None , sample_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"test_on_batch\" ) _disallow_inside_tf_function ( \"test_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope () : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight ) self . test_function = self . make_test_function () logs = self . test_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"test_on_batch"},{"location":"reference/grid_transformer/mask/#test_step","text":"def test_step ( self , data ) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. View Source def test_step ( self , data ) : \" \"\" The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by `Model.make_test_function`. This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_test_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) # Updates stateful loss metrics. self . compute_loss ( x , y , y_pred , sample_weight ) return self . compute_metrics ( x , y , y_pred , sample_weight )","title":"test_step"},{"location":"reference/grid_transformer/mask/#to_json","text":"def to_json ( self , ** kwargs ) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to * json.dumps() . None Returns: Type Description None A JSON string. View Source def to_json ( self , ** kwargs ): \"\"\"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to *`json.dumps()`. Returns: A JSON string. \"\"\" model_config = self . _updated_config () return json . dumps ( model_config , default = json_utils . get_json_type , ** kwargs )","title":"to_json"},{"location":"reference/grid_transformer/mask/#to_yaml","text":"def to_yaml ( self , ** kwargs ) Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Parameters: Name Type Description Default **kwargs None Additional keyword arguments to be passed to yaml.dump() . None Returns: Type Description None A YAML string. Raises: Type Description RuntimeError announces that the method poses a security risk View Source def to_yaml ( self , ** kwargs ) : \" \"\" Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk \"\" \" raise RuntimeError ( \"Method `model.to_yaml()` has been removed due to security risk of \" \"arbitrary code execution. Please use `model.to_json()` instead.\" )","title":"to_yaml"},{"location":"reference/grid_transformer/mask/#train_on_batch","text":"def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False ) Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x None Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. None y None Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). None sample_weight None Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight None Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None reset_metrics None If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. None return_dict None If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. None Returns: Type Description None Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: Type Description RuntimeError If model.train_on_batch is wrapped in a tf.function . View Source def train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None , reset_metrics = True , return_dict = False , ) : \" \"\" Runs a single gradient update on a single batch of data. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \" pay more attention \" to samples from an under-represented class. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`. \"\" \" self . _assert_compile_was_called () self . _check_call_args ( \"train_on_batch\" ) _disallow_inside_tf_function ( \"train_on_batch\" ) if reset_metrics : self . reset_metrics () with self . distribute_strategy . scope (), training_utils . RespectCompiledTrainableState ( # noqa: E501 self ) : iterator = data_adapter . single_batch_iterator ( self . distribute_strategy , x , y , sample_weight , class_weight ) self . train_function = self . make_train_function () logs = self . train_function ( iterator ) logs = tf_utils . sync_to_numpy_or_python_type ( logs ) if return_dict : return logs else : return flatten_metrics_in_order ( logs , self . metrics_names )","title":"train_on_batch"},{"location":"reference/grid_transformer/mask/#train_step","text":"def train_step ( self , data ) The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see Customizing what happens in fit . This method is called by Model.make_train_function . This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Parameters: Name Type Description Default data None A nested structure of Tensor s. None Returns: Type Description None A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source def train_step ( self , data ) : \" \"\" The logic for one training step. This method can be overridden to support custom training logic. For concrete examples of how to override this method see [Customizing what happens in fit]( https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This method is called by `Model.make_train_function`. This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_train_function`, which can also be overridden. Args: data: A nested structure of `Tensor`s. Returns: A `dict` containing values that will be passed to `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" x , y , sample_weight = data_adapter . unpack_x_y_sample_weight ( data ) # Run forward pass. with tf . GradientTape () as tape : y_pred = self ( x , training = True ) loss = self . compute_loss ( x , y , y_pred , sample_weight ) self . _validate_target_and_loss ( y , loss ) # Run backwards pass. self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return self . compute_metrics ( x , y , y_pred , sample_weight )","title":"train_step"},{"location":"reference/grid_transformer/parameters/","text":"Module grid_transformer.parameters The module is a set of classes that define the parameters for Transformer models and GridTransformer models. The TransformerParameters class is a dataclass that holds the parameters for Transformer models, such as the number of layers, hidden state dimension, and dropout rate. The GridTransformerParameters class is a subclass of TransformerParameters that holds additional parameters specific to GridTransformer models, such as the extractor type, mapping function, and masking strategy. You can then pass the instance to a function or class that uses these parameters to define the architecture of the model. To use these classes, you can create an instance of the class with the desired values for the parameters. For example: from dataclasses import asdict params = TransformerParameters ( no = 6 , size = 256 , dropout = 0.2 ) model = transformer ( asdict ( params )) For GridTransformerParameters: params = GridTransformerParameters ( extractor = 'ef' , last = 'start' , last_index = 8 , col = 3 , row = 3 , return_attention_scores = True ) model = grid_trans ( asdict ( params )) View Source \"\"\" The module is a set of classes that define the parameters for Transformer models and GridTransformer models. The `TransformerParameters` class is a dataclass that holds the parameters for Transformer models, such as the number of layers, hidden state dimension, and dropout rate. The `GridTransformerParameters` class is a subclass of `TransformerParameters` that holds additional parameters specific to GridTransformer models, such as the extractor type, mapping function, and masking strategy. You can then pass the instance to a function or class that uses these parameters to define the architecture of the model. To use these classes, you can create an instance of the class with the desired values for the parameters. For example: ```python from dataclasses import asdict params = TransformerParameters(no=6, size=256, dropout=0.2) model = transformer(asdict(params)) ``` For GridTransformerParameters: ```python params = GridTransformerParameters(extractor='ef',last='start',last_index=8,col=3,row=3,return_attention_scores=True) model = grid_trans(asdict(params)) ``` \"\"\" from dataclasses import dataclass from typing import Tuple @dataclass class TransformerParameters : \"\"\" Class that holds parameters for Transformer models. :param pos_emd: Type of position embedding. Can be 'sum' or 'cat'. :param no: Number of layers in the Transformer encoder. :param size: Dimension of the hidden states and the self-attention layers. :param ff_mul: Multiplier for the dimension of the feed-forward layers. :param ff_size: Dimension of the feed-forward layers. If None, it is set to size * ff_mul. :param dropout: Dropout rate for the self-attention and feed-forward layers. :param num_heads: Number of heads in the multi-head self-attention layers. :param ff_act: Activation function for the feed-forward layers. Can be 'relu' or 'gelu'. :param extractor: Type of extractor to use before the transformer layers. Can be 'cnn' or 'rnn'. :param extractor_pooling: Type of pooling to use after the extractor. Can be 'first' or 'flat2d'. :param pooling: Type of pooling to use after the transformer layers. Can be 'first' or 'mean'. :param out_layers: Tuple containing the number of neurons for the output layers. :param show_shape: Whether to print shape of the intermediate tensors. \"\"\" pos_emd : str = \"sum\" no : int = 4 size : int = 128 ff_mul : int = 4 ff_size : int = None dropout : float = 0.1 num_heads : int = 8 ff_act : str = \"gelu\" extractor : str = None extractor_pooling : str = \"flat2d\" pooling : str = \"first\" out_layers : Tuple = ( 1000 , 1000 ) show_shape : bool = True @dataclass class GridTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for grid_transformer models. :param extractor: Type of extractor to use for the image sequence. Can be 'ef' for EfficientNet or 'res' for ResNet. :param extractor_shape: The shape of the extracted features from the extractor. :param last: How to handle the last element of the sequence, can be 'start' to make it the first element or 'end' to make it the last element :param map_fn: The function used to map the image sequence, can be 'batch' or 'map' :param last_index: The index of the last element of the sequence :param channel: The channel used to map the image sequence, can be 'channel' or 'spatial' :param batch_no: Number of batches to use for the map function :param col: Number of columns in the image sequence :param row: Number of rows in the image sequence :param return_extractor_input: Whether to return the input to the extractor :param return_attention_scores: Whether to return the attention scores :param mask: The masking strategy to use, can be 'last' or 'all' :param pre: Preprocessing function to use on the image sequence, can be 'normalize' or 'standardize' :param noise: Noise function to use on the image sequence, can be 'gaussian' or 'poisson' :param augmentation: Augmentation function to use on the image sequence, can be 'random_rotate' or 'random_flip' :param print_: Whether to print information about the model \"\"\" extractor : int = \"ef\" extractor_shape : int = 84 last : str = \"start\" map_fn : str = \"batch\" last_index : int = None channel : str = \"channel\" batch_no : int = None col : int = 3 row : int = 3 return_extractor_input : bool = False return_attention_scores : bool = False mask : str = \"last\" pre : str = None noise : str = None augmentation : str = None print_ : bool = False @dataclass class MidTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for Mid-level Transformer models. \"\"\" size : int = 256 num_heads : int = 8 no : int = 4 @dataclass class SmallTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for Small-level Transformer models. \"\"\" size : int = 64 num_heads : int = 4 no : int = 4 @dataclass class BigTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for Big-level Transformer models. \"\"\" size : int = 768 num_heads : int = 12 no : int = 12 @dataclass class LargeTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for Large-level Transformer models. \"\"\" size : int = 1024 num_heads : int = 16 no : int = 24 Classes BigTransformerParameters class BigTransformerParameters ( pos_emd : str = 'sum' , no : int = 12 , size : int = 768 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 12 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Big-level Transformer models. View Source @dataclass class BigTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for Big-level Transformer models. \"\"\" size : int = 768 num_heads : int = 12 no : int = 12 Ancestors (in MRO) grid_transformer.parameters.TransformerParameters Class variables dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size GridTransformerParameters class GridTransformerParameters ( pos_emd : str = 'sum' , no : int = 4 , size : int = 128 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 8 , ff_act : str = 'gelu' , extractor : int = 'ef' , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True , extractor_shape : int = 84 , last : str = 'start' , map_fn : str = 'batch' , last_index : int = None , channel : str = 'channel' , batch_no : int = None , col : int = 3 , row : int = 3 , return_extractor_input : bool = False , return_attention_scores : bool = False , mask : str = 'last' , pre : str = None , noise : str = None , augmentation : str = None , print_ : bool = False ) Class that holds parameters for grid_transformer models. Attributes Name Type Description Default extractor None Type of extractor to use for the image sequence. Can be 'ef' for EfficientNet or 'res' for ResNet. None extractor_shape None The shape of the extracted features from the extractor. None last None How to handle the last element of the sequence, can be 'start' to make it the first element or 'end' to make it the last element None map_fn None The function used to map the image sequence, can be 'batch' or 'map' None last_index None The index of the last element of the sequence None channel None The channel used to map the image sequence, can be 'channel' or 'spatial' None batch_no None Number of batches to use for the map function None col None Number of columns in the image sequence None row None Number of rows in the image sequence None return_extractor_input None Whether to return the input to the extractor None return_attention_scores None Whether to return the attention scores None mask None The masking strategy to use, can be 'last' or 'all' None pre None Preprocessing function to use on the image sequence, can be 'normalize' or 'standardize' None noise None Noise function to use on the image sequence, can be 'gaussian' or 'poisson' None augmentation None Augmentation function to use on the image sequence, can be 'random_rotate' or 'random_flip' None print_ None Whether to print information about the model None View Source @dataclass class GridTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for grid_transformer models. :param extractor: Type of extractor to use for the image sequence. Can be 'ef' for EfficientNet or 'res' for ResNet. :param extractor_shape: The shape of the extracted features from the extractor. :param last: How to handle the last element of the sequence, can be 'start' to make it the first element or 'end' to make it the last element :param map_fn: The function used to map the image sequence, can be 'batch' or 'map' :param last_index: The index of the last element of the sequence :param channel: The channel used to map the image sequence, can be 'channel' or 'spatial' :param batch_no: Number of batches to use for the map function :param col: Number of columns in the image sequence :param row: Number of rows in the image sequence :param return_extractor_input: Whether to return the input to the extractor :param return_attention_scores: Whether to return the attention scores :param mask: The masking strategy to use, can be 'last' or 'all' :param pre: Preprocessing function to use on the image sequence, can be 'normalize' or 'standardize' :param noise: Noise function to use on the image sequence, can be 'gaussian' or 'poisson' :param augmentation: Augmentation function to use on the image sequence, can be 'random_rotate' or 'random_flip' :param print_: Whether to print information about the model \"\"\" extractor : int = \"ef\" extractor_shape : int = 84 last : str = \"start\" map_fn : str = \"batch\" last_index : int = None channel : str = \"channel\" batch_no : int = None col : int = 3 row : int = 3 return_extractor_input : bool = False return_attention_scores : bool = False mask : str = \"last\" pre : str = None noise : str = None augmentation : str = None print_ : bool = False Ancestors (in MRO) grid_transformer.parameters.TransformerParameters Class variables augmentation batch_no channel col dropout extractor extractor_pooling extractor_shape ff_act ff_mul ff_size last last_index map_fn mask no noise num_heads out_layers pooling pos_emd pre print_ return_attention_scores return_extractor_input row show_shape size LargeTransformerParameters class LargeTransformerParameters ( pos_emd : str = 'sum' , no : int = 24 , size : int = 1024 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 16 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Large-level Transformer models. View Source @dataclass class LargeTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for Large-level Transformer models. \"\"\" size : int = 1024 num_heads : int = 16 no : int = 24 Ancestors (in MRO) grid_transformer.parameters.TransformerParameters Class variables dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size MidTransformerParameters class MidTransformerParameters ( pos_emd : str = 'sum' , no : int = 4 , size : int = 256 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 8 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Mid-level Transformer models. View Source @dataclass class MidTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for Mid-level Transformer models. \"\"\" size : int = 256 num_heads : int = 8 no : int = 4 Ancestors (in MRO) grid_transformer.parameters.TransformerParameters Class variables dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size SmallTransformerParameters class SmallTransformerParameters ( pos_emd : str = 'sum' , no : int = 4 , size : int = 64 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 4 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Small-level Transformer models. View Source @dataclass class SmallTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for Small-level Transformer models. \"\"\" size : int = 64 num_heads : int = 4 no : int = 4 Ancestors (in MRO) grid_transformer.parameters.TransformerParameters Class variables dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size TransformerParameters class TransformerParameters ( pos_emd : str = 'sum' , no : int = 4 , size : int = 128 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 8 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Transformer models. Attributes Name Type Description Default pos_emd None Type of position embedding. Can be 'sum' or 'cat'. None no None Number of layers in the Transformer encoder. None size None Dimension of the hidden states and the self-attention layers. None ff_mul None Multiplier for the dimension of the feed-forward layers. None ff_size None Dimension of the feed-forward layers. If None, it is set to size * ff_mul. None dropout None Dropout rate for the self-attention and feed-forward layers. None num_heads None Number of heads in the multi-head self-attention layers. None ff_act None Activation function for the feed-forward layers. Can be 'relu' or 'gelu'. None extractor None Type of extractor to use before the transformer layers. Can be 'cnn' or 'rnn'. None extractor_pooling None Type of pooling to use after the extractor. Can be 'first' or 'flat2d'. None pooling None Type of pooling to use after the transformer layers. Can be 'first' or 'mean'. None out_layers None Tuple containing the number of neurons for the output layers. None show_shape None Whether to print shape of the intermediate tensors. None View Source @dataclass class TransformerParameters : \"\"\" Class that holds parameters for Transformer models. :param pos_emd: Type of position embedding. Can be 'sum' or 'cat'. :param no: Number of layers in the Transformer encoder. :param size: Dimension of the hidden states and the self-attention layers. :param ff_mul: Multiplier for the dimension of the feed-forward layers. :param ff_size: Dimension of the feed-forward layers. If None, it is set to size * ff_mul. :param dropout: Dropout rate for the self-attention and feed-forward layers. :param num_heads: Number of heads in the multi-head self-attention layers. :param ff_act: Activation function for the feed-forward layers. Can be 'relu' or 'gelu'. :param extractor: Type of extractor to use before the transformer layers. Can be 'cnn' or 'rnn'. :param extractor_pooling: Type of pooling to use after the extractor. Can be 'first' or 'flat2d'. :param pooling: Type of pooling to use after the transformer layers. Can be 'first' or 'mean'. :param out_layers: Tuple containing the number of neurons for the output layers. :param show_shape: Whether to print shape of the intermediate tensors. \"\"\" pos_emd : str = \"sum\" no : int = 4 size : int = 128 ff_mul : int = 4 ff_size : int = None dropout : float = 0.1 num_heads : int = 8 ff_act : str = \"gelu\" extractor : str = None extractor_pooling : str = \"flat2d\" pooling : str = \"first\" out_layers : Tuple = ( 1000 , 1000 ) show_shape : bool = True Descendants grid_transformer.parameters.GridTransformerParameters grid_transformer.parameters.MidTransformerParameters grid_transformer.parameters.SmallTransformerParameters grid_transformer.parameters.BigTransformerParameters grid_transformer.parameters.LargeTransformerParameters Class variables dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size","title":"Parameters"},{"location":"reference/grid_transformer/parameters/#module-grid_transformerparameters","text":"The module is a set of classes that define the parameters for Transformer models and GridTransformer models. The TransformerParameters class is a dataclass that holds the parameters for Transformer models, such as the number of layers, hidden state dimension, and dropout rate. The GridTransformerParameters class is a subclass of TransformerParameters that holds additional parameters specific to GridTransformer models, such as the extractor type, mapping function, and masking strategy. You can then pass the instance to a function or class that uses these parameters to define the architecture of the model. To use these classes, you can create an instance of the class with the desired values for the parameters. For example: from dataclasses import asdict params = TransformerParameters ( no = 6 , size = 256 , dropout = 0.2 ) model = transformer ( asdict ( params )) For GridTransformerParameters: params = GridTransformerParameters ( extractor = 'ef' , last = 'start' , last_index = 8 , col = 3 , row = 3 , return_attention_scores = True ) model = grid_trans ( asdict ( params )) View Source \"\"\" The module is a set of classes that define the parameters for Transformer models and GridTransformer models. The `TransformerParameters` class is a dataclass that holds the parameters for Transformer models, such as the number of layers, hidden state dimension, and dropout rate. The `GridTransformerParameters` class is a subclass of `TransformerParameters` that holds additional parameters specific to GridTransformer models, such as the extractor type, mapping function, and masking strategy. You can then pass the instance to a function or class that uses these parameters to define the architecture of the model. To use these classes, you can create an instance of the class with the desired values for the parameters. For example: ```python from dataclasses import asdict params = TransformerParameters(no=6, size=256, dropout=0.2) model = transformer(asdict(params)) ``` For GridTransformerParameters: ```python params = GridTransformerParameters(extractor='ef',last='start',last_index=8,col=3,row=3,return_attention_scores=True) model = grid_trans(asdict(params)) ``` \"\"\" from dataclasses import dataclass from typing import Tuple @dataclass class TransformerParameters : \"\"\" Class that holds parameters for Transformer models. :param pos_emd: Type of position embedding. Can be 'sum' or 'cat'. :param no: Number of layers in the Transformer encoder. :param size: Dimension of the hidden states and the self-attention layers. :param ff_mul: Multiplier for the dimension of the feed-forward layers. :param ff_size: Dimension of the feed-forward layers. If None, it is set to size * ff_mul. :param dropout: Dropout rate for the self-attention and feed-forward layers. :param num_heads: Number of heads in the multi-head self-attention layers. :param ff_act: Activation function for the feed-forward layers. Can be 'relu' or 'gelu'. :param extractor: Type of extractor to use before the transformer layers. Can be 'cnn' or 'rnn'. :param extractor_pooling: Type of pooling to use after the extractor. Can be 'first' or 'flat2d'. :param pooling: Type of pooling to use after the transformer layers. Can be 'first' or 'mean'. :param out_layers: Tuple containing the number of neurons for the output layers. :param show_shape: Whether to print shape of the intermediate tensors. \"\"\" pos_emd : str = \"sum\" no : int = 4 size : int = 128 ff_mul : int = 4 ff_size : int = None dropout : float = 0.1 num_heads : int = 8 ff_act : str = \"gelu\" extractor : str = None extractor_pooling : str = \"flat2d\" pooling : str = \"first\" out_layers : Tuple = ( 1000 , 1000 ) show_shape : bool = True @dataclass class GridTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for grid_transformer models. :param extractor: Type of extractor to use for the image sequence. Can be 'ef' for EfficientNet or 'res' for ResNet. :param extractor_shape: The shape of the extracted features from the extractor. :param last: How to handle the last element of the sequence, can be 'start' to make it the first element or 'end' to make it the last element :param map_fn: The function used to map the image sequence, can be 'batch' or 'map' :param last_index: The index of the last element of the sequence :param channel: The channel used to map the image sequence, can be 'channel' or 'spatial' :param batch_no: Number of batches to use for the map function :param col: Number of columns in the image sequence :param row: Number of rows in the image sequence :param return_extractor_input: Whether to return the input to the extractor :param return_attention_scores: Whether to return the attention scores :param mask: The masking strategy to use, can be 'last' or 'all' :param pre: Preprocessing function to use on the image sequence, can be 'normalize' or 'standardize' :param noise: Noise function to use on the image sequence, can be 'gaussian' or 'poisson' :param augmentation: Augmentation function to use on the image sequence, can be 'random_rotate' or 'random_flip' :param print_: Whether to print information about the model \"\"\" extractor : int = \"ef\" extractor_shape : int = 84 last : str = \"start\" map_fn : str = \"batch\" last_index : int = None channel : str = \"channel\" batch_no : int = None col : int = 3 row : int = 3 return_extractor_input : bool = False return_attention_scores : bool = False mask : str = \"last\" pre : str = None noise : str = None augmentation : str = None print_ : bool = False @dataclass class MidTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for Mid-level Transformer models. \"\"\" size : int = 256 num_heads : int = 8 no : int = 4 @dataclass class SmallTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for Small-level Transformer models. \"\"\" size : int = 64 num_heads : int = 4 no : int = 4 @dataclass class BigTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for Big-level Transformer models. \"\"\" size : int = 768 num_heads : int = 12 no : int = 12 @dataclass class LargeTransformerParameters ( TransformerParameters ): \"\"\" Class that holds parameters for Large-level Transformer models. \"\"\" size : int = 1024 num_heads : int = 16 no : int = 24","title":"Module grid_transformer.parameters"},{"location":"reference/grid_transformer/parameters/#classes","text":"","title":"Classes"},{"location":"reference/grid_transformer/parameters/#bigtransformerparameters","text":"class BigTransformerParameters ( pos_emd : str = 'sum' , no : int = 12 , size : int = 768 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 12 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Big-level Transformer models. View Source @dataclass class BigTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for Big-level Transformer models. \"\"\" size : int = 768 num_heads : int = 12 no : int = 12","title":"BigTransformerParameters"},{"location":"reference/grid_transformer/parameters/#ancestors-in-mro","text":"grid_transformer.parameters.TransformerParameters","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/parameters/#class-variables","text":"dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size","title":"Class variables"},{"location":"reference/grid_transformer/parameters/#gridtransformerparameters","text":"class GridTransformerParameters ( pos_emd : str = 'sum' , no : int = 4 , size : int = 128 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 8 , ff_act : str = 'gelu' , extractor : int = 'ef' , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True , extractor_shape : int = 84 , last : str = 'start' , map_fn : str = 'batch' , last_index : int = None , channel : str = 'channel' , batch_no : int = None , col : int = 3 , row : int = 3 , return_extractor_input : bool = False , return_attention_scores : bool = False , mask : str = 'last' , pre : str = None , noise : str = None , augmentation : str = None , print_ : bool = False ) Class that holds parameters for grid_transformer models.","title":"GridTransformerParameters"},{"location":"reference/grid_transformer/parameters/#attributes","text":"Name Type Description Default extractor None Type of extractor to use for the image sequence. Can be 'ef' for EfficientNet or 'res' for ResNet. None extractor_shape None The shape of the extracted features from the extractor. None last None How to handle the last element of the sequence, can be 'start' to make it the first element or 'end' to make it the last element None map_fn None The function used to map the image sequence, can be 'batch' or 'map' None last_index None The index of the last element of the sequence None channel None The channel used to map the image sequence, can be 'channel' or 'spatial' None batch_no None Number of batches to use for the map function None col None Number of columns in the image sequence None row None Number of rows in the image sequence None return_extractor_input None Whether to return the input to the extractor None return_attention_scores None Whether to return the attention scores None mask None The masking strategy to use, can be 'last' or 'all' None pre None Preprocessing function to use on the image sequence, can be 'normalize' or 'standardize' None noise None Noise function to use on the image sequence, can be 'gaussian' or 'poisson' None augmentation None Augmentation function to use on the image sequence, can be 'random_rotate' or 'random_flip' None print_ None Whether to print information about the model None View Source @dataclass class GridTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for grid_transformer models. :param extractor: Type of extractor to use for the image sequence. Can be 'ef' for EfficientNet or 'res' for ResNet. :param extractor_shape: The shape of the extracted features from the extractor. :param last: How to handle the last element of the sequence, can be 'start' to make it the first element or 'end' to make it the last element :param map_fn: The function used to map the image sequence, can be 'batch' or 'map' :param last_index: The index of the last element of the sequence :param channel: The channel used to map the image sequence, can be 'channel' or 'spatial' :param batch_no: Number of batches to use for the map function :param col: Number of columns in the image sequence :param row: Number of rows in the image sequence :param return_extractor_input: Whether to return the input to the extractor :param return_attention_scores: Whether to return the attention scores :param mask: The masking strategy to use, can be 'last' or 'all' :param pre: Preprocessing function to use on the image sequence, can be 'normalize' or 'standardize' :param noise: Noise function to use on the image sequence, can be 'gaussian' or 'poisson' :param augmentation: Augmentation function to use on the image sequence, can be 'random_rotate' or 'random_flip' :param print_: Whether to print information about the model \"\"\" extractor : int = \"ef\" extractor_shape : int = 84 last : str = \"start\" map_fn : str = \"batch\" last_index : int = None channel : str = \"channel\" batch_no : int = None col : int = 3 row : int = 3 return_extractor_input : bool = False return_attention_scores : bool = False mask : str = \"last\" pre : str = None noise : str = None augmentation : str = None print_ : bool = False","title":"Attributes"},{"location":"reference/grid_transformer/parameters/#ancestors-in-mro_1","text":"grid_transformer.parameters.TransformerParameters","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/parameters/#class-variables_1","text":"augmentation batch_no channel col dropout extractor extractor_pooling extractor_shape ff_act ff_mul ff_size last last_index map_fn mask no noise num_heads out_layers pooling pos_emd pre print_ return_attention_scores return_extractor_input row show_shape size","title":"Class variables"},{"location":"reference/grid_transformer/parameters/#largetransformerparameters","text":"class LargeTransformerParameters ( pos_emd : str = 'sum' , no : int = 24 , size : int = 1024 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 16 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Large-level Transformer models. View Source @dataclass class LargeTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for Large-level Transformer models. \"\"\" size : int = 1024 num_heads : int = 16 no : int = 24","title":"LargeTransformerParameters"},{"location":"reference/grid_transformer/parameters/#ancestors-in-mro_2","text":"grid_transformer.parameters.TransformerParameters","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/parameters/#class-variables_2","text":"dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size","title":"Class variables"},{"location":"reference/grid_transformer/parameters/#midtransformerparameters","text":"class MidTransformerParameters ( pos_emd : str = 'sum' , no : int = 4 , size : int = 256 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 8 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Mid-level Transformer models. View Source @dataclass class MidTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for Mid-level Transformer models. \"\"\" size : int = 256 num_heads : int = 8 no : int = 4","title":"MidTransformerParameters"},{"location":"reference/grid_transformer/parameters/#ancestors-in-mro_3","text":"grid_transformer.parameters.TransformerParameters","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/parameters/#class-variables_3","text":"dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size","title":"Class variables"},{"location":"reference/grid_transformer/parameters/#smalltransformerparameters","text":"class SmallTransformerParameters ( pos_emd : str = 'sum' , no : int = 4 , size : int = 64 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 4 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Small-level Transformer models. View Source @dataclass class SmallTransformerParameters ( TransformerParameters ) : \"\"\" Class that holds parameters for Small-level Transformer models. \"\"\" size : int = 64 num_heads : int = 4 no : int = 4","title":"SmallTransformerParameters"},{"location":"reference/grid_transformer/parameters/#ancestors-in-mro_4","text":"grid_transformer.parameters.TransformerParameters","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/parameters/#class-variables_4","text":"dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size","title":"Class variables"},{"location":"reference/grid_transformer/parameters/#transformerparameters","text":"class TransformerParameters ( pos_emd : str = 'sum' , no : int = 4 , size : int = 128 , ff_mul : int = 4 , ff_size : int = None , dropout : float = 0.1 , num_heads : int = 8 , ff_act : str = 'gelu' , extractor : str = None , extractor_pooling : str = 'flat2d' , pooling : str = 'first' , out_layers : Tuple = ( 1000 , 1000 ), show_shape : bool = True ) Class that holds parameters for Transformer models.","title":"TransformerParameters"},{"location":"reference/grid_transformer/parameters/#attributes_1","text":"Name Type Description Default pos_emd None Type of position embedding. Can be 'sum' or 'cat'. None no None Number of layers in the Transformer encoder. None size None Dimension of the hidden states and the self-attention layers. None ff_mul None Multiplier for the dimension of the feed-forward layers. None ff_size None Dimension of the feed-forward layers. If None, it is set to size * ff_mul. None dropout None Dropout rate for the self-attention and feed-forward layers. None num_heads None Number of heads in the multi-head self-attention layers. None ff_act None Activation function for the feed-forward layers. Can be 'relu' or 'gelu'. None extractor None Type of extractor to use before the transformer layers. Can be 'cnn' or 'rnn'. None extractor_pooling None Type of pooling to use after the extractor. Can be 'first' or 'flat2d'. None pooling None Type of pooling to use after the transformer layers. Can be 'first' or 'mean'. None out_layers None Tuple containing the number of neurons for the output layers. None show_shape None Whether to print shape of the intermediate tensors. None View Source @dataclass class TransformerParameters : \"\"\" Class that holds parameters for Transformer models. :param pos_emd: Type of position embedding. Can be 'sum' or 'cat'. :param no: Number of layers in the Transformer encoder. :param size: Dimension of the hidden states and the self-attention layers. :param ff_mul: Multiplier for the dimension of the feed-forward layers. :param ff_size: Dimension of the feed-forward layers. If None, it is set to size * ff_mul. :param dropout: Dropout rate for the self-attention and feed-forward layers. :param num_heads: Number of heads in the multi-head self-attention layers. :param ff_act: Activation function for the feed-forward layers. Can be 'relu' or 'gelu'. :param extractor: Type of extractor to use before the transformer layers. Can be 'cnn' or 'rnn'. :param extractor_pooling: Type of pooling to use after the extractor. Can be 'first' or 'flat2d'. :param pooling: Type of pooling to use after the transformer layers. Can be 'first' or 'mean'. :param out_layers: Tuple containing the number of neurons for the output layers. :param show_shape: Whether to print shape of the intermediate tensors. \"\"\" pos_emd : str = \"sum\" no : int = 4 size : int = 128 ff_mul : int = 4 ff_size : int = None dropout : float = 0.1 num_heads : int = 8 ff_act : str = \"gelu\" extractor : str = None extractor_pooling : str = \"flat2d\" pooling : str = \"first\" out_layers : Tuple = ( 1000 , 1000 ) show_shape : bool = True","title":"Attributes"},{"location":"reference/grid_transformer/parameters/#descendants","text":"grid_transformer.parameters.GridTransformerParameters grid_transformer.parameters.MidTransformerParameters grid_transformer.parameters.SmallTransformerParameters grid_transformer.parameters.BigTransformerParameters grid_transformer.parameters.LargeTransformerParameters","title":"Descendants"},{"location":"reference/grid_transformer/parameters/#class-variables_5","text":"dropout extractor extractor_pooling ff_act ff_mul ff_size no num_heads out_layers pooling pos_emd show_shape size","title":"Class variables"},{"location":"reference/grid_transformer/position_embedding/","text":"Module grid_transformer.position_embedding The module contains models for adding positional embeddings to tokens. The TokenAndPositionEmbedding class is a Keras layer that concatenates token embeddings with position embeddings. The layer takes in three parameters - maxlen which is the maximum length of input sequences, vocab_size which is the vocabulary size of the input sequences and embed_dim which is the dimension of the embeddings. The layer has a call method that takes in an input tensor x with shape (batch_size, sequence_length) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the token_emb Embedding layer to embed the input tensor and pos_emb Embedding layer to embed the positions. Finally, it adds the position embeddings to the token embeddings and returns the concatenated tensor. The CatPositionEmbedding class is a Keras layer that concatenates position embeddings to the input tensor. The layer takes in a single parameter embed_dim which is the dimension of the position embeddings. The layer has a build method that takes in an input shape tuple (batch_size, sequence_length) and creates an Embedding layer pos_emb with input_dim as the second element of the input shape tuple and output_dim as the embed_dim. The layer also has a call method that takes in an input tensor x with shape (batch_size, sequence_length) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the pos_emb Embedding layer to embed the positions and concatenates it with the input tensor along the last axis. The SumPositionEmbedding class is a Keras layer that sums position embeddings to the input tensor. It has a build method that takes in an input shape tuple (batch_size, sequence_length, embed_dim) and creates an Embedding layer pos_emb with input_dim as the second element of the input shape tuple and output_dim as the third element of the input shape tuple. The layer also has a call method that takes in an input tensor x with shape (batch_size, sequence_length, embed_dim) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the pos_emb Embedding layer to embed the positions and adds it to the input tensor. View Source \" \"\" The module contains models for adding positional embeddings to tokens. The `TokenAndPositionEmbedding` class is a Keras layer that concatenates token embeddings with position embeddings. The layer takes in three parameters - `maxlen` which is the maximum length of input sequences, `vocab_size` which is the vocabulary size of the input sequences and `embed_dim` which is the dimension of the embeddings. The layer has a call method that takes in an input tensor `x` with shape (batch_size, sequence_length) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the token_emb Embedding layer to embed the input tensor and pos_emb Embedding layer to embed the positions. Finally, it adds the position embeddings to the token embeddings and returns the concatenated tensor. The `CatPositionEmbedding` class is a Keras layer that concatenates position embeddings to the input tensor. The layer takes in a single parameter `embed_dim` which is the dimension of the position embeddings. The layer has a build method that takes in an input shape tuple (batch_size, sequence_length) and creates an Embedding layer pos_emb with input_dim as the second element of the input shape tuple and output_dim as the embed_dim. The layer also has a call method that takes in an input tensor `x` with shape (batch_size, sequence_length) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the pos_emb Embedding layer to embed the positions and concatenates it with the input tensor along the last axis. The `SumPositionEmbedding` class is a Keras layer that sums position embeddings to the input tensor. It has a build method that takes in an input shape tuple (batch_size, sequence_length, embed_dim) and creates an Embedding layer pos_emb with input_dim as the second element of the input shape tuple and output_dim as the third element of the input shape tuple. The layer also has a call method that takes in an input tensor `x` with shape (batch_size, sequence_length, embed_dim) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the pos_emb Embedding layer to embed the positions and adds it to the input tensor. \"\" \" from typing import Tuple import tensorflow as tf from tensorflow . keras . layers import Layer , Embedding class TokenAndPositionEmbedding ( Layer ) : \" \"\" A Keras Layer that concatenates token embeddings with position embeddings. :param maxlen: The maximum length of input sequences. :param vocab_size: The vocabulary size of the input sequences. :param embed_dim: The dimension of the embeddings. \"\" \" def __init__ ( self , maxlen : int , vocab_size : int , embed_dim : int ) : super ( TokenAndPositionEmbedding , self ). __init__ () self . token_emb = Embedding ( input_dim = vocab_size , output_dim = embed_dim ) self . pos_emb = Embedding ( input_dim = maxlen , output_dim = embed_dim ) def call ( self , x : tf . Tensor ) -> tf . Tensor : \" \"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\" \" maxlen = tf . shape ( x ) [ - 1 ] positions = tf . range ( start = 0 , limit = maxlen , delta = 1 ) positions = self . pos_emb ( positions ) x = self . token_emb ( x ) return x + positions class CatPositionEmbedding ( Layer ) : \" \"\" A Keras Layer that concatenates position embeddings to the input tensor. :param embed_dim: The dimension of the position embeddings. \"\" \" def __init__ ( self , embed_dim : int = 16 ) : super (). __init__ () self . embed_dim = embed_dim def build ( self , input_shape : Tuple [ int , int ] ) : \" \"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length) \"\" \" self . pos_emb = Embedding ( input_dim = input_shape [ 1 ] , output_dim = self . embed_dim ) super (). build ( input_shape ) def call ( self , x : tf . Tensor ) -> tf . Tensor : \" \"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\" \" positions = tf . range ( start = 0 , limit = tf . shape ( x ) [ 1 ] , delta = 1 ) positions = self . pos_emb ( positions ) positions = tf . expand_dims ( positions , axis = 0 ) positions = tf . tile ( positions , [ tf . shape ( x ) [ 0 ] , 1 , 1 ] ) return tf . concat ( [ x , positions ] , axis =- 1 ) class SumPositionEmbedding ( Layer ) : \" \"\" A Keras Layer that sums position embeddings to the input tensor. \"\" \" def build ( self , input_shape : Tuple [ int , int , int ] ) : \" \"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length, embed_dim) \"\" \" self . pos_emb = Embedding ( input_dim = input_shape [ 1 ] , output_dim = input_shape [ 2 ] ) super (). build ( input_shape ) def call ( self , x : tf . Tensor ) -> tf . Tensor : \" \"\" :param x: Input tensor with shape (batch_size, sequence_length, embed_dim) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\" \" positions = tf . range ( start = 0 , limit = tf . shape ( x ) [ 1 ] , delta = 1 ) positions = self . pos_emb ( positions ) return x + positions Classes CatPositionEmbedding class CatPositionEmbedding ( embed_dim : int = 16 ) A Keras Layer that concatenates position embeddings to the input tensor. Attributes Name Type Description Default embed_dim None The dimension of the position embeddings. None View Source class CatPositionEmbedding ( Layer ): \"\"\" A Keras Layer that concatenates position embeddings to the input tensor. :param embed_dim: The dimension of the position embeddings. \"\"\" def __init__ ( self , embed_dim: int = 16 ): super (). __init__ () self . embed_dim = embed_dim def build ( self , input_shape: Tuple [ int , int ]): \"\"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length) \"\"\" self . pos_emb = Embedding ( input_dim = input_shape [ 1 ], output_dim = self . embed_dim ) super (). build ( input_shape ) def call ( self , x : tf . Tensor ) -> tf . Tensor: \"\"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" positions = tf . range ( start = 0 , limit = tf . shape ( x )[ 1 ], delta = 1 ) positions = self . pos_emb ( positions ) positions = tf . expand_dims ( positions , axis = 0 ) positions = tf . tile ( positions , [ tf . shape ( x )[ 0 ], 1 , 1 ]) return tf . concat ([ x , positions ], axis =- 1 ) Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape : Tuple [ int , int ] ) Builds the layer Parameters: Name Type Description Default input_shape None The shape of the input, in the format (batch_size, sequence_length) None View Source def build(self, input_shape: Tuple[int, int]): \"\"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length) \"\"\" self.pos_emb = Embedding(input_dim=input_shape[1], output_dim=self.embed_dim) super().build(input_shape) call def call ( self , x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Parameters: Name Type Description Default x None Input tensor with shape (batch_size, sequence_length) None Returns: Type Description None Tensor with shape (batch_size, sequence_length, embed_dim) View Source def call ( self , x : tf . Tensor ) -> tf . Tensor : \"\"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" positions = tf . range ( start = 0 , limit = tf . shape ( x )[ 1 ], delta = 1 ) positions = self . pos_emb ( positions ) positions = tf . expand_dims ( positions , axis = 0 ) positions = tf . tile ( positions , [ tf . shape ( x )[ 0 ], 1 , 1 ]) return tf . concat ([ x , positions ], axis =- 1 ) compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weights def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state () SumPositionEmbedding class SumPositionEmbedding ( trainable = True , name = None , dtype = None , dynamic = False , ** kwargs ) A Keras Layer that sums position embeddings to the input tensor. View Source class SumPositionEmbedding ( Layer ): \"\"\" A Keras Layer that sums position embeddings to the input tensor. \"\"\" def build ( self , input_shape: Tuple [ int , int , int ]): \"\"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length, embed_dim) \"\"\" self . pos_emb = Embedding ( input_dim = input_shape [ 1 ], output_dim = input_shape [ 2 ]) super (). build ( input_shape ) def call ( self , x : tf . Tensor ) -> tf . Tensor: \"\"\" :param x: Input tensor with shape (batch_size, sequence_length, embed_dim) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" positions = tf . range ( start = 0 , limit = tf . shape ( x )[ 1 ], delta = 1 ) positions = self . pos_emb ( positions ) return x + positions Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape : Tuple [ int , int , int ] ) Builds the layer Parameters: Name Type Description Default input_shape None The shape of the input, in the format (batch_size, sequence_length, embed_dim) None View Source def build(self, input_shape: Tuple[int, int, int]): \"\"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length, embed_dim) \"\"\" self.pos_emb = Embedding(input_dim=input_shape[1], output_dim=input_shape[2]) super().build(input_shape) call def call ( self , x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Parameters: Name Type Description Default x None Input tensor with shape (batch_size, sequence_length, embed_dim) None Returns: Type Description None Tensor with shape (batch_size, sequence_length, embed_dim) View Source def call ( self , x : tf . Tensor ) -> tf . Tensor : \"\"\" :param x: Input tensor with shape (batch_size, sequence_length, embed_dim) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" positions = tf . range ( start = 0 , limit = tf . shape ( x )[ 1 ], delta = 1 ) positions = self . pos_emb ( positions ) return x + positions compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weights def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state () TokenAndPositionEmbedding class TokenAndPositionEmbedding ( maxlen : int , vocab_size : int , embed_dim : int ) A Keras Layer that concatenates token embeddings with position embeddings. Attributes Name Type Description Default maxlen None The maximum length of input sequences. None vocab_size None The vocabulary size of the input sequences. None embed_dim None The dimension of the embeddings. None View Source class TokenAndPositionEmbedding ( Layer ): \"\"\" A Keras Layer that concatenates token embeddings with position embeddings. :param maxlen: The maximum length of input sequences. :param vocab_size: The vocabulary size of the input sequences. :param embed_dim: The dimension of the embeddings. \"\"\" def __init__ ( self , maxlen: int , vocab_size: int , embed_dim: int ): super ( TokenAndPositionEmbedding , self ). __init__ () self . token_emb = Embedding ( input_dim = vocab_size , output_dim = embed_dim ) self . pos_emb = Embedding ( input_dim = maxlen , output_dim = embed_dim ) def call ( self , x : tf . Tensor ) -> tf . Tensor: \"\"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" maxlen = tf . shape ( x )[- 1 ] positions = tf . range ( start = 0 , limit = maxlen , delta = 1 ) positions = self . pos_emb ( positions ) x = self . token_emb ( x ) return x + positions Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of call() . This is typically used to create the weights of Layer subclasses (at the discretion of the subclass implementer). Parameters: Name Type Description Default input_shape None Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). None View Source @tf.__internal__.tracking.no_automatic_dependency_tracking @generic_utils.default def build ( self , input_shape ) : \" \"\" Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of `call()`. This is typically used to create the weights of `Layer` subclasses (at the discretion of the subclass implementer). Args: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). \"\" \" self . _build_input_shape = input_shape self . built = True call def call ( self , x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Parameters: Name Type Description Default x None Input tensor with shape (batch_size, sequence_length) None Returns: Type Description None Tensor with shape (batch_size, sequence_length, embed_dim) View Source def call ( self , x : tf . Tensor ) -> tf . Tensor : \"\"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" maxlen = tf . shape ( x )[ - 1 ] positions = tf . range ( start = 0 , limit = maxlen , delta = 1 ) positions = self . pos_emb ( positions ) x = self . token_emb ( x ) return x + positions compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weights def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"Position Embedding"},{"location":"reference/grid_transformer/position_embedding/#module-grid_transformerposition_embedding","text":"The module contains models for adding positional embeddings to tokens. The TokenAndPositionEmbedding class is a Keras layer that concatenates token embeddings with position embeddings. The layer takes in three parameters - maxlen which is the maximum length of input sequences, vocab_size which is the vocabulary size of the input sequences and embed_dim which is the dimension of the embeddings. The layer has a call method that takes in an input tensor x with shape (batch_size, sequence_length) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the token_emb Embedding layer to embed the input tensor and pos_emb Embedding layer to embed the positions. Finally, it adds the position embeddings to the token embeddings and returns the concatenated tensor. The CatPositionEmbedding class is a Keras layer that concatenates position embeddings to the input tensor. The layer takes in a single parameter embed_dim which is the dimension of the position embeddings. The layer has a build method that takes in an input shape tuple (batch_size, sequence_length) and creates an Embedding layer pos_emb with input_dim as the second element of the input shape tuple and output_dim as the embed_dim. The layer also has a call method that takes in an input tensor x with shape (batch_size, sequence_length) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the pos_emb Embedding layer to embed the positions and concatenates it with the input tensor along the last axis. The SumPositionEmbedding class is a Keras layer that sums position embeddings to the input tensor. It has a build method that takes in an input shape tuple (batch_size, sequence_length, embed_dim) and creates an Embedding layer pos_emb with input_dim as the second element of the input shape tuple and output_dim as the third element of the input shape tuple. The layer also has a call method that takes in an input tensor x with shape (batch_size, sequence_length, embed_dim) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the pos_emb Embedding layer to embed the positions and adds it to the input tensor. View Source \" \"\" The module contains models for adding positional embeddings to tokens. The `TokenAndPositionEmbedding` class is a Keras layer that concatenates token embeddings with position embeddings. The layer takes in three parameters - `maxlen` which is the maximum length of input sequences, `vocab_size` which is the vocabulary size of the input sequences and `embed_dim` which is the dimension of the embeddings. The layer has a call method that takes in an input tensor `x` with shape (batch_size, sequence_length) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the token_emb Embedding layer to embed the input tensor and pos_emb Embedding layer to embed the positions. Finally, it adds the position embeddings to the token embeddings and returns the concatenated tensor. The `CatPositionEmbedding` class is a Keras layer that concatenates position embeddings to the input tensor. The layer takes in a single parameter `embed_dim` which is the dimension of the position embeddings. The layer has a build method that takes in an input shape tuple (batch_size, sequence_length) and creates an Embedding layer pos_emb with input_dim as the second element of the input shape tuple and output_dim as the embed_dim. The layer also has a call method that takes in an input tensor `x` with shape (batch_size, sequence_length) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the pos_emb Embedding layer to embed the positions and concatenates it with the input tensor along the last axis. The `SumPositionEmbedding` class is a Keras layer that sums position embeddings to the input tensor. It has a build method that takes in an input shape tuple (batch_size, sequence_length, embed_dim) and creates an Embedding layer pos_emb with input_dim as the second element of the input shape tuple and output_dim as the third element of the input shape tuple. The layer also has a call method that takes in an input tensor `x` with shape (batch_size, sequence_length, embed_dim) and returns a tensor with shape (batch_size, sequence_length, embed_dim). In the call method, it uses the pos_emb Embedding layer to embed the positions and adds it to the input tensor. \"\" \" from typing import Tuple import tensorflow as tf from tensorflow . keras . layers import Layer , Embedding class TokenAndPositionEmbedding ( Layer ) : \" \"\" A Keras Layer that concatenates token embeddings with position embeddings. :param maxlen: The maximum length of input sequences. :param vocab_size: The vocabulary size of the input sequences. :param embed_dim: The dimension of the embeddings. \"\" \" def __init__ ( self , maxlen : int , vocab_size : int , embed_dim : int ) : super ( TokenAndPositionEmbedding , self ). __init__ () self . token_emb = Embedding ( input_dim = vocab_size , output_dim = embed_dim ) self . pos_emb = Embedding ( input_dim = maxlen , output_dim = embed_dim ) def call ( self , x : tf . Tensor ) -> tf . Tensor : \" \"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\" \" maxlen = tf . shape ( x ) [ - 1 ] positions = tf . range ( start = 0 , limit = maxlen , delta = 1 ) positions = self . pos_emb ( positions ) x = self . token_emb ( x ) return x + positions class CatPositionEmbedding ( Layer ) : \" \"\" A Keras Layer that concatenates position embeddings to the input tensor. :param embed_dim: The dimension of the position embeddings. \"\" \" def __init__ ( self , embed_dim : int = 16 ) : super (). __init__ () self . embed_dim = embed_dim def build ( self , input_shape : Tuple [ int , int ] ) : \" \"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length) \"\" \" self . pos_emb = Embedding ( input_dim = input_shape [ 1 ] , output_dim = self . embed_dim ) super (). build ( input_shape ) def call ( self , x : tf . Tensor ) -> tf . Tensor : \" \"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\" \" positions = tf . range ( start = 0 , limit = tf . shape ( x ) [ 1 ] , delta = 1 ) positions = self . pos_emb ( positions ) positions = tf . expand_dims ( positions , axis = 0 ) positions = tf . tile ( positions , [ tf . shape ( x ) [ 0 ] , 1 , 1 ] ) return tf . concat ( [ x , positions ] , axis =- 1 ) class SumPositionEmbedding ( Layer ) : \" \"\" A Keras Layer that sums position embeddings to the input tensor. \"\" \" def build ( self , input_shape : Tuple [ int , int , int ] ) : \" \"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length, embed_dim) \"\" \" self . pos_emb = Embedding ( input_dim = input_shape [ 1 ] , output_dim = input_shape [ 2 ] ) super (). build ( input_shape ) def call ( self , x : tf . Tensor ) -> tf . Tensor : \" \"\" :param x: Input tensor with shape (batch_size, sequence_length, embed_dim) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\" \" positions = tf . range ( start = 0 , limit = tf . shape ( x ) [ 1 ] , delta = 1 ) positions = self . pos_emb ( positions ) return x + positions","title":"Module grid_transformer.position_embedding"},{"location":"reference/grid_transformer/position_embedding/#classes","text":"","title":"Classes"},{"location":"reference/grid_transformer/position_embedding/#catpositionembedding","text":"class CatPositionEmbedding ( embed_dim : int = 16 ) A Keras Layer that concatenates position embeddings to the input tensor.","title":"CatPositionEmbedding"},{"location":"reference/grid_transformer/position_embedding/#attributes","text":"Name Type Description Default embed_dim None The dimension of the position embeddings. None View Source class CatPositionEmbedding ( Layer ): \"\"\" A Keras Layer that concatenates position embeddings to the input tensor. :param embed_dim: The dimension of the position embeddings. \"\"\" def __init__ ( self , embed_dim: int = 16 ): super (). __init__ () self . embed_dim = embed_dim def build ( self , input_shape: Tuple [ int , int ]): \"\"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length) \"\"\" self . pos_emb = Embedding ( input_dim = input_shape [ 1 ], output_dim = self . embed_dim ) super (). build ( input_shape ) def call ( self , x : tf . Tensor ) -> tf . Tensor: \"\"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" positions = tf . range ( start = 0 , limit = tf . shape ( x )[ 1 ], delta = 1 ) positions = self . pos_emb ( positions ) positions = tf . expand_dims ( positions , axis = 0 ) positions = tf . tile ( positions , [ tf . shape ( x )[ 0 ], 1 , 1 ]) return tf . concat ([ x , positions ], axis =- 1 )","title":"Attributes"},{"location":"reference/grid_transformer/position_embedding/#ancestors-in-mro","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/position_embedding/#static-methods","text":"","title":"Static methods"},{"location":"reference/grid_transformer/position_embedding/#from_config","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/grid_transformer/position_embedding/#with_name_scope","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/position_embedding/#instance-variables","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights.","title":"Instance variables"},{"location":"reference/grid_transformer/position_embedding/#methods","text":"","title":"Methods"},{"location":"reference/grid_transformer/position_embedding/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/position_embedding/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/position_embedding/#add_update","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/position_embedding/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/position_embedding/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/position_embedding/#build","text":"def build ( self , input_shape : Tuple [ int , int ] ) Builds the layer Parameters: Name Type Description Default input_shape None The shape of the input, in the format (batch_size, sequence_length) None View Source def build(self, input_shape: Tuple[int, int]): \"\"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length) \"\"\" self.pos_emb = Embedding(input_dim=input_shape[1], output_dim=self.embed_dim) super().build(input_shape)","title":"build"},{"location":"reference/grid_transformer/position_embedding/#call","text":"def call ( self , x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Parameters: Name Type Description Default x None Input tensor with shape (batch_size, sequence_length) None Returns: Type Description None Tensor with shape (batch_size, sequence_length, embed_dim) View Source def call ( self , x : tf . Tensor ) -> tf . Tensor : \"\"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" positions = tf . range ( start = 0 , limit = tf . shape ( x )[ 1 ], delta = 1 ) positions = self . pos_emb ( positions ) positions = tf . expand_dims ( positions , axis = 0 ) positions = tf . tile ( positions , [ tf . shape ( x )[ 0 ], 1 , 1 ]) return tf . concat ([ x , positions ], axis =- 1 )","title":"call"},{"location":"reference/grid_transformer/position_embedding/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/position_embedding/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/position_embedding/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/position_embedding/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/position_embedding/#finalize_state","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/position_embedding/#get_config","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) )","title":"get_config"},{"location":"reference/grid_transformer/position_embedding/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/position_embedding/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/position_embedding/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/position_embedding/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/position_embedding/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/position_embedding/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/position_embedding/#get_weights","text":"def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/grid_transformer/position_embedding/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/position_embedding/#sumpositionembedding","text":"class SumPositionEmbedding ( trainable = True , name = None , dtype = None , dynamic = False , ** kwargs ) A Keras Layer that sums position embeddings to the input tensor. View Source class SumPositionEmbedding ( Layer ): \"\"\" A Keras Layer that sums position embeddings to the input tensor. \"\"\" def build ( self , input_shape: Tuple [ int , int , int ]): \"\"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length, embed_dim) \"\"\" self . pos_emb = Embedding ( input_dim = input_shape [ 1 ], output_dim = input_shape [ 2 ]) super (). build ( input_shape ) def call ( self , x : tf . Tensor ) -> tf . Tensor: \"\"\" :param x: Input tensor with shape (batch_size, sequence_length, embed_dim) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" positions = tf . range ( start = 0 , limit = tf . shape ( x )[ 1 ], delta = 1 ) positions = self . pos_emb ( positions ) return x + positions","title":"SumPositionEmbedding"},{"location":"reference/grid_transformer/position_embedding/#ancestors-in-mro_1","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/position_embedding/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/grid_transformer/position_embedding/#from_config_1","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/grid_transformer/position_embedding/#with_name_scope_1","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/position_embedding/#instance-variables_1","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights.","title":"Instance variables"},{"location":"reference/grid_transformer/position_embedding/#methods_1","text":"","title":"Methods"},{"location":"reference/grid_transformer/position_embedding/#add_loss_1","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/position_embedding/#add_metric_1","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/position_embedding/#add_update_1","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/position_embedding/#add_variable_1","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/position_embedding/#add_weight_1","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/position_embedding/#build_1","text":"def build ( self , input_shape : Tuple [ int , int , int ] ) Builds the layer Parameters: Name Type Description Default input_shape None The shape of the input, in the format (batch_size, sequence_length, embed_dim) None View Source def build(self, input_shape: Tuple[int, int, int]): \"\"\" Builds the layer :param input_shape: The shape of the input, in the format (batch_size, sequence_length, embed_dim) \"\"\" self.pos_emb = Embedding(input_dim=input_shape[1], output_dim=input_shape[2]) super().build(input_shape)","title":"build"},{"location":"reference/grid_transformer/position_embedding/#call_1","text":"def call ( self , x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Parameters: Name Type Description Default x None Input tensor with shape (batch_size, sequence_length, embed_dim) None Returns: Type Description None Tensor with shape (batch_size, sequence_length, embed_dim) View Source def call ( self , x : tf . Tensor ) -> tf . Tensor : \"\"\" :param x: Input tensor with shape (batch_size, sequence_length, embed_dim) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" positions = tf . range ( start = 0 , limit = tf . shape ( x )[ 1 ], delta = 1 ) positions = self . pos_emb ( positions ) return x + positions","title":"call"},{"location":"reference/grid_transformer/position_embedding/#compute_mask_1","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/position_embedding/#compute_output_shape_1","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/position_embedding/#compute_output_signature_1","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/position_embedding/#count_params_1","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/position_embedding/#finalize_state_1","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/position_embedding/#get_config_1","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) )","title":"get_config"},{"location":"reference/grid_transformer/position_embedding/#get_input_at_1","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/position_embedding/#get_input_mask_at_1","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/position_embedding/#get_input_shape_at_1","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/position_embedding/#get_output_at_1","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/position_embedding/#get_output_mask_at_1","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/position_embedding/#get_output_shape_at_1","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/position_embedding/#get_weights_1","text":"def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/grid_transformer/position_embedding/#set_weights_1","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/position_embedding/#tokenandpositionembedding","text":"class TokenAndPositionEmbedding ( maxlen : int , vocab_size : int , embed_dim : int ) A Keras Layer that concatenates token embeddings with position embeddings.","title":"TokenAndPositionEmbedding"},{"location":"reference/grid_transformer/position_embedding/#attributes_1","text":"Name Type Description Default maxlen None The maximum length of input sequences. None vocab_size None The vocabulary size of the input sequences. None embed_dim None The dimension of the embeddings. None View Source class TokenAndPositionEmbedding ( Layer ): \"\"\" A Keras Layer that concatenates token embeddings with position embeddings. :param maxlen: The maximum length of input sequences. :param vocab_size: The vocabulary size of the input sequences. :param embed_dim: The dimension of the embeddings. \"\"\" def __init__ ( self , maxlen: int , vocab_size: int , embed_dim: int ): super ( TokenAndPositionEmbedding , self ). __init__ () self . token_emb = Embedding ( input_dim = vocab_size , output_dim = embed_dim ) self . pos_emb = Embedding ( input_dim = maxlen , output_dim = embed_dim ) def call ( self , x : tf . Tensor ) -> tf . Tensor: \"\"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" maxlen = tf . shape ( x )[- 1 ] positions = tf . range ( start = 0 , limit = maxlen , delta = 1 ) positions = self . pos_emb ( positions ) x = self . token_emb ( x ) return x + positions","title":"Attributes"},{"location":"reference/grid_transformer/position_embedding/#ancestors-in-mro_2","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/position_embedding/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/grid_transformer/position_embedding/#from_config_2","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/grid_transformer/position_embedding/#with_name_scope_2","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/position_embedding/#instance-variables_2","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights.","title":"Instance variables"},{"location":"reference/grid_transformer/position_embedding/#methods_2","text":"","title":"Methods"},{"location":"reference/grid_transformer/position_embedding/#add_loss_2","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/position_embedding/#add_metric_2","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/position_embedding/#add_update_2","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/position_embedding/#add_variable_2","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/position_embedding/#add_weight_2","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/position_embedding/#build_2","text":"def build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of call() . This is typically used to create the weights of Layer subclasses (at the discretion of the subclass implementer). Parameters: Name Type Description Default input_shape None Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). None View Source @tf.__internal__.tracking.no_automatic_dependency_tracking @generic_utils.default def build ( self , input_shape ) : \" \"\" Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of `call()`. This is typically used to create the weights of `Layer` subclasses (at the discretion of the subclass implementer). Args: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). \"\" \" self . _build_input_shape = input_shape self . built = True","title":"build"},{"location":"reference/grid_transformer/position_embedding/#call_2","text":"def call ( self , x : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Parameters: Name Type Description Default x None Input tensor with shape (batch_size, sequence_length) None Returns: Type Description None Tensor with shape (batch_size, sequence_length, embed_dim) View Source def call ( self , x : tf . Tensor ) -> tf . Tensor : \"\"\" :param x: Input tensor with shape (batch_size, sequence_length) :return: Tensor with shape (batch_size, sequence_length, embed_dim) \"\"\" maxlen = tf . shape ( x )[ - 1 ] positions = tf . range ( start = 0 , limit = maxlen , delta = 1 ) positions = self . pos_emb ( positions ) x = self . token_emb ( x ) return x + positions","title":"call"},{"location":"reference/grid_transformer/position_embedding/#compute_mask_2","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/position_embedding/#compute_output_shape_2","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/position_embedding/#compute_output_signature_2","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/position_embedding/#count_params_2","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/position_embedding/#finalize_state_2","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/position_embedding/#get_config_2","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Note that `get_config()` does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Python dictionary. \"\" \" config = { \"name\" : self . name , \"trainable\" : self . trainable , } config [ \"dtype\" ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , \"_batch_input_shape\" ) : config [ \"batch_input_shape\" ] = self . _batch_input_shape if not generic_utils . is_default ( self . get_config ) : # In this case the subclass implements get_config() return config # In this case the subclass doesn't implement get_config(): # Let's see if we can autogenerate it. if getattr ( self , \"_auto_get_config\" , False ) : config . update ( self . _auto_config . config ) return config else : raise NotImplementedError ( textwrap . dedent ( f \" \"\" Layer {self.__class__.__name__} was created by passing non-serializable argument values in `__init__()`, and therefore the layer must override `get_config()` in order to be serializable. Please implement `get_config()`. Example: class CustomLayer(keras.layers.Layer): def __init__(self, arg1, arg2, **kwargs): super().__init__(**kwargs) self.arg1 = arg1 self.arg2 = arg2 def get_config(self): config = super().get_config() config.update({{ \" arg1 \": self.arg1, \" arg2 \": self.arg2, }}) return config \"\" \" ) )","title":"get_config"},{"location":"reference/grid_transformer/position_embedding/#get_input_at_2","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/position_embedding/#get_input_mask_at_2","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/position_embedding/#get_input_shape_at_2","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/position_embedding/#get_output_at_2","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/position_embedding/#get_output_mask_at_2","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/position_embedding/#get_output_shape_at_2","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/position_embedding/#get_weights_2","text":"def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/grid_transformer/position_embedding/#set_weights_2","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/preprocessing/","text":"Module grid_transformer.preprocessing This module contains functions for preprocessing input data for use in a transformer model. The functions in this module are used to create the initial grid for the model. The get_matrix function takes in an input tensor and an index tensor, and returns a new tensor that is a concatenation of the first last_index elements of the input tensor, and the element at the index specified in the index tensor. The last_index parameter defaults to 8. The get_images function takes in a tuple containing the input tensor and the indices tensor, and returns the output tensor obtained by calling the get_matrix function with these inputs. The last_index parameter defaults to 8. The get_images_with_index function takes in an input tensor, and an index and last_index and returns a tensor where the last element of the input tensor is replaced with the element at the specified index. The random_last function takes in an input tensor and a last_index and returns a tensor where the last element of the input tensor is replaced with a randomly chosen element from the input tensor. The get_images_no_answer function takes in an input tensor, an index and last_index and returns a tensor with last element removed from the input tensor. The repeat_last function takes in an input tensor, an index and last_index and returns a tensor where the last element of the input tensor is repeated. Example usage: # Get images based on indices inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) index = tf . constant ([[ 1 ], [ 2 ], [ 0 ]]) output = get_images (( inputs , index ), 2 ) # output will be [[1, 2, 4], [4, 5, 7], [7, 8, 1]] # Get images with specific index inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) output = get_images_with_index ( inputs , index ), index = 1 , last_index = 2 ) # output will be [[1, 2, 2], [4, 5, 5], [7, 8, 8]] # Get images with random index inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) output = random_last ( inputs , index ), last_index = 2 ) # output will be [[1, 2, 3], [4, 5, 6], [7, 8, 9]] # Get images without answer inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) output = get_images_no_answer ( inputs , index ), last_index = 2 ) # output will be [[1, 2], [4, 5], [7, 8]] # Get images by repeating last inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) output = repeat_last ( inputs , index ), last_index = 2 ) # output will be [[1, 2, 2], [4, 5, 5], [7, 8, 8]] View Source \" \"\" This module contains functions for preprocessing input data for use in a transformer model. The functions in this module are used to create the initial grid for the model. The `get_matrix` function takes in an input tensor and an index tensor, and returns a new tensor that is a concatenation of the first `last_index` elements of the input tensor, and the element at the index specified in the index tensor. The `last_index` parameter defaults to 8. The `get_images` function takes in a tuple containing the input tensor and the indices tensor, and returns the output tensor obtained by calling the `get_matrix` function with these inputs. The `last_index` parameter defaults to 8. The `get_images_with_index` function takes in an input tensor, and an index and last_index and returns a tensor where the last element of the input tensor is replaced with the element at the specified index. The `random_last` function takes in an input tensor and a last_index and returns a tensor where the last element of the input tensor is replaced with a randomly chosen element from the input tensor. The `get_images_no_answer` function takes in an input tensor, an index and last_index and returns a tensor with last element removed from the input tensor. The `repeat_last` function takes in an input tensor, an index and last_index and returns a tensor where the last element of the input tensor is repeated. Example usage: # Get images based on indices inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) index = tf.constant([[1], [2], [0]]) output = get_images((inputs, index), 2) # output will be [[1, 2, 4], [4, 5, 7], [7, 8, 1]] # Get images with specific index inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) output = get_images_with_index(inputs, index),index=1, last_index=2) # output will be [[1, 2, 2], [4, 5, 5], [7, 8, 8]] # Get images with random index inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) output = random_last(inputs, index), last_index=2) # output will be [[1, 2, 3], [4, 5, 6], [7, 8, 9]] # Get images without answer inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) output = get_images_no_answer(inputs, index), last_index=2) # output will be [[1, 2], [4, 5], [7, 8]] # Get images by repeating last inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) output = repeat_last(inputs, index), last_index=2) # output will be [[1, 2, 2], [4, 5, 5], [7, 8, 8]] \"\" \" from typing import Tuple import tensorflow as tf from models_utils import ops as K def get_matrix ( inputs : tf . Tensor , index : tf . Tensor , last_index : int = 8 ) -> tf . Tensor : \" \"\" Returns a tensor of shape (batch_size, last_index+1) which is a concatenation of inputs[:, :last_index] and K.gather(inputs, index[:, 0])[:, None] :param inputs : a tensor of shape (batch_size, features) :param index : a tensor of shape (batch_size, 1) representing the index of the element to be selected from inputs :param last_index : an int representing the index from where to split the input tensor. Default value is 8 \"\" \" return tf . concat ( [ inputs [ : , : last_index ] , K . gather ( inputs , index [ : , 0 ] ) [ : , None ]] , axis = 1 ) def get_images ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images based on indices in the input tensor. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" return get_matrix ( inputs [ 0 ] , inputs [ 1 ] , last_index = last_index ) def get_images_with_index ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , index : int = 0 , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images from input tensor along with specific index. :param inputs: Tuple containing the input tensor and indices tensor :param index: Index of the element to be included in the output tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" return tf . concat ( [ inputs [ 0 ][ : , : last_index ] , inputs [ 0 ][ : , index : index + 1 ]] , axis = 1 ) def random_last ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images from input tensor along with a randomly chosen index. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" index = K . init . label ( max = last_index , shape = [ tf . shape ( inputs [ 0 ] ) [ 0 ]] ) [ ..., None ] return get_matrix ( inputs [ 0 ] , index ) def get_images_no_answer ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images from input tensor without including any answer. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" return inputs [ 0 ][ : , : last_index + 1 ] def repeat_last ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images from input tensor by repeating the last element. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" return inputs [ 0 ][ : , list ( range ( last_index )) + [ last_index - 1 ]] Functions get_images def get_images ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images based on indices in the input tensor. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def get_images ( inputs : Tuple [ tf . Tensor , tf . Tensor ], last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images based on indices in the input tensor. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" return get_matrix ( inputs [ 0 ], inputs [ 1 ], last_index = last_index ) get_images_no_answer def get_images_no_answer ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images from input tensor without including any answer. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def get_images_no_answer ( inputs : Tuple [ tf . Tensor , tf . Tensor ], last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images from input tensor without including any answer. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" return inputs [ 0 ][ : , : last_index + 1 ] get_images_with_index def get_images_with_index ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], index : int = 0 , last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images from input tensor along with specific index. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None index None Index of the element to be included in the output tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def get_images_with_index ( inputs : Tuple [ tf . Tensor , tf . Tensor ], index : int = 0 , last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images from input tensor along with specific index. :param inputs: Tuple containing the input tensor and indices tensor :param index: Index of the element to be included in the output tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" return tf . concat ([ inputs [ 0 ][ : , : last_index ], inputs [ 0 ][ : , index : index + 1 ]], axis = 1 ) get_matrix def get_matrix ( inputs : tensorflow . python . framework . ops . Tensor , index : tensorflow . python . framework . ops . Tensor , last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Returns a tensor of shape (batch_size, last_index+1) which is a concatenation of inputs[:, :last_index] and K.gather(inputs, index[:, 0])[:, None] Parameters: Name Type Description Default inputs None a tensor of shape (batch_size, features) None index None a tensor of shape (batch_size, 1) representing the index of the element to be selected from inputs None last_index None an int representing the index from where to split the input tensor. Default value is 8 None View Source def get_matrix ( inputs : tf . Tensor , index : tf . Tensor , last_index : int = 8 ) -> tf . Tensor : \"\"\" Returns a tensor of shape (batch_size, last_index+1) which is a concatenation of inputs[:, :last_index] and K.gather(inputs, index[:, 0])[:, None] :param inputs : a tensor of shape (batch_size, features) :param index : a tensor of shape (batch_size, 1) representing the index of the element to be selected from inputs :param last_index : an int representing the index from where to split the input tensor. Default value is 8 \"\"\" return tf . concat ([ inputs [ : , : last_index ], K . gather ( inputs , index [ : , 0 ])[ : , None ]], axis = 1 ) random_last def random_last ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images from input tensor along with a randomly chosen index. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def random_last ( inputs : Tuple [ tf . Tensor , tf . Tensor ], last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images from input tensor along with a randomly chosen index. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" index = K . init . label ( max = last_index , shape = [ tf . shape ( inputs [ 0 ])[ 0 ]])[..., None ] return get_matrix ( inputs [ 0 ], index ) repeat_last def repeat_last ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images from input tensor by repeating the last element. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def repeat_last ( inputs : Tuple [ tf . Tensor , tf . Tensor ], last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images from input tensor by repeating the last element. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" return inputs [ 0 ][ : , list ( range ( last_index )) + [ last_index - 1 ]]","title":"Preprocessing"},{"location":"reference/grid_transformer/preprocessing/#module-grid_transformerpreprocessing","text":"This module contains functions for preprocessing input data for use in a transformer model. The functions in this module are used to create the initial grid for the model. The get_matrix function takes in an input tensor and an index tensor, and returns a new tensor that is a concatenation of the first last_index elements of the input tensor, and the element at the index specified in the index tensor. The last_index parameter defaults to 8. The get_images function takes in a tuple containing the input tensor and the indices tensor, and returns the output tensor obtained by calling the get_matrix function with these inputs. The last_index parameter defaults to 8. The get_images_with_index function takes in an input tensor, and an index and last_index and returns a tensor where the last element of the input tensor is replaced with the element at the specified index. The random_last function takes in an input tensor and a last_index and returns a tensor where the last element of the input tensor is replaced with a randomly chosen element from the input tensor. The get_images_no_answer function takes in an input tensor, an index and last_index and returns a tensor with last element removed from the input tensor. The repeat_last function takes in an input tensor, an index and last_index and returns a tensor where the last element of the input tensor is repeated. Example usage: # Get images based on indices inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) index = tf . constant ([[ 1 ], [ 2 ], [ 0 ]]) output = get_images (( inputs , index ), 2 ) # output will be [[1, 2, 4], [4, 5, 7], [7, 8, 1]] # Get images with specific index inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) output = get_images_with_index ( inputs , index ), index = 1 , last_index = 2 ) # output will be [[1, 2, 2], [4, 5, 5], [7, 8, 8]] # Get images with random index inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) output = random_last ( inputs , index ), last_index = 2 ) # output will be [[1, 2, 3], [4, 5, 6], [7, 8, 9]] # Get images without answer inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) output = get_images_no_answer ( inputs , index ), last_index = 2 ) # output will be [[1, 2], [4, 5], [7, 8]] # Get images by repeating last inputs = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) output = repeat_last ( inputs , index ), last_index = 2 ) # output will be [[1, 2, 2], [4, 5, 5], [7, 8, 8]] View Source \" \"\" This module contains functions for preprocessing input data for use in a transformer model. The functions in this module are used to create the initial grid for the model. The `get_matrix` function takes in an input tensor and an index tensor, and returns a new tensor that is a concatenation of the first `last_index` elements of the input tensor, and the element at the index specified in the index tensor. The `last_index` parameter defaults to 8. The `get_images` function takes in a tuple containing the input tensor and the indices tensor, and returns the output tensor obtained by calling the `get_matrix` function with these inputs. The `last_index` parameter defaults to 8. The `get_images_with_index` function takes in an input tensor, and an index and last_index and returns a tensor where the last element of the input tensor is replaced with the element at the specified index. The `random_last` function takes in an input tensor and a last_index and returns a tensor where the last element of the input tensor is replaced with a randomly chosen element from the input tensor. The `get_images_no_answer` function takes in an input tensor, an index and last_index and returns a tensor with last element removed from the input tensor. The `repeat_last` function takes in an input tensor, an index and last_index and returns a tensor where the last element of the input tensor is repeated. Example usage: # Get images based on indices inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) index = tf.constant([[1], [2], [0]]) output = get_images((inputs, index), 2) # output will be [[1, 2, 4], [4, 5, 7], [7, 8, 1]] # Get images with specific index inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) output = get_images_with_index(inputs, index),index=1, last_index=2) # output will be [[1, 2, 2], [4, 5, 5], [7, 8, 8]] # Get images with random index inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) output = random_last(inputs, index), last_index=2) # output will be [[1, 2, 3], [4, 5, 6], [7, 8, 9]] # Get images without answer inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) output = get_images_no_answer(inputs, index), last_index=2) # output will be [[1, 2], [4, 5], [7, 8]] # Get images by repeating last inputs = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) output = repeat_last(inputs, index), last_index=2) # output will be [[1, 2, 2], [4, 5, 5], [7, 8, 8]] \"\" \" from typing import Tuple import tensorflow as tf from models_utils import ops as K def get_matrix ( inputs : tf . Tensor , index : tf . Tensor , last_index : int = 8 ) -> tf . Tensor : \" \"\" Returns a tensor of shape (batch_size, last_index+1) which is a concatenation of inputs[:, :last_index] and K.gather(inputs, index[:, 0])[:, None] :param inputs : a tensor of shape (batch_size, features) :param index : a tensor of shape (batch_size, 1) representing the index of the element to be selected from inputs :param last_index : an int representing the index from where to split the input tensor. Default value is 8 \"\" \" return tf . concat ( [ inputs [ : , : last_index ] , K . gather ( inputs , index [ : , 0 ] ) [ : , None ]] , axis = 1 ) def get_images ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images based on indices in the input tensor. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" return get_matrix ( inputs [ 0 ] , inputs [ 1 ] , last_index = last_index ) def get_images_with_index ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , index : int = 0 , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images from input tensor along with specific index. :param inputs: Tuple containing the input tensor and indices tensor :param index: Index of the element to be included in the output tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" return tf . concat ( [ inputs [ 0 ][ : , : last_index ] , inputs [ 0 ][ : , index : index + 1 ]] , axis = 1 ) def random_last ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images from input tensor along with a randomly chosen index. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" index = K . init . label ( max = last_index , shape = [ tf . shape ( inputs [ 0 ] ) [ 0 ]] ) [ ..., None ] return get_matrix ( inputs [ 0 ] , index ) def get_images_no_answer ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images from input tensor without including any answer. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" return inputs [ 0 ][ : , : last_index + 1 ] def repeat_last ( inputs : Tuple [ tf . Tensor , tf . Tensor ] , last_index : int = 8 ) -> tf . Tensor : \" \"\" Get images from input tensor by repeating the last element. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\" \" return inputs [ 0 ][ : , list ( range ( last_index )) + [ last_index - 1 ]]","title":"Module grid_transformer.preprocessing"},{"location":"reference/grid_transformer/preprocessing/#functions","text":"","title":"Functions"},{"location":"reference/grid_transformer/preprocessing/#get_images","text":"def get_images ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images based on indices in the input tensor. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def get_images ( inputs : Tuple [ tf . Tensor , tf . Tensor ], last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images based on indices in the input tensor. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" return get_matrix ( inputs [ 0 ], inputs [ 1 ], last_index = last_index )","title":"get_images"},{"location":"reference/grid_transformer/preprocessing/#get_images_no_answer","text":"def get_images_no_answer ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images from input tensor without including any answer. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def get_images_no_answer ( inputs : Tuple [ tf . Tensor , tf . Tensor ], last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images from input tensor without including any answer. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" return inputs [ 0 ][ : , : last_index + 1 ]","title":"get_images_no_answer"},{"location":"reference/grid_transformer/preprocessing/#get_images_with_index","text":"def get_images_with_index ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], index : int = 0 , last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images from input tensor along with specific index. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None index None Index of the element to be included in the output tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def get_images_with_index ( inputs : Tuple [ tf . Tensor , tf . Tensor ], index : int = 0 , last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images from input tensor along with specific index. :param inputs: Tuple containing the input tensor and indices tensor :param index: Index of the element to be included in the output tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" return tf . concat ([ inputs [ 0 ][ : , : last_index ], inputs [ 0 ][ : , index : index + 1 ]], axis = 1 )","title":"get_images_with_index"},{"location":"reference/grid_transformer/preprocessing/#get_matrix","text":"def get_matrix ( inputs : tensorflow . python . framework . ops . Tensor , index : tensorflow . python . framework . ops . Tensor , last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Returns a tensor of shape (batch_size, last_index+1) which is a concatenation of inputs[:, :last_index] and K.gather(inputs, index[:, 0])[:, None] Parameters: Name Type Description Default inputs None a tensor of shape (batch_size, features) None index None a tensor of shape (batch_size, 1) representing the index of the element to be selected from inputs None last_index None an int representing the index from where to split the input tensor. Default value is 8 None View Source def get_matrix ( inputs : tf . Tensor , index : tf . Tensor , last_index : int = 8 ) -> tf . Tensor : \"\"\" Returns a tensor of shape (batch_size, last_index+1) which is a concatenation of inputs[:, :last_index] and K.gather(inputs, index[:, 0])[:, None] :param inputs : a tensor of shape (batch_size, features) :param index : a tensor of shape (batch_size, 1) representing the index of the element to be selected from inputs :param last_index : an int representing the index from where to split the input tensor. Default value is 8 \"\"\" return tf . concat ([ inputs [ : , : last_index ], K . gather ( inputs , index [ : , 0 ])[ : , None ]], axis = 1 )","title":"get_matrix"},{"location":"reference/grid_transformer/preprocessing/#random_last","text":"def random_last ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images from input tensor along with a randomly chosen index. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def random_last ( inputs : Tuple [ tf . Tensor , tf . Tensor ], last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images from input tensor along with a randomly chosen index. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" index = K . init . label ( max = last_index , shape = [ tf . shape ( inputs [ 0 ])[ 0 ]])[..., None ] return get_matrix ( inputs [ 0 ], index )","title":"random_last"},{"location":"reference/grid_transformer/preprocessing/#repeat_last","text":"def repeat_last ( inputs : Tuple [ tensorflow . python . framework . ops . Tensor , tensorflow . python . framework . ops . Tensor ], last_index : int = 8 ) -> tensorflow . python . framework . ops . Tensor Get images from input tensor by repeating the last element. Parameters: Name Type Description Default inputs None Tuple containing the input tensor and indices tensor None last_index None Index of last element to be included in the output tensor None Returns: Type Description None Output tensor View Source def repeat_last ( inputs : Tuple [ tf . Tensor , tf . Tensor ], last_index : int = 8 ) -> tf . Tensor : \"\"\" Get images from input tensor by repeating the last element. :param inputs: Tuple containing the input tensor and indices tensor :param last_index: Index of last element to be included in the output tensor :return: Output tensor \"\"\" return inputs [ 0 ][ : , list ( range ( last_index )) + [ last_index - 1 ]]","title":"repeat_last"},{"location":"reference/grid_transformer/simple_transformer/","text":"Module grid_transformer.simple_transformer This module contains the simple_transformer function for creating a transformer model with customizable embedding options. Functions: simple_transformer: Returns a transformer model constructor with the provided embedding options. View Source \"\"\" This module contains the `simple_transformer` function for creating a transformer model with customizable embedding options. Functions: - simple_transformer: Returns a transformer model constructor with the provided embedding options. \"\"\" from functools import partial from typing import Union , Callable import tensorflow as tf from keras.layers import Dense from ml_utils import il , filter_init from models_utils.layers.flatten2d import Flatten2D from models_utils.ops_core import IndexReshape from tensorflow.keras import Sequential from tensorflow.keras.layers import Lambda , Embedding , Flatten from grid_transformer import transformer def simple_transformer ( * args , embed_in : int , size : int = 256 , pre : Union [ str , Callable ] = None , embed : Union [ str , Callable ] = None , embed_out : Union [ int , str ] = 'auto' , flatten : Union [ str , Callable , bool ] = False , ** kwargs ) -> Callable : \"\"\" This function returns a transformer model with the provided embedding options. Args: *args: positional arguments that will be passed to the transformer model embed_in (int): the input dimension of the embedding layer size (int): the size of the transformer model pre (Union[str, Callable]): pre-processing function to be applied to the input. Default is None embed (Union[str, Callable]): embedding function to be applied to the input. Default is None embed_out (Union[int, str]): the output dimension of the embedding layer. Default is 'auto' flatten (Union[str, Callable, bool]): flatten function to be applied to the input. Default is False **kwargs: keyword arguments that will be passed to the transformer model Returns: A callable transformer model functional template \"\"\" if embed_out == \"auto\" : embed_out = size if embed == \"one_hot\" : embed = Lambda ( partial ( tf . one_hot , depth = embed_in )) elif embed == \"linear\" : embed = Dense ( embed_out ) else : embed = Embedding ( input_dim = embed_in , output_dim = embed_out ) embed = [ embed ] if flatten : if flatten in [ '2d' , \"flat2d\" ]: flatten = Flatten2D () elif il ( flatten ): flatten = IndexReshape ( flatten ) elif callable ( flatten ): flatten = flatten else : flatten = Flatten () embed = [ flatten ] + embed if pre : embed = [ pre ] + embed kwargs = { ** kwargs , \"extractor\" : Sequential ( embed ) if len ( embed ) > 1 else embed [ 0 ], \"extractor_pooling\" : None } return filter_init ( transformer , * args , size = size , ** kwargs ) Functions simple_transformer def simple_transformer ( * args , embed_in : int , size : int = 256 , pre : Union [ str , Callable ] = None , embed : Union [ str , Callable ] = None , embed_out : Union [ int , str ] = 'auto' , flatten : Union [ str , Callable , bool ] = False , ** kwargs ) -> Callable This function returns a transformer model with the provided embedding options. Parameters: Name Type Description Default *args None positional arguments that will be passed to the transformer model None embed_in int the input dimension of the embedding layer None size int the size of the transformer model None pre Union[str, Callable] pre-processing function to be applied to the input. Default is None None embed Union[str, Callable] embedding function to be applied to the input. Default is None None embed_out Union[int, str] the output dimension of the embedding layer. Default is 'auto' None flatten Union[str, Callable, bool] flatten function to be applied to the input. Default is False None **kwargs None keyword arguments that will be passed to the transformer model None Returns: Type Description None A callable transformer model functional template View Source def simple_transformer ( * args , embed_in : int , size : int = 256 , pre : Union [ str, Callable ] = None , embed : Union [ str, Callable ] = None , embed_out : Union [ int, str ] = 'auto' , flatten : Union [ str, Callable, bool ] = False , ** kwargs ) -> Callable : \"\"\" This function returns a transformer model with the provided embedding options. Args: *args: positional arguments that will be passed to the transformer model embed_in (int): the input dimension of the embedding layer size (int): the size of the transformer model pre (Union[str, Callable]): pre-processing function to be applied to the input. Default is None embed (Union[str, Callable]): embedding function to be applied to the input. Default is None embed_out (Union[int, str]): the output dimension of the embedding layer. Default is 'auto' flatten (Union[str, Callable, bool]): flatten function to be applied to the input. Default is False **kwargs: keyword arguments that will be passed to the transformer model Returns: A callable transformer model functional template \"\"\" if embed_out == \"auto\" : embed_out = size if embed == \"one_hot\" : embed = Lambda ( partial ( tf . one_hot , depth = embed_in )) elif embed == \"linear\" : embed = Dense ( embed_out ) else : embed = Embedding ( input_dim = embed_in , output_dim = embed_out ) embed = [ embed ] if flatten : if flatten in [ '2d', \"flat2d\" ] : flatten = Flatten2D () elif il ( flatten ) : flatten = IndexReshape ( flatten ) elif callable ( flatten ) : flatten = flatten else : flatten = Flatten () embed = [ flatten ] + embed if pre : embed = [ pre ] + embed kwargs = { ** kwargs , \"extractor\" : Sequential ( embed ) if len ( embed ) > 1 else embed [ 0 ] , \"extractor_pooling\" : None } return filter_init ( transformer , * args , size = size , ** kwargs )","title":"Simple Transformer"},{"location":"reference/grid_transformer/simple_transformer/#module-grid_transformersimple_transformer","text":"This module contains the simple_transformer function for creating a transformer model with customizable embedding options. Functions: simple_transformer: Returns a transformer model constructor with the provided embedding options. View Source \"\"\" This module contains the `simple_transformer` function for creating a transformer model with customizable embedding options. Functions: - simple_transformer: Returns a transformer model constructor with the provided embedding options. \"\"\" from functools import partial from typing import Union , Callable import tensorflow as tf from keras.layers import Dense from ml_utils import il , filter_init from models_utils.layers.flatten2d import Flatten2D from models_utils.ops_core import IndexReshape from tensorflow.keras import Sequential from tensorflow.keras.layers import Lambda , Embedding , Flatten from grid_transformer import transformer def simple_transformer ( * args , embed_in : int , size : int = 256 , pre : Union [ str , Callable ] = None , embed : Union [ str , Callable ] = None , embed_out : Union [ int , str ] = 'auto' , flatten : Union [ str , Callable , bool ] = False , ** kwargs ) -> Callable : \"\"\" This function returns a transformer model with the provided embedding options. Args: *args: positional arguments that will be passed to the transformer model embed_in (int): the input dimension of the embedding layer size (int): the size of the transformer model pre (Union[str, Callable]): pre-processing function to be applied to the input. Default is None embed (Union[str, Callable]): embedding function to be applied to the input. Default is None embed_out (Union[int, str]): the output dimension of the embedding layer. Default is 'auto' flatten (Union[str, Callable, bool]): flatten function to be applied to the input. Default is False **kwargs: keyword arguments that will be passed to the transformer model Returns: A callable transformer model functional template \"\"\" if embed_out == \"auto\" : embed_out = size if embed == \"one_hot\" : embed = Lambda ( partial ( tf . one_hot , depth = embed_in )) elif embed == \"linear\" : embed = Dense ( embed_out ) else : embed = Embedding ( input_dim = embed_in , output_dim = embed_out ) embed = [ embed ] if flatten : if flatten in [ '2d' , \"flat2d\" ]: flatten = Flatten2D () elif il ( flatten ): flatten = IndexReshape ( flatten ) elif callable ( flatten ): flatten = flatten else : flatten = Flatten () embed = [ flatten ] + embed if pre : embed = [ pre ] + embed kwargs = { ** kwargs , \"extractor\" : Sequential ( embed ) if len ( embed ) > 1 else embed [ 0 ], \"extractor_pooling\" : None } return filter_init ( transformer , * args , size = size , ** kwargs )","title":"Module grid_transformer.simple_transformer"},{"location":"reference/grid_transformer/simple_transformer/#functions","text":"","title":"Functions"},{"location":"reference/grid_transformer/simple_transformer/#simple_transformer","text":"def simple_transformer ( * args , embed_in : int , size : int = 256 , pre : Union [ str , Callable ] = None , embed : Union [ str , Callable ] = None , embed_out : Union [ int , str ] = 'auto' , flatten : Union [ str , Callable , bool ] = False , ** kwargs ) -> Callable This function returns a transformer model with the provided embedding options. Parameters: Name Type Description Default *args None positional arguments that will be passed to the transformer model None embed_in int the input dimension of the embedding layer None size int the size of the transformer model None pre Union[str, Callable] pre-processing function to be applied to the input. Default is None None embed Union[str, Callable] embedding function to be applied to the input. Default is None None embed_out Union[int, str] the output dimension of the embedding layer. Default is 'auto' None flatten Union[str, Callable, bool] flatten function to be applied to the input. Default is False None **kwargs None keyword arguments that will be passed to the transformer model None Returns: Type Description None A callable transformer model functional template View Source def simple_transformer ( * args , embed_in : int , size : int = 256 , pre : Union [ str, Callable ] = None , embed : Union [ str, Callable ] = None , embed_out : Union [ int, str ] = 'auto' , flatten : Union [ str, Callable, bool ] = False , ** kwargs ) -> Callable : \"\"\" This function returns a transformer model with the provided embedding options. Args: *args: positional arguments that will be passed to the transformer model embed_in (int): the input dimension of the embedding layer size (int): the size of the transformer model pre (Union[str, Callable]): pre-processing function to be applied to the input. Default is None embed (Union[str, Callable]): embedding function to be applied to the input. Default is None embed_out (Union[int, str]): the output dimension of the embedding layer. Default is 'auto' flatten (Union[str, Callable, bool]): flatten function to be applied to the input. Default is False **kwargs: keyword arguments that will be passed to the transformer model Returns: A callable transformer model functional template \"\"\" if embed_out == \"auto\" : embed_out = size if embed == \"one_hot\" : embed = Lambda ( partial ( tf . one_hot , depth = embed_in )) elif embed == \"linear\" : embed = Dense ( embed_out ) else : embed = Embedding ( input_dim = embed_in , output_dim = embed_out ) embed = [ embed ] if flatten : if flatten in [ '2d', \"flat2d\" ] : flatten = Flatten2D () elif il ( flatten ) : flatten = IndexReshape ( flatten ) elif callable ( flatten ) : flatten = flatten else : flatten = Flatten () embed = [ flatten ] + embed if pre : embed = [ pre ] + embed kwargs = { ** kwargs , \"extractor\" : Sequential ( embed ) if len ( embed ) > 1 else embed [ 0 ] , \"extractor_pooling\" : None } return filter_init ( transformer , * args , size = size , ** kwargs )","title":"simple_transformer"},{"location":"reference/grid_transformer/transformer/","text":"Module grid_transformer.transformer This module contains the transformer function for creating a transformer model with customizable position embedding, pooling, and output layers. Functions: transformer: Returns a transformer model constructor with the provided options. View Source \"\"\" This module contains the `transformer` function for creating a transformer model with customizable position embedding, pooling, and output layers. Functions: - transformer: Returns a transformer model constructor with the provided options. \"\"\" from data_utils import sv from tensorflow.keras.layers import Dense , LayerNormalization from ml_utils import pj , get_str_name from models_utils import short , is_model , add_end , feature_extractor from grid_transformer import TransformerBlock from grid_transformer.position_embedding import CatPositionEmbedding , SumPositionEmbedding def transformer ( pos_emd = \"cat\" , size = 256 , pooling = \"first\" , no = 8 , out_layers = ( 1000 , 1000 ), output_size = 10 , extractor = \"ef\" , extractor_pooling = \"flat2d\" , last_norm = True , num_heads = 8 , ff_mul = 4 , ff_size = None , dropout = 0.1 , ff_act = \"gelu\" , show_shape = False , save_shape = True , return_attention_scores = False , ): if pos_emd == \"sum\" : pos_emd = SumPositionEmbedding () elif callable ( pos_emd ): pos_emd = pos_emd else : pos_emd = CatPositionEmbedding () pool = short ( pooling ) end = out_layers if is_model ( out_layers ) else add_end ( out_layers , output_size , activation = \"gelu\" ) # last_norm = True # num_heads = 8 # ff_mul = 4 # ff_size = None # dropout = 0.1 # ff_act = \"gelu\" # size = size def apply ( x ): nonlocal extractor nonlocal extractor_pooling nonlocal show_shape nonlocal save_shape nonlocal size if show_shape == \"batch\" : def shape_ ( tensor ): return tensor . shape else : def shape_ ( tensor ): return tensor . shape [ 1 :] if show_shape : print ( f \"Transformer shape\" ) print ( f \"Extractor input shape: { shape_ ( x ) } \" ) tensor_shapes = { \"Extractor input shape\" : shape_ ( x )} if callable ( extractor ) and extractor_pooling is None : x = extractor ( x ) if x . shape [ - 1 ] != size : x = Dense ( size )( x ) elif extractor : x = feature_extractor ( data = tuple ( x . shape ), model = extractor , # projection=size - pos_emd.embed_dim if pos_emd == \"cut\" else size, projection = size , pooling = extractor_pooling )( x ) if show_shape : print ( f \"Transformer blocks input shape: { shape_ ( x ) } \" ) tensor_shapes [ \"Transformer blocks input shape\" ] = shape_ ( x ) x = pos_emd ( x ) scores = [] for _ in range ( no ): trans = TransformerBlock ( num_heads = num_heads , # size=size, ff_mul = ff_mul , ff_size = ff_size , dropout = dropout , ff_act = ff_act , return_attention_scores = return_attention_scores ) if return_attention_scores : x , score = trans ( x ) scores . append ( score ) else : x = trans ( x ) if last_norm : x = LayerNormalization ( epsilon = 1e-6 )( x ) # if show_shape: # print(f\"Transformer block shape: {x.shape}\") x = pool ()( x ) if show_shape : print ( f \"Transformer blocks output shape: { shape_ ( x ) } \" ) tensor_shapes [ \"Transformer blocks output shape\" ] = shape_ ( x ) x = end ( x ) if show_shape : print ( f \"Output shape: { shape_ ( x ) } \" ) tensor_shapes [ \"Output shape\" ] = shape_ ( x ) if show_shape : if save_shape is True : save_shape = pj ( \"output\" , \"shapes\" , f \" { list ( tensor_shapes . values ())[ 0 ] } _ { get_str_name ( extractor ) } _ { get_str_name ( extractor_pooling ) } .yml\" ) sv ( tensor_shapes , save_shape ) if return_attention_scores : return x , scores return x return apply Functions transformer def transformer ( pos_emd = 'cat' , size = 256 , pooling = 'first' , no = 8 , out_layers = ( 1000 , 1000 ), output_size = 10 , extractor = 'ef' , extractor_pooling = 'flat2d' , last_norm = True , num_heads = 8 , ff_mul = 4 , ff_size = None , dropout = 0.1 , ff_act = 'gelu' , show_shape = False , save_shape = True , return_attention_scores = False ) View Source def transformer ( pos_emd = \"cat\" , size = 256 , pooling = \"first\" , no = 8 , out_layers = ( 1000 , 1000 ) , output_size = 10 , extractor = \"ef\" , extractor_pooling = \"flat2d\" , last_norm = True , num_heads = 8 , ff_mul = 4 , ff_size = None , dropout = 0 . 1 , ff_act = \"gelu\" , show_shape = False , save_shape = True , return_attention_scores = False , ) : if pos_emd == \"sum\" : pos_emd = SumPositionEmbedding () elif callable ( pos_emd ) : pos_emd = pos_emd else : pos_emd = CatPositionEmbedding () pool = short ( pooling ) end = out_layers if is_model ( out_layers ) else add_end ( out_layers , output_size , activation = \"gelu\" ) # last_norm = True # num_heads = 8 # ff_mul = 4 # ff_size = None # dropout = 0 . 1 # ff_act = \"gelu\" # size = size def apply ( x ) : nonlocal extractor nonlocal extractor_pooling nonlocal show_shape nonlocal save_shape nonlocal size if show_shape == \"batch\" : def shape_ ( tensor ) : return tensor . shape else : def shape_ ( tensor ) : return tensor . shape [ 1 :] if show_shape : print ( f \"Transformer shape\" ) print ( f \"Extractor input shape: {shape_(x)}\" ) tensor_shapes = { \"Extractor input shape\" : shape_ ( x ) } if callable ( extractor ) and extractor_pooling is None : x = extractor ( x ) if x . shape [ - 1 ] != size : x = Dense ( size )( x ) elif extractor : x = feature_extractor ( data = tuple ( x . shape ) , model = extractor , # projection = size - pos_emd . embed_dim if pos_emd == \"cut\" else size , projection = size , pooling = extractor_pooling )( x ) if show_shape : print ( f \"Transformer blocks input shape: {shape_(x)}\" ) tensor_shapes [ \"Transformer blocks input shape\" ] = shape_ ( x ) x = pos_emd ( x ) scores = [] for _ in range ( no ) : trans = TransformerBlock ( num_heads = num_heads , # size = size , ff_mul = ff_mul , ff_size = ff_size , dropout = dropout , ff_act = ff_act , return_attention_scores = return_attention_scores ) if return_attention_scores : x , score = trans ( x ) scores . append ( score ) else : x = trans ( x ) if last_norm : x = LayerNormalization ( epsilon = 1 e - 6 )( x ) # if show_shape : # print ( f \"Transformer block shape: {x.shape}\" ) x = pool ()( x ) if show_shape : print ( f \"Transformer blocks output shape: {shape_(x)}\" ) tensor_shapes [ \"Transformer blocks output shape\" ] = shape_ ( x ) x = end ( x ) if show_shape : print ( f \"Output shape: {shape_(x)}\" ) tensor_shapes [ \"Output shape\" ] = shape_ ( x ) if show_shape : if save_shape is True : save_shape = pj ( \"output\" , \"shapes\" , f \"{list(tensor_shapes.values())[0]}_{get_str_name(extractor)}_{get_str_name(extractor_pooling)}.yml\" ) sv ( tensor_shapes , save_shape ) if return_attention_scores : return x , scores return x return apply","title":"Transformer"},{"location":"reference/grid_transformer/transformer/#module-grid_transformertransformer","text":"This module contains the transformer function for creating a transformer model with customizable position embedding, pooling, and output layers. Functions: transformer: Returns a transformer model constructor with the provided options. View Source \"\"\" This module contains the `transformer` function for creating a transformer model with customizable position embedding, pooling, and output layers. Functions: - transformer: Returns a transformer model constructor with the provided options. \"\"\" from data_utils import sv from tensorflow.keras.layers import Dense , LayerNormalization from ml_utils import pj , get_str_name from models_utils import short , is_model , add_end , feature_extractor from grid_transformer import TransformerBlock from grid_transformer.position_embedding import CatPositionEmbedding , SumPositionEmbedding def transformer ( pos_emd = \"cat\" , size = 256 , pooling = \"first\" , no = 8 , out_layers = ( 1000 , 1000 ), output_size = 10 , extractor = \"ef\" , extractor_pooling = \"flat2d\" , last_norm = True , num_heads = 8 , ff_mul = 4 , ff_size = None , dropout = 0.1 , ff_act = \"gelu\" , show_shape = False , save_shape = True , return_attention_scores = False , ): if pos_emd == \"sum\" : pos_emd = SumPositionEmbedding () elif callable ( pos_emd ): pos_emd = pos_emd else : pos_emd = CatPositionEmbedding () pool = short ( pooling ) end = out_layers if is_model ( out_layers ) else add_end ( out_layers , output_size , activation = \"gelu\" ) # last_norm = True # num_heads = 8 # ff_mul = 4 # ff_size = None # dropout = 0.1 # ff_act = \"gelu\" # size = size def apply ( x ): nonlocal extractor nonlocal extractor_pooling nonlocal show_shape nonlocal save_shape nonlocal size if show_shape == \"batch\" : def shape_ ( tensor ): return tensor . shape else : def shape_ ( tensor ): return tensor . shape [ 1 :] if show_shape : print ( f \"Transformer shape\" ) print ( f \"Extractor input shape: { shape_ ( x ) } \" ) tensor_shapes = { \"Extractor input shape\" : shape_ ( x )} if callable ( extractor ) and extractor_pooling is None : x = extractor ( x ) if x . shape [ - 1 ] != size : x = Dense ( size )( x ) elif extractor : x = feature_extractor ( data = tuple ( x . shape ), model = extractor , # projection=size - pos_emd.embed_dim if pos_emd == \"cut\" else size, projection = size , pooling = extractor_pooling )( x ) if show_shape : print ( f \"Transformer blocks input shape: { shape_ ( x ) } \" ) tensor_shapes [ \"Transformer blocks input shape\" ] = shape_ ( x ) x = pos_emd ( x ) scores = [] for _ in range ( no ): trans = TransformerBlock ( num_heads = num_heads , # size=size, ff_mul = ff_mul , ff_size = ff_size , dropout = dropout , ff_act = ff_act , return_attention_scores = return_attention_scores ) if return_attention_scores : x , score = trans ( x ) scores . append ( score ) else : x = trans ( x ) if last_norm : x = LayerNormalization ( epsilon = 1e-6 )( x ) # if show_shape: # print(f\"Transformer block shape: {x.shape}\") x = pool ()( x ) if show_shape : print ( f \"Transformer blocks output shape: { shape_ ( x ) } \" ) tensor_shapes [ \"Transformer blocks output shape\" ] = shape_ ( x ) x = end ( x ) if show_shape : print ( f \"Output shape: { shape_ ( x ) } \" ) tensor_shapes [ \"Output shape\" ] = shape_ ( x ) if show_shape : if save_shape is True : save_shape = pj ( \"output\" , \"shapes\" , f \" { list ( tensor_shapes . values ())[ 0 ] } _ { get_str_name ( extractor ) } _ { get_str_name ( extractor_pooling ) } .yml\" ) sv ( tensor_shapes , save_shape ) if return_attention_scores : return x , scores return x return apply","title":"Module grid_transformer.transformer"},{"location":"reference/grid_transformer/transformer/#functions","text":"","title":"Functions"},{"location":"reference/grid_transformer/transformer/#transformer","text":"def transformer ( pos_emd = 'cat' , size = 256 , pooling = 'first' , no = 8 , out_layers = ( 1000 , 1000 ), output_size = 10 , extractor = 'ef' , extractor_pooling = 'flat2d' , last_norm = True , num_heads = 8 , ff_mul = 4 , ff_size = None , dropout = 0.1 , ff_act = 'gelu' , show_shape = False , save_shape = True , return_attention_scores = False ) View Source def transformer ( pos_emd = \"cat\" , size = 256 , pooling = \"first\" , no = 8 , out_layers = ( 1000 , 1000 ) , output_size = 10 , extractor = \"ef\" , extractor_pooling = \"flat2d\" , last_norm = True , num_heads = 8 , ff_mul = 4 , ff_size = None , dropout = 0 . 1 , ff_act = \"gelu\" , show_shape = False , save_shape = True , return_attention_scores = False , ) : if pos_emd == \"sum\" : pos_emd = SumPositionEmbedding () elif callable ( pos_emd ) : pos_emd = pos_emd else : pos_emd = CatPositionEmbedding () pool = short ( pooling ) end = out_layers if is_model ( out_layers ) else add_end ( out_layers , output_size , activation = \"gelu\" ) # last_norm = True # num_heads = 8 # ff_mul = 4 # ff_size = None # dropout = 0 . 1 # ff_act = \"gelu\" # size = size def apply ( x ) : nonlocal extractor nonlocal extractor_pooling nonlocal show_shape nonlocal save_shape nonlocal size if show_shape == \"batch\" : def shape_ ( tensor ) : return tensor . shape else : def shape_ ( tensor ) : return tensor . shape [ 1 :] if show_shape : print ( f \"Transformer shape\" ) print ( f \"Extractor input shape: {shape_(x)}\" ) tensor_shapes = { \"Extractor input shape\" : shape_ ( x ) } if callable ( extractor ) and extractor_pooling is None : x = extractor ( x ) if x . shape [ - 1 ] != size : x = Dense ( size )( x ) elif extractor : x = feature_extractor ( data = tuple ( x . shape ) , model = extractor , # projection = size - pos_emd . embed_dim if pos_emd == \"cut\" else size , projection = size , pooling = extractor_pooling )( x ) if show_shape : print ( f \"Transformer blocks input shape: {shape_(x)}\" ) tensor_shapes [ \"Transformer blocks input shape\" ] = shape_ ( x ) x = pos_emd ( x ) scores = [] for _ in range ( no ) : trans = TransformerBlock ( num_heads = num_heads , # size = size , ff_mul = ff_mul , ff_size = ff_size , dropout = dropout , ff_act = ff_act , return_attention_scores = return_attention_scores ) if return_attention_scores : x , score = trans ( x ) scores . append ( score ) else : x = trans ( x ) if last_norm : x = LayerNormalization ( epsilon = 1 e - 6 )( x ) # if show_shape : # print ( f \"Transformer block shape: {x.shape}\" ) x = pool ()( x ) if show_shape : print ( f \"Transformer blocks output shape: {shape_(x)}\" ) tensor_shapes [ \"Transformer blocks output shape\" ] = shape_ ( x ) x = end ( x ) if show_shape : print ( f \"Output shape: {shape_(x)}\" ) tensor_shapes [ \"Output shape\" ] = shape_ ( x ) if show_shape : if save_shape is True : save_shape = pj ( \"output\" , \"shapes\" , f \"{list(tensor_shapes.values())[0]}_{get_str_name(extractor)}_{get_str_name(extractor_pooling)}.yml\" ) sv ( tensor_shapes , save_shape ) if return_attention_scores : return x , scores return x return apply","title":"transformer"},{"location":"reference/grid_transformer/transformer_block/","text":"Module grid_transformer.transformer_block This module contains two classes TransformerBlockBase and TransformerBlock , which implement the transformer block as defined in the \"Attention Is All You Need\" paper by Google. A transformer block consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. Classes: TransformerBlockBase: A transformer block with original implementation of the call function TransformerBlock: A transformer block with modified implementation of the call function which changes the order of layernorm and residual connection. Example usage: import tensorflow as tf # Instantiate the transformer block tb = transformer_block . TransformerBlockBase () # Build the transformer block with input shape (batch_size, sequence_length, input_dim) tb . build (( None , 10 , 32 )) # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim) output = tb ( tf . random . normal (( 32 , 10 , 32 ))) # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim) output , scores = tb ( tf . random . normal (( 32 , 10 , 32 )), return_attention_scores = True ) # Instantiate a transformer block with user-specified feed-forward network ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( 256 , activation = 'relu' ), tf . keras . layers . Dense ( 32 )]) tb = transformer_block . TransformerBlock ( ffn = ffn ) # Instantiate a TransformerBlock2 tb2 = transformer_block . TransformerBlock () # Build the transformer block with input shape (batch_size, sequence_length, input_dim) tb2 . build (( None , 10 , 32 )) # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim) output = tb2 ( tf . random . normal (( 32 , 10 , 32 ))) # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim) output , scores = tb2 ( tf . random . normal (( 32 , 10 , 32 )), return_attention_scores = True ) # Instantiate a transformer block with user-specified feed-forward network ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( 256 , activation = 'relu' ), tf . keras . layers . Dense ( 32 )]) tb = transformer_block . TransformerBlock ( ffn = ffn ) View Source \"\"\" This module contains two classes `TransformerBlockBase` and `TransformerBlock`, which implement the transformer block as defined in the \"Attention Is All You Need\" paper by Google. A transformer block consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. Classes: - TransformerBlockBase: A transformer block with original implementation of the call function - TransformerBlock: A transformer block with modified implementation of the call function which changes the order of layernorm and residual connection. Example usage: import tensorflow as tf # Instantiate the transformer block tb = transformer_block.TransformerBlockBase() # Build the transformer block with input shape (batch_size, sequence_length, input_dim) tb.build((None, 10, 32)) # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim) output = tb(tf.random.normal((32, 10, 32))) # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim) output, scores = tb(tf.random.normal((32, 10, 32)), return_attention_scores=True) # Instantiate a transformer block with user-specified feed-forward network ffn = tf.keras.Sequential([tf.keras.layers.Dense(256, activation='relu'), tf.keras.layers.Dense(32)]) tb = transformer_block.TransformerBlock(ffn=ffn) # Instantiate a TransformerBlock2 tb2 = transformer_block.TransformerBlock() # Build the transformer block with input shape (batch_size, sequence_length, input_dim) tb2.build((None, 10, 32)) # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim) output = tb2(tf.random.normal((32, 10, 32))) # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim) output, scores = tb2(tf.random.normal((32, 10, 32)), return_attention_scores=True) # Instantiate a transformer block with user-specified feed-forward network ffn = tf.keras.Sequential([tf.keras.layers.Dense(256, activation='relu'), tf.keras.layers.Dense(32)]) tb = transformer_block.TransformerBlock(ffn=ffn) \"\"\" from tensorflow.keras import Sequential from tensorflow.keras.layers import Layer , MultiHeadAttention , Dense , Dropout , LayerNormalization class TransformerBlockBase ( Layer ): \"\"\" A transformer block as defined in the \"Attention Is All You Need\" paper by Google. It consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. \"\"\" def __init__ ( self , num_heads = 8 , size = None , ff_mul = 4 , ff_size = None , dropout = 0.1 , mha_dropout = 0.1 , ff_act = \"gelu\" , ffn = None , return_attention_scores = False ): \"\"\" Initialize the transformer block. :param num_heads: Number of heads in the multi-head self-attention mechanism :param size: Dimension of the input/output vectors. If None, it will be inferred from input_shape during build() :param ff_mul: Multiplier for the feed-forward network size :param ff_size: Size of the feed-forward network. If None, it will be inferred from size and ff_mul :param dropout: Dropout rate for the feed-forward network :param mha_dropout: Dropout rate for the multi-head self-attention mechanism :param ff_act: Activation function for the feed-forward network :param ffn: A user-specified feed-forward network. If None, a default one will be created :param return_attention_scores: Whether to return attention scores \"\"\" super ( TransformerBlockBase , self ) . __init__ () self . size = size self . num_heads = num_heads self . ff_mul = ff_mul self . ff_size = ff_size self . ff_act = ff_act self . dropout = dropout self . mha_dropout = mha_dropout self . return_attention_scores = return_attention_scores self . ffn = ffn def build ( self , input_shape ): \"\"\" Build the transformer block. :param input_shape: Shape of the input tensor \"\"\" if self . size is None : self . size = input_shape [ - 1 ] self . att = MultiHeadAttention ( num_heads = self . num_heads , key_dim = self . size , dropout = self . mha_dropout , ) if self . ff_size is None : self . ff_size = self . size * self . ff_mul # self.ffn = self.ffn or build_dense_model( # [ # self.ff_size, # Dropout(self.dropout), # self.size, # Dropout(self.dropout) # ], # activation=self.ff_act, # last_activation=None) self . ffn = self . ffn or Sequential ([ Dense ( self . ff_size , activation = self . ff_act ), Dropout ( self . dropout ), Dense ( self . size ), Dropout ( self . dropout ) ]) self . layernorm1 = LayerNormalization ( epsilon = 1e-6 ) self . layernorm2 = LayerNormalization ( epsilon = 1e-6 ) def call ( self , inputs ): \"\"\" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\"\" if self . return_attention_scores : x , scores = self . att ( inputs , inputs , return_attention_scores = True ) else : x = self . att ( inputs , inputs ) x = self . layernorm1 ( x + inputs ) y = self . ffn ( x ) if self . return_attention_scores : return self . layernorm2 ( x + y ), scores return self . layernorm2 ( x + y ) def get_config ( self ): config = super () . get_config () config . update ({ \"size\" : self . size , \"num_heads\" : self . num_heads , \"ff_mul\" : self . ff_mul , \"ff_size\" : self . ff_size , \"ff_act\" : self . ff_act , \"dropout\" : self . dropout , \"mha_dropout\" : self . mha_dropout , \"return_attention_scores\" : self . return_attention_scores , \"ffn\" : self . ffn }) return config class TransformerBlock ( TransformerBlockBase ): \"\"\" A TransformerBlock with a modified call function, which changes the order of layernorm and residual connection. \"\"\" def call ( self , inputs ): \"\"\" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\"\" x = self . layernorm1 ( inputs ) if self . return_attention_scores : x , scores = self . att ( x , x , return_attention_scores = True ) else : x = self . att ( x , x ) x = x + inputs y = self . layernorm2 ( x ) y = self . ffn ( y ) if self . return_attention_scores : return x + y , scores return x + y Classes TransformerBlock class TransformerBlock ( num_heads = 8 , size = None , ff_mul = 4 , ff_size = None , dropout = 0.1 , mha_dropout = 0.1 , ff_act = 'gelu' , ffn = None , return_attention_scores = False ) A TransformerBlock with a modified call function, which changes the order of layernorm and residual connection. View Source class TransformerBlock ( TransformerBlockBase ): \"\"\" A TransformerBlock with a modified call function, which changes the order of layernorm and residual connection. \"\"\" def call ( self , inputs ): \"\"\" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\"\" x = self . layernorm1 ( inputs ) if self . return_attention_scores: x , scores = self . att ( x , x , return_attention_scores = True ) else: x = self . att ( x , x ) x = x + inputs y = self . layernorm2 ( x ) y = self . ffn ( y ) if self . return_attention_scores: return x + y , scores return x + y Ancestors (in MRO) grid_transformer.transformer_block.TransformerBlockBase keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape ) Build the transformer block. Parameters: Name Type Description Default input_shape None Shape of the input tensor None View Source def build ( self , input_shape ) : \"\" \" Build the transformer block. :param input_shape: Shape of the input tensor \"\" \" if self.size is None: self.size = input_shape[-1] self.att = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.size, dropout=self.mha_dropout, ) if self.ff_size is None: self.ff_size = self.size * self.ff_mul # self.ffn = self.ffn or build_dense_model( # [ # self.ff_size, # Dropout(self.dropout), # self.size, # Dropout(self.dropout) # ], # activation=self.ff_act, # last_activation=None) self.ffn = self.ffn or Sequential([ Dense(self.ff_size, activation=self.ff_act), Dropout(self.dropout), Dense(self.size), Dropout(self.dropout) ]) self.layernorm1 = LayerNormalization(epsilon=1e-6) self.layernorm2 = LayerNormalization(epsilon=1e-6) call def call ( self , inputs ) Perform the forward pass of the transformer block. Parameters: Name Type Description Default inputs None Input tensor None Returns: Type Description None Output tensor and attention scores if return_attention_scores is True View Source def call ( self , inputs ) : \"\" \" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\" \" x = self.layernorm1(inputs) if self.return_attention_scores: x, scores = self.att(x, x, return_attention_scores=True) else: x = self.att(x, x) x = x + inputs y = self.layernorm2(x) y = self.ffn(y) if self.return_attention_scores: return x + y, scores return x + y compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source def get_config ( self ) : config = super () . get_config () config . update ( { \"size\" : self . size , \"num_heads\" : self . num_heads , \"ff_mul\" : self . ff_mul , \"ff_size\" : self . ff_size , \"ff_act\" : self . ff_act , \"dropout\" : self . dropout , \"mha_dropout\" : self . mha_dropout , \"return_attention_scores\" : self . return_attention_scores , \"ffn\" : self . ffn } ) return config get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weights def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state () TransformerBlockBase class TransformerBlockBase ( num_heads = 8 , size = None , ff_mul = 4 , ff_size = None , dropout = 0.1 , mha_dropout = 0.1 , ff_act = 'gelu' , ffn = None , return_attention_scores = False ) A transformer block as defined in the \"Attention Is All You Need\" paper by Google. It consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. View Source class TransformerBlockBase ( Layer ): \"\"\" A transformer block as defined in the \" Attention Is All You Need \" paper by Google. It consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. \"\"\" def __init__ ( self , num_heads = 8 , size = None , ff_mul = 4 , ff_size = None , dropout = 0.1 , mha_dropout = 0.1 , ff_act = \"gelu\" , ffn = None , return_attention_scores = False ): \"\"\" Initialize the transformer block. :param num_heads: Number of heads in the multi-head self-attention mechanism :param size: Dimension of the input/output vectors. If None, it will be inferred from input_shape during build() :param ff_mul: Multiplier for the feed-forward network size :param ff_size: Size of the feed-forward network. If None, it will be inferred from size and ff_mul :param dropout: Dropout rate for the feed-forward network :param mha_dropout: Dropout rate for the multi-head self-attention mechanism :param ff_act: Activation function for the feed-forward network :param ffn: A user-specified feed-forward network. If None, a default one will be created :param return_attention_scores: Whether to return attention scores \"\"\" super ( TransformerBlockBase , self ). __init__ () self . size = size self . num_heads = num_heads self . ff_mul = ff_mul self . ff_size = ff_size self . ff_act = ff_act self . dropout = dropout self . mha_dropout = mha_dropout self . return_attention_scores = return_attention_scores self . ffn = ffn def build ( self , input_shape ): \"\"\" Build the transformer block. :param input_shape: Shape of the input tensor \"\"\" if self . size is None: self . size = input_shape [- 1 ] self . att = MultiHeadAttention ( num_heads = self . num_heads , key_dim = self . size , dropout = self . mha_dropout , ) if self . ff_size is None: self . ff_size = self . size * self . ff_mul # self.ffn = self.ffn or build_dense_model( # [ # self.ff_size, # Dropout(self.dropout), # self.size, # Dropout(self.dropout) # ], # activation=self.ff_act, # last_activation=None) self . ffn = self . ffn or Sequential ([ Dense ( self . ff_size , activation = self . ff_act ), Dropout ( self . dropout ), Dense ( self . size ), Dropout ( self . dropout ) ]) self . layernorm1 = LayerNormalization ( epsilon = 1e-6 ) self . layernorm2 = LayerNormalization ( epsilon = 1e-6 ) def call ( self , inputs ): \"\"\" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\"\" if self . return_attention_scores: x , scores = self . att ( inputs , inputs , return_attention_scores = True ) else: x = self . att ( inputs , inputs ) x = self . layernorm1 ( x + inputs ) y = self . ffn ( x ) if self . return_attention_scores: return self . layernorm2 ( x + y ), scores return self . layernorm2 ( x + y ) def get_config ( self ): config = super (). get_config () config . update ({ \"size\" : self . size , \"num_heads\" : self . num_heads , \"ff_mul\" : self . ff_mul , \"ff_size\" : self . ff_size , \"ff_act\" : self . ff_act , \"dropout\" : self . dropout , \"mha_dropout\" : self . mha_dropout , \"return_attention_scores\" : self . return_attention_scores , \"ffn\" : self . ffn }) return config Ancestors (in MRO) keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector Descendants grid_transformer.transformer_block.TransformerBlock Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Instance variables activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights. Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update () add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable build def build ( self , input_shape ) Build the transformer block. Parameters: Name Type Description Default input_shape None Shape of the input tensor None View Source def build ( self , input_shape ) : \"\" \" Build the transformer block. :param input_shape: Shape of the input tensor \"\" \" if self.size is None: self.size = input_shape[-1] self.att = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.size, dropout=self.mha_dropout, ) if self.ff_size is None: self.ff_size = self.size * self.ff_mul # self.ffn = self.ffn or build_dense_model( # [ # self.ff_size, # Dropout(self.dropout), # self.size, # Dropout(self.dropout) # ], # activation=self.ff_act, # last_activation=None) self.ffn = self.ffn or Sequential([ Dense(self.ff_size, activation=self.ff_act), Dropout(self.dropout), Dense(self.size), Dropout(self.dropout) ]) self.layernorm1 = LayerNormalization(epsilon=1e-6) self.layernorm2 = LayerNormalization(epsilon=1e-6) call def call ( self , inputs ) Perform the forward pass of the transformer block. Parameters: Name Type Description Default inputs None Input tensor None Returns: Type Description None Output tensor and attention scores if return_attention_scores is True View Source def call ( self , inputs ) : \"\" \" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\" \" if self.return_attention_scores: x, scores = self.att(inputs, inputs, return_attention_scores=True) else: x = self.att(inputs, inputs) x = self.layernorm1(x + inputs) y = self.ffn(x) if self.return_attention_scores: return self.layernorm2(x + y), scores return self.layernorm2(x + y) compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights ) finalize_state def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source def get_config ( self ) : config = super () . get_config () config . update ( { \"size\" : self . size , \"num_heads\" : self . num_heads , \"ff_mul\" : self . ff_mul , \"ff_size\" : self . ff_size , \"ff_act\" : self . ff_act , \"dropout\" : self . dropout , \"mha_dropout\" : self . mha_dropout , \"return_attention_scores\" : self . return_attention_scores , \"ffn\" : self . ffn } ) return config get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" ) get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" ) get_weights def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"Transformer Block"},{"location":"reference/grid_transformer/transformer_block/#module-grid_transformertransformer_block","text":"This module contains two classes TransformerBlockBase and TransformerBlock , which implement the transformer block as defined in the \"Attention Is All You Need\" paper by Google. A transformer block consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. Classes: TransformerBlockBase: A transformer block with original implementation of the call function TransformerBlock: A transformer block with modified implementation of the call function which changes the order of layernorm and residual connection. Example usage: import tensorflow as tf # Instantiate the transformer block tb = transformer_block . TransformerBlockBase () # Build the transformer block with input shape (batch_size, sequence_length, input_dim) tb . build (( None , 10 , 32 )) # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim) output = tb ( tf . random . normal (( 32 , 10 , 32 ))) # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim) output , scores = tb ( tf . random . normal (( 32 , 10 , 32 )), return_attention_scores = True ) # Instantiate a transformer block with user-specified feed-forward network ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( 256 , activation = 'relu' ), tf . keras . layers . Dense ( 32 )]) tb = transformer_block . TransformerBlock ( ffn = ffn ) # Instantiate a TransformerBlock2 tb2 = transformer_block . TransformerBlock () # Build the transformer block with input shape (batch_size, sequence_length, input_dim) tb2 . build (( None , 10 , 32 )) # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim) output = tb2 ( tf . random . normal (( 32 , 10 , 32 ))) # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim) output , scores = tb2 ( tf . random . normal (( 32 , 10 , 32 )), return_attention_scores = True ) # Instantiate a transformer block with user-specified feed-forward network ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( 256 , activation = 'relu' ), tf . keras . layers . Dense ( 32 )]) tb = transformer_block . TransformerBlock ( ffn = ffn ) View Source \"\"\" This module contains two classes `TransformerBlockBase` and `TransformerBlock`, which implement the transformer block as defined in the \"Attention Is All You Need\" paper by Google. A transformer block consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. Classes: - TransformerBlockBase: A transformer block with original implementation of the call function - TransformerBlock: A transformer block with modified implementation of the call function which changes the order of layernorm and residual connection. Example usage: import tensorflow as tf # Instantiate the transformer block tb = transformer_block.TransformerBlockBase() # Build the transformer block with input shape (batch_size, sequence_length, input_dim) tb.build((None, 10, 32)) # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim) output = tb(tf.random.normal((32, 10, 32))) # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim) output, scores = tb(tf.random.normal((32, 10, 32)), return_attention_scores=True) # Instantiate a transformer block with user-specified feed-forward network ffn = tf.keras.Sequential([tf.keras.layers.Dense(256, activation='relu'), tf.keras.layers.Dense(32)]) tb = transformer_block.TransformerBlock(ffn=ffn) # Instantiate a TransformerBlock2 tb2 = transformer_block.TransformerBlock() # Build the transformer block with input shape (batch_size, sequence_length, input_dim) tb2.build((None, 10, 32)) # Perform the forward pass with input tensor of shape (batch_size, sequence_length, input_dim) output = tb2(tf.random.normal((32, 10, 32))) # Perform the forward pass and return attention scores with input tensor of shape (batch_size, sequence_length, input_dim) output, scores = tb2(tf.random.normal((32, 10, 32)), return_attention_scores=True) # Instantiate a transformer block with user-specified feed-forward network ffn = tf.keras.Sequential([tf.keras.layers.Dense(256, activation='relu'), tf.keras.layers.Dense(32)]) tb = transformer_block.TransformerBlock(ffn=ffn) \"\"\" from tensorflow.keras import Sequential from tensorflow.keras.layers import Layer , MultiHeadAttention , Dense , Dropout , LayerNormalization class TransformerBlockBase ( Layer ): \"\"\" A transformer block as defined in the \"Attention Is All You Need\" paper by Google. It consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. \"\"\" def __init__ ( self , num_heads = 8 , size = None , ff_mul = 4 , ff_size = None , dropout = 0.1 , mha_dropout = 0.1 , ff_act = \"gelu\" , ffn = None , return_attention_scores = False ): \"\"\" Initialize the transformer block. :param num_heads: Number of heads in the multi-head self-attention mechanism :param size: Dimension of the input/output vectors. If None, it will be inferred from input_shape during build() :param ff_mul: Multiplier for the feed-forward network size :param ff_size: Size of the feed-forward network. If None, it will be inferred from size and ff_mul :param dropout: Dropout rate for the feed-forward network :param mha_dropout: Dropout rate for the multi-head self-attention mechanism :param ff_act: Activation function for the feed-forward network :param ffn: A user-specified feed-forward network. If None, a default one will be created :param return_attention_scores: Whether to return attention scores \"\"\" super ( TransformerBlockBase , self ) . __init__ () self . size = size self . num_heads = num_heads self . ff_mul = ff_mul self . ff_size = ff_size self . ff_act = ff_act self . dropout = dropout self . mha_dropout = mha_dropout self . return_attention_scores = return_attention_scores self . ffn = ffn def build ( self , input_shape ): \"\"\" Build the transformer block. :param input_shape: Shape of the input tensor \"\"\" if self . size is None : self . size = input_shape [ - 1 ] self . att = MultiHeadAttention ( num_heads = self . num_heads , key_dim = self . size , dropout = self . mha_dropout , ) if self . ff_size is None : self . ff_size = self . size * self . ff_mul # self.ffn = self.ffn or build_dense_model( # [ # self.ff_size, # Dropout(self.dropout), # self.size, # Dropout(self.dropout) # ], # activation=self.ff_act, # last_activation=None) self . ffn = self . ffn or Sequential ([ Dense ( self . ff_size , activation = self . ff_act ), Dropout ( self . dropout ), Dense ( self . size ), Dropout ( self . dropout ) ]) self . layernorm1 = LayerNormalization ( epsilon = 1e-6 ) self . layernorm2 = LayerNormalization ( epsilon = 1e-6 ) def call ( self , inputs ): \"\"\" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\"\" if self . return_attention_scores : x , scores = self . att ( inputs , inputs , return_attention_scores = True ) else : x = self . att ( inputs , inputs ) x = self . layernorm1 ( x + inputs ) y = self . ffn ( x ) if self . return_attention_scores : return self . layernorm2 ( x + y ), scores return self . layernorm2 ( x + y ) def get_config ( self ): config = super () . get_config () config . update ({ \"size\" : self . size , \"num_heads\" : self . num_heads , \"ff_mul\" : self . ff_mul , \"ff_size\" : self . ff_size , \"ff_act\" : self . ff_act , \"dropout\" : self . dropout , \"mha_dropout\" : self . mha_dropout , \"return_attention_scores\" : self . return_attention_scores , \"ffn\" : self . ffn }) return config class TransformerBlock ( TransformerBlockBase ): \"\"\" A TransformerBlock with a modified call function, which changes the order of layernorm and residual connection. \"\"\" def call ( self , inputs ): \"\"\" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\"\" x = self . layernorm1 ( inputs ) if self . return_attention_scores : x , scores = self . att ( x , x , return_attention_scores = True ) else : x = self . att ( x , x ) x = x + inputs y = self . layernorm2 ( x ) y = self . ffn ( y ) if self . return_attention_scores : return x + y , scores return x + y","title":"Module grid_transformer.transformer_block"},{"location":"reference/grid_transformer/transformer_block/#classes","text":"","title":"Classes"},{"location":"reference/grid_transformer/transformer_block/#transformerblock","text":"class TransformerBlock ( num_heads = 8 , size = None , ff_mul = 4 , ff_size = None , dropout = 0.1 , mha_dropout = 0.1 , ff_act = 'gelu' , ffn = None , return_attention_scores = False ) A TransformerBlock with a modified call function, which changes the order of layernorm and residual connection. View Source class TransformerBlock ( TransformerBlockBase ): \"\"\" A TransformerBlock with a modified call function, which changes the order of layernorm and residual connection. \"\"\" def call ( self , inputs ): \"\"\" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\"\" x = self . layernorm1 ( inputs ) if self . return_attention_scores: x , scores = self . att ( x , x , return_attention_scores = True ) else: x = self . att ( x , x ) x = x + inputs y = self . layernorm2 ( x ) y = self . ffn ( y ) if self . return_attention_scores: return x + y , scores return x + y","title":"TransformerBlock"},{"location":"reference/grid_transformer/transformer_block/#ancestors-in-mro","text":"grid_transformer.transformer_block.TransformerBlockBase keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/transformer_block/#static-methods","text":"","title":"Static methods"},{"location":"reference/grid_transformer/transformer_block/#from_config","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/grid_transformer/transformer_block/#with_name_scope","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/transformer_block/#instance-variables","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights.","title":"Instance variables"},{"location":"reference/grid_transformer/transformer_block/#methods","text":"","title":"Methods"},{"location":"reference/grid_transformer/transformer_block/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/transformer_block/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/transformer_block/#add_update","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/transformer_block/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/transformer_block/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/transformer_block/#build","text":"def build ( self , input_shape ) Build the transformer block. Parameters: Name Type Description Default input_shape None Shape of the input tensor None View Source def build ( self , input_shape ) : \"\" \" Build the transformer block. :param input_shape: Shape of the input tensor \"\" \" if self.size is None: self.size = input_shape[-1] self.att = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.size, dropout=self.mha_dropout, ) if self.ff_size is None: self.ff_size = self.size * self.ff_mul # self.ffn = self.ffn or build_dense_model( # [ # self.ff_size, # Dropout(self.dropout), # self.size, # Dropout(self.dropout) # ], # activation=self.ff_act, # last_activation=None) self.ffn = self.ffn or Sequential([ Dense(self.ff_size, activation=self.ff_act), Dropout(self.dropout), Dense(self.size), Dropout(self.dropout) ]) self.layernorm1 = LayerNormalization(epsilon=1e-6) self.layernorm2 = LayerNormalization(epsilon=1e-6)","title":"build"},{"location":"reference/grid_transformer/transformer_block/#call","text":"def call ( self , inputs ) Perform the forward pass of the transformer block. Parameters: Name Type Description Default inputs None Input tensor None Returns: Type Description None Output tensor and attention scores if return_attention_scores is True View Source def call ( self , inputs ) : \"\" \" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\" \" x = self.layernorm1(inputs) if self.return_attention_scores: x, scores = self.att(x, x, return_attention_scores=True) else: x = self.att(x, x) x = x + inputs y = self.layernorm2(x) y = self.ffn(y) if self.return_attention_scores: return x + y, scores return x + y","title":"call"},{"location":"reference/grid_transformer/transformer_block/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/transformer_block/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/transformer_block/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/transformer_block/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/transformer_block/#finalize_state","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/transformer_block/#get_config","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source def get_config ( self ) : config = super () . get_config () config . update ( { \"size\" : self . size , \"num_heads\" : self . num_heads , \"ff_mul\" : self . ff_mul , \"ff_size\" : self . ff_size , \"ff_act\" : self . ff_act , \"dropout\" : self . dropout , \"mha_dropout\" : self . mha_dropout , \"return_attention_scores\" : self . return_attention_scores , \"ffn\" : self . ffn } ) return config","title":"get_config"},{"location":"reference/grid_transformer/transformer_block/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/transformer_block/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/transformer_block/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/transformer_block/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/transformer_block/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/transformer_block/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/transformer_block/#get_weights","text":"def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/grid_transformer/transformer_block/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"},{"location":"reference/grid_transformer/transformer_block/#transformerblockbase","text":"class TransformerBlockBase ( num_heads = 8 , size = None , ff_mul = 4 , ff_size = None , dropout = 0.1 , mha_dropout = 0.1 , ff_act = 'gelu' , ffn = None , return_attention_scores = False ) A transformer block as defined in the \"Attention Is All You Need\" paper by Google. It consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. View Source class TransformerBlockBase ( Layer ): \"\"\" A transformer block as defined in the \" Attention Is All You Need \" paper by Google. It consists of a multi-head self-attention mechanism and a position-wise feed-forward neural network. \"\"\" def __init__ ( self , num_heads = 8 , size = None , ff_mul = 4 , ff_size = None , dropout = 0.1 , mha_dropout = 0.1 , ff_act = \"gelu\" , ffn = None , return_attention_scores = False ): \"\"\" Initialize the transformer block. :param num_heads: Number of heads in the multi-head self-attention mechanism :param size: Dimension of the input/output vectors. If None, it will be inferred from input_shape during build() :param ff_mul: Multiplier for the feed-forward network size :param ff_size: Size of the feed-forward network. If None, it will be inferred from size and ff_mul :param dropout: Dropout rate for the feed-forward network :param mha_dropout: Dropout rate for the multi-head self-attention mechanism :param ff_act: Activation function for the feed-forward network :param ffn: A user-specified feed-forward network. If None, a default one will be created :param return_attention_scores: Whether to return attention scores \"\"\" super ( TransformerBlockBase , self ). __init__ () self . size = size self . num_heads = num_heads self . ff_mul = ff_mul self . ff_size = ff_size self . ff_act = ff_act self . dropout = dropout self . mha_dropout = mha_dropout self . return_attention_scores = return_attention_scores self . ffn = ffn def build ( self , input_shape ): \"\"\" Build the transformer block. :param input_shape: Shape of the input tensor \"\"\" if self . size is None: self . size = input_shape [- 1 ] self . att = MultiHeadAttention ( num_heads = self . num_heads , key_dim = self . size , dropout = self . mha_dropout , ) if self . ff_size is None: self . ff_size = self . size * self . ff_mul # self.ffn = self.ffn or build_dense_model( # [ # self.ff_size, # Dropout(self.dropout), # self.size, # Dropout(self.dropout) # ], # activation=self.ff_act, # last_activation=None) self . ffn = self . ffn or Sequential ([ Dense ( self . ff_size , activation = self . ff_act ), Dropout ( self . dropout ), Dense ( self . size ), Dropout ( self . dropout ) ]) self . layernorm1 = LayerNormalization ( epsilon = 1e-6 ) self . layernorm2 = LayerNormalization ( epsilon = 1e-6 ) def call ( self , inputs ): \"\"\" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\"\" if self . return_attention_scores: x , scores = self . att ( inputs , inputs , return_attention_scores = True ) else: x = self . att ( inputs , inputs ) x = self . layernorm1 ( x + inputs ) y = self . ffn ( x ) if self . return_attention_scores: return self . layernorm2 ( x + y ), scores return self . layernorm2 ( x + y ) def get_config ( self ): config = super (). get_config () config . update ({ \"size\" : self . size , \"num_heads\" : self . num_heads , \"ff_mul\" : self . ff_mul , \"ff_size\" : self . ff_size , \"ff_act\" : self . ff_act , \"dropout\" : self . dropout , \"mha_dropout\" : self . mha_dropout , \"return_attention_scores\" : self . return_attention_scores , \"ffn\" : self . ffn }) return config","title":"TransformerBlockBase"},{"location":"reference/grid_transformer/transformer_block/#ancestors-in-mro_1","text":"keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.trackable.autotrackable.AutoTrackable tensorflow.python.trackable.base.Trackable keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/grid_transformer/transformer_block/#descendants","text":"grid_transformer.transformer_block.TransformerBlock","title":"Descendants"},{"location":"reference/grid_transformer/transformer_block/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/grid_transformer/transformer_block/#from_config_1","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config None A Python dictionary, typically the output of get_config. None Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Args: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/grid_transformer/transformer_block/#with_name_scope_1","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Type Description Default method None The method to wrap. None Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/grid_transformer/transformer_block/#instance-variables_1","text":"activity_regularizer Optional regularizer function for the output of this layer. compute_dtype The dtype of the layer's computations. This is equivalent to Layer.dtype_policy.compute_dtype . Unless mixed precision is used, this is the same as Layer.dtype , the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in Layer.__call__ , so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when compute_dtype is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. dtype The dtype of the layer weights. This is equivalent to Layer.dtype_policy.variable_dtype . Unless mixed precision is used, this is the same as Layer.compute_dtype , the dtype of the layer's computations. dtype_policy The dtype policy associated with this layer. This is an instance of a tf.keras.mixed_precision.Policy . dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Return Functional API nodes upstream of this layer. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. input_spec InputSpec instance(s) describing the input format for this layer. When you create a layer subclass, you can set self.input_spec to enable the layer to run input compatibility checks when it is called. Consider a Conv2D layer: it can only be called on a single input tensor of rank 4. As such, you can set, in __init__() : self . input_spec = tf . keras . layers . InputSpec ( ndim = 4 ) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape (2,) , it will raise a nicely-formatted error: ValueError : Input 0 of layer conv2d is incompatible with the layer : expected ndim = 4 , found ndim = 1 . Full shape received : [ 2 ] Input checks that can be specified via input_spec include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see tf.keras.layers.InputSpec . losses List of losses added using the add_loss() API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. metrics List of metrics added using the add_metric() API. name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . outbound_nodes Return Functional API nodes downstream of this layer. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. stateful submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). a = tf.Module() b = tf.Module() c = tf.Module() a.b = b b.c = c list(a.submodules) == [b, c] True list(b.submodules) == [c] True list(c.submodules) == [] True supports_masking Whether this layer supports computing a mask using compute_mask . trainable trainable_variables trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. updates variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of self.weights . Note: This will not track the weights of nested tf.Modules that are not themselves Keras layers. weights Returns the list of all layer variables/weights.","title":"Instance variables"},{"location":"reference/grid_transformer/transformer_block/#methods_1","text":"","title":"Methods"},{"location":"reference/grid_transformer/transformer_block/#add_loss_1","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Parameters: Name Type Description Default losses None Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. None **kwargs None Used for backwards compatibility only. None View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Used for backwards compatibility only. \"\"\" kwargs . pop ( \"inputs\" , None ) if kwargs: raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}\" ) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: # Will be filtered out when computing the .losses property return None if not tf . is_tensor ( loss ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True return loss losses = tf . nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tf . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = tf . convert_to_tensor ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if ( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor ) ) and not base_layer_utils . is_in_tf_function (): symbolic_losses . append ( loss ) elif tf . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( \"Expected a symbolic Tensors or a callable for the loss value. \" \"Please wrap your loss computation in a zero argument `lambda`.\" ) self . _eager_losses . extend ( eager_losses ) for symbolic_loss in symbolic_losses: if getattr ( self , \"_is_graph_network\" , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/grid_transformer/transformer_block/#add_metric_1","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( inputs )) self . add_metric ( tf . reduce_sum ( inputs ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Type Description Default value None Metric tensor. None name None String metric name. None **kwargs None Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . None View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(inputs)) self.add_metric(tf.reduce_sum(inputs), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != \"aggregation\" ) : raise TypeError ( f \"Unknown keyword arguments: {kwargs.keys()}. \" \"Expected `aggregation`.\" ) from_metric_obj = hasattr ( value , \"_metric_obj\" ) is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we # use metric name to lookup a metric. Without a name, a new Mean # metric wrapper will be created on every model/layer call. So, we # raise an error when no name is provided. We will do the same for # symbolic mode for consistency although a name will be generated if # no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( \"Please provide a name for your metric like \" \"`self.add_metric(tf.reduce_sum(inputs), \" \"name='mean_activation')`\" ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( \"Expected a symbolic Tensor for the metric value, received: \" + str ( value ) ) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , \"_is_graph_network\" , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update # state on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , \"_metric_obj\" , None ) # Tensors that come from a Metric object already updated the Metric # state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it # defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , \"dtype\" , None ) ) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( \"Using the result of calling a `Metric` object \" \"when calling `add_metric` on a Functional \" \"Model is not supported. Please pass the \" \"Tensor to monitor directly.\" ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else \"mean\" self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/grid_transformer/transformer_block/#add_update_1","text":"def add_update ( self , updates ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Type Description Default updates None Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. None View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Args: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. \"\" \" call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in tf . nest . flatten ( updates ) : if callable ( update ) : update ()","title":"add_update"},{"location":"reference/grid_transformer/transformer_block/#add_variable_1","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( \"`layer.add_variable` is deprecated and \" \"will be removed in a future version. \" \"Please use the `layer.add_weight()` method instead.\" , stacklevel = 2 , ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/grid_transformer/transformer_block/#add_weight_1","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregationV2 . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Type Description Default name None Variable name. None shape None Variable shape. Defaults to scalar if unspecified. scalar if unspecified dtype None The type of the variable. Defaults to self.dtype . self.dtype initializer None Initializer instance (callable). None regularizer None Regularizer instance (callable). None trainable None Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . None constraint None Constraint instance (callable). None use_resource None Whether to use a ResourceVariable or not. See this guide for more information. None synchronization None Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . None aggregation None Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . None **kwargs None Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . None Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @ doc_controls . for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . VariableAggregation . NONE , ** kwargs , ) : \"\"\"Adds a new variable to the layer. Args : name : Variable name . shape : Variable shape . Defaults to scalar if unspecified . dtype : The type of the variable . Defaults to ` self . dtype ` . initializer : Initializer instance ( callable ). regularizer : Regularizer instance ( callable ). trainable : Boolean , whether the variable should be part of the layer ' s \"trainable_variables\" ( e . g . variables , biases ) or \"non_trainable_variables\" ( e . g . BatchNorm mean and variance ). Note that ` trainable ` cannot be ` True ` if ` synchronization ` is set to ` ON_READ ` . constraint : Constraint instance ( callable ). use_resource : Whether to use a ` ResourceVariable ` or not . See [ this guide ]( https : //www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables) for more information . synchronization : Indicates when a distributed a variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableSynchronization ` . By default the synchronization is set to ` AUTO ` and the current ` DistributionStrategy ` chooses when to synchronize . If ` synchronization ` is set to ` ON_READ ` , ` trainable ` must not be set to ` True ` . aggregation : Indicates how a distributed variable will be aggregated . Accepted values are constants defined in the class ` tf . VariableAggregation ` . ** kwargs : Additional keyword arguments . Accepted values are ` getter ` , ` collections ` , ` experimental_autocast ` and ` caching_device ` . Returns : The variable created . Raises : ValueError : When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ` ON_READ ` . \"\"\" if shape is None : shape = () kwargs . pop ( \"partitioner\" , None ) # Ignored . # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ \"collections\" , \"experimental_autocast\" , \"caching_device\" , \"getter\" , \"layout\" , ] : raise TypeError ( \"Unknown keyword argument:\" , kwarg ) collections_arg = kwargs . pop ( \"collections\" , None ) # 'experimental_autocast' can be set to False by the caller to indicate # an AutoCastVariable should never be created. autocast = kwargs . pop ( \"experimental_autocast\" , True ) # See the docstring for tf.Variable about the details for # caching_device. caching_device = kwargs . pop ( \"caching_device\" , None ) layout = kwargs . pop ( \"layout\" , None ) # Specially handling of auto layout fetch, based on the variable name # and attribute name. For built-in keras layers, usually the variable # name, eg 'kernel', will match with a 'kernel_layout' attribute name on # the instance. We will try to do this auto fetch if layout is not # explicitly specified. This is mainly a quick workaround for not # applying too many interface change to built-in layers, until DTensor # is a public API. Also see dtensor.utils.allow_initializer_layout for # more details. # TODO(scottzhu): Remove this once dtensor is public to end user. if not layout and name : layout = getattr ( self , name + \"_layout\" , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = tf . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable # dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf . VariableSynchronization . ON_READ : if trainable : raise ValueError ( \"Synchronization value can be set to \" \"VariableSynchronization.ON_READ only for non-trainable \" \"variables. You have specified trainable=True and \" \"synchronization=VariableSynchronization.ON_READ.\" ) else : # Set trainable to be false when variable is to be synced on # read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( \"glorot_uniform\" ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( \"zeros\" ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX # here? elif \"getter\" not in kwargs : # When `getter` is specified, it's possibly fine for # `initializer` to be None since it's up to the custom `getter` # to raise error in case it indeed needs `initializer`. raise ValueError ( f \"An initializer for variable {name} of type \" f \"{dtype.base_dtype} is required for layer \" f \"{self.name}. Received: {initializer}.\" ) getter = kwargs . pop ( \"getter\" , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision # API, disable it if it is specified. # TODO(b/142020079): Re-enable it once the bug is fixed. if caching_device is not None : tf_logging . warning ( \"`caching_device` does not work with mixed precision API. \" \"Ignoring user specified `caching_device`.\" ) caching_device = None if layout : getter = functools . partial ( getter , layout = layout ) variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device , ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( \":\" )] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/grid_transformer/transformer_block/#build_1","text":"def build ( self , input_shape ) Build the transformer block. Parameters: Name Type Description Default input_shape None Shape of the input tensor None View Source def build ( self , input_shape ) : \"\" \" Build the transformer block. :param input_shape: Shape of the input tensor \"\" \" if self.size is None: self.size = input_shape[-1] self.att = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.size, dropout=self.mha_dropout, ) if self.ff_size is None: self.ff_size = self.size * self.ff_mul # self.ffn = self.ffn or build_dense_model( # [ # self.ff_size, # Dropout(self.dropout), # self.size, # Dropout(self.dropout) # ], # activation=self.ff_act, # last_activation=None) self.ffn = self.ffn or Sequential([ Dense(self.ff_size, activation=self.ff_act), Dropout(self.dropout), Dense(self.size), Dropout(self.dropout) ]) self.layernorm1 = LayerNormalization(epsilon=1e-6) self.layernorm2 = LayerNormalization(epsilon=1e-6)","title":"build"},{"location":"reference/grid_transformer/transformer_block/#call_1","text":"def call ( self , inputs ) Perform the forward pass of the transformer block. Parameters: Name Type Description Default inputs None Input tensor None Returns: Type Description None Output tensor and attention scores if return_attention_scores is True View Source def call ( self , inputs ) : \"\" \" Perform the forward pass of the transformer block. :param inputs: Input tensor :return: Output tensor and attention scores if return_attention_scores is True \"\" \" if self.return_attention_scores: x, scores = self.att(inputs, inputs, return_attention_scores=True) else: x = self.att(inputs, inputs) x = self.layernorm1(x + inputs) y = self.ffn(x) if self.return_attention_scores: return self.layernorm2(x + y), scores return self.layernorm2(x + y)","title":"call"},{"location":"reference/grid_transformer/transformer_block/#compute_mask_1","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Type Description Default inputs None Tensor or list of tensors. None mask None Tensor or list of tensors. None Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : \"\"\"Computes an output mask tensor. Args: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in tf . nest . flatten ( mask )) : raise TypeError ( \"Layer \" + self . name + \" does not support masking, \" \"but was passed an input_mask: \" + str ( mask ) ) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/grid_transformer/transformer_block/#compute_output_shape_1","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Type Description Default input_shape None Shape tuple (tuple of integers) or tf.TensorShape , or structure of shape tuples / tf.TensorShape instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. None Returns: Type Description None A tf.TensorShape instance or structure of tf.TensorShape instances. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here. Args: input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`, or structure of shape tuples / `tf.TensorShape` instances (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: A `tf.TensorShape` instance or structure of `tf.TensorShape` instances. \"\"\" if tf . executing_eagerly (): # In this case we build the model first in order to do shape # inference. This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later # be built for. It would however cause issues in case a user # attempts to use `compute_output_shape` manually with shapes that # are incompatible with the shape the Layer will be called on (these # users will have to implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) graph_name = str ( self . name ) + \"_scratch_graph\" with tf . __internal__ . FuncGraph ( graph_name ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = tf . nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: raise NotImplementedError ( \"We could not automatically infer the static shape of \" \"the layer's output. Please implement the \" \"`compute_output_shape` method on your layer (%s).\" % self . __class__ . __name__ ) from e return tf . nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( \"Please run in eager mode or implement the `compute_output_shape` \" \"method on your layer (%s).\" % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/grid_transformer/transformer_block/#compute_output_signature_1","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Type Description Default input_signature None Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. None Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tf . TensorSpec ) : raise TypeError ( \"Only TensorSpec signature types are supported. \" f \"Received: {s}.\" ) return s . shape input_shape = tf . nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in tf . nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first # input's dtype. dtype = input_dtypes [ 0 ] return tf . nest . map_structure ( lambda s : tf . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/grid_transformer/transformer_block/#count_params_1","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , \"_is_graph_network\" , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( \"You tried to call `count_params` \" f \"on layer {self.name}\" \", but the layer isn't built. \" \"You can build it manually via: \" f \"`{self.name}.build(batch_input_shape)`.\" ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/grid_transformer/transformer_block/#finalize_state_1","text":"def finalize_state ( self ) Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. View Source @ doc_controls . do_not_generate_docs def finalize_state ( self ): \"\"\"Finalizes the layers state after updating layer weights. This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update. This function will be called after weights of a layer have been restored from a loaded model. \"\"\" pass","title":"finalize_state"},{"location":"reference/grid_transformer/transformer_block/#get_config_1","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description None Python dictionary. View Source def get_config ( self ) : config = super () . get_config () config . update ( { \"size\" : self . size , \"num_heads\" : self . num_heads , \"ff_mul\" : self . ff_mul , \"ff_size\" : self . ff_size , \"ff_act\" : self . ff_act , \"dropout\" : self . dropout , \"mha_dropout\" : self . mha_dropout , \"return_attention_scores\" : self . return_attention_scores , \"ffn\" : self . ffn } ) return config","title":"get_config"},{"location":"reference/grid_transformer/transformer_block/#get_input_at_1","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first input node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first input node of the layer. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_tensors\" , \"input\" )","title":"get_input_at"},{"location":"reference/grid_transformer/transformer_block/#get_input_mask_at_1","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, \"_keras_mask\", None) for x in inputs ] else : return getattr ( inputs , \"_keras_mask\" , None )","title":"get_input_mask_at"},{"location":"reference/grid_transformer/transformer_block/#get_input_shape_at_1","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"input_shapes\" , \"input shape\" )","title":"get_input_shape_at"},{"location":"reference/grid_transformer/transformer_block/#get_output_at_1","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first output node of the layer. None Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first output node of the layer. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_tensors\" , \"output\" )","title":"get_output_at"},{"location":"reference/grid_transformer/transformer_block/#get_output_mask_at_1","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, \"_keras_mask\", None) for x in output ] else : return getattr ( output , \"_keras_mask\" , None )","title":"get_output_mask_at"},{"location":"reference/grid_transformer/transformer_block/#get_output_shape_at_1","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Type Description Default node_index None Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. None Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , \"output_shapes\" , \"output shape\" )","title":"get_output_shape_at"},{"location":"reference/grid_transformer/transformer_block/#get_weights_1","text":"def get_weights ( self ) Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of NumPy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer, as NumPy arrays. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: >>> layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) >>> layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) >>> layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> layer_b.set_weights(layer_a.get_weights()) >>> layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of NumPy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/grid_transformer/transformer_block/#set_weights_1","text":"def set_weights ( self , weights ) Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer. For example, a Dense layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another Dense layer: layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Type Description Default weights None a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). None Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ) : \"\"\"Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer . This function sets the weight values from numpy arrays . The weight values should be passed in the order they are created by the layer . Note that the layer ' s weights must be instantiated before calling this function , by calling the layer . For example , a ` Dense ` layer returns a list of two values : the kernel matrix and the bias vector . These can be used to set the weights of another ` Dense ` layer : >>> layer_a = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 1. )) >>> a_out = layer_a ( tf . convert_to_tensor ([[ 1. , 2. , 3. ]])) >>> layer_a . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b = tf . keras . layers . Dense ( 1 , ... kernel_initializer = tf . constant_initializer ( 2. )) >>> b_out = layer_b ( tf . convert_to_tensor ([[ 10. , 20. , 30. ]])) >>> layer_b . get_weights () [ array ([[ 2. ], [ 2. ], [ 2. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] >>> layer_b . set_weights ( layer_a . get_weights ()) >>> layer_b . get_weights () [ array ([[ 1. ], [ 1. ], [ 1. ]], dtype = float32 ), array ([ 0. ], dtype = float32 )] Args : weights : a list of NumPy arrays . The number of arrays and their shape must match number of the dimensions of the weights of the layer ( i . e . it should match the output of ` get_weights ` ). Raises : ValueError : If the provided weights list does not match the layer ' s specifications . \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ) : raise ValueError ( ' You called ` set_weights ( weights ) ` on layer \"%s\" ' \"with a weight list of length %s, but the layer was \" \"expecting %s weights. Provided weights: %s...\" % ( self . name , len ( weights ), expected_num_weights , str ( weights )[ : 50 ], ) ) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ) : num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] weight_shape = weight . shape if hasattr ( weight , \"shape\" ) else () ref_shape = param . shape if not ref_shape . is_compatible_with ( weight_shape ) : raise ValueError ( f \"Layer {self.name} weight shape {ref_shape} \" \"is not compatible with provided weight \" f \"shape {weight_shape}.\" ) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) # Perform any layer defined finalization of the layer state. for layer in self . _flatten_layers () : layer . finalize_state ()","title":"set_weights"}]}